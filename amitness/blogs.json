{"text": "I recently discovered a way to set up VSCode on Google Colab and use it as an editor to write code and run experiments on the Colab VM.With this setup, you can still prototype in the Colab Notebook while also using VSCode for all the advantages of a full-fledged code editor. Here is how you can replicate my setup.In this setup, we use the  package that automates all the manual setup steps previously described in the  section of this blog post. You can make a copy of this  directly to get started.First, install the  package using the following command:Now, import  class from the package and specify the port and password.You can also use it directly with the default port and without any password as shown below.You will get the ngrok URL in the output. Click the link and a login page will open in a new tab.Type the password you had set in step 2 and click submit. If the page gets stuck for more than 4-5 seconds, refresh the page and you should be redirected to the editor.Now you will get access to the editor interface and can use it to work on python files.I have described the setup steps in detail below. After going through all the steps, please use this  to try it out directly.First, we will install the  package to run VSCode editor as a web app. Copy and run the following command on colab to install .After the installation is complete, we will expose a random port  to an external URL we can access using the  package. To install , runThen, run the following command to get a public ngrok URL. This will be the URL we will use to access VSCode.Now, we will start the VSCode server in the background at port 9000 without any authentication using the following command.Now, you can access the VSCode interface at the URL you got from step 3. The interface and functionality are the same as the desktop version of VSCode.You can switch to the dark theme by going to the bottom-left corner of the editor, clicking the , and then clicking \u2018\u2019.A popup will open. Select  in the options and the editor will switch to a dark theme.\nAll the keyword shortcuts of regular VSCode works with this. For example, you can use  to open a popup for various actions.To open a terminal, you can use the shortcut .To get python code completions, you can install the Python() extension from the extensions page on the left sidebar.The Colab interface is still usable as a notebook and regular functions to upload and download files and mount with Google Drive. Thus, you get the benefits of both a notebook and a code editor.", "meta": {"title": "VSCode on Google Colab"}}
{"text": "Hugging Face recently released  for almost 140 languages on their model hub.These models were originally trained by  of the . They were trained on the  using a neural machine translation framework called .In this post, I will explain how you can use the MarianMT models to augment data text data.We will use a data augmentation technique called \u201cBack Translation\u201d. In this, we take an original text written in English. Then, we convert it into another language (eg. French) using MarianMT. We translate the French text back into English using MarianMT. We keep the back-translated English text if it is different from the original English sentence.First, we need to install Hugging Face transformers and Moses Tokenizers with the following commandAfter installation, we can now import the MarianMT model and tokenizer.Then, we can create a initialize the model that can translate from English to Romance languages. This is a single model that can translate to any of the romance languages()Similarly, we can initialize models that can translate Romance languages to English.Next, we write a helper function to translate a batch of text given the machine translation model, tokenizer and the target romance language.Next, we will prepare a function to use the above  function to perform back translation.Now, we can perform data augmentation using back-translation from English to Spanish on a list of sentences as shown below.Similarly, we can perform augmentation using English to French as shown below with the exact same helper method.You can also run back translation in a chain to get more diversity. For example, Here are language codes for a subset of major romance language that you can use above.To view all available language codes, you can runBesides data augmentation, the back translation process can also be used for text paraphrasing.Similarly, we can also use it as an adversarial attack. Suppose we have a training dataset on which we trained an NLP model. Then, we can augment the training dataset and generate prediction from our model on augmented texts. If the predictions are different than our ground-truth labels, then we have a list of texts where our model fails. We can get good insights by analyzing those responses.Thus, MarianMT is a decent free and offline alternative to Google Translate for back-translation.", "meta": {"title": "Text Data Augmentation with MarianMT"}}
{"text": "It\u2019s a common task in NLP to either check a text against a pattern or extract parts from the text that matches a certain pattern. A regular expression or \u201cregex\u201d is a powerful tool to achieve this.While powerful, regex can feel daunting as it comes with a lot of features and sub-parts that you need to remember.In this post, I will illustrate the various concepts underlying regex. The goal is to help you build a good mental model of how a regex pattern works.Let\u2019s start with a simple example where we are trying to find the word \u2018cool\u2019 in the text.With regex, we could simply type out the word \u2018cool\u2019 as the pattern and it will match the word.While regex matched our desired word \u2018\u2019, the way it operates is not at the word level but the character level. This is the key idea.: Regex works at the character-level, not word-level.The implication of this is that the regex  would match the following sentences as well.Now that we understand the key idea, let\u2019s understand how we can match simple characters using regex.We can simply specify the character in the regular expression and it will match all instances in the text.For example, a regular expression given below will match all instances of \u2018a\u2019 in the text. You can use any of the small and capital alphabets.You can also use any digits from 0 to 9 and it will work as well.Note that regex is case-sensitive by default and thus the following regex won\u2019t match anything.We can detect special characters such as whitespace and newlines using special escape sequences.Besides the common ones above, we have:Regex provides a bunch of built-in special symbols that can match a group of characters at once. These begin with backslash .It matches any single-digit number between 0 to 9.Notice that matches are single digit. So we have 4 different matches below instead of a single number .It matches any whitespace character (,  or ).It matches any of the small alphabets(a to z), capital alphabets(A to Z), digits (0 to 9), and underscore.It matches any character except the new line (\\n).If you use the capitalized versions of the patterns above, they act as negation.For example, if \u201c\\d\u201d matched any digits from 0 to 9, then \u201c\\D\u201d will match anything except \u201c0 to 9\u201d.These are patterns starting with  and ending with  and specify the characters that should be matched enclosed by brackets.For example, the following pattern matches any of the characters \u2018a\u2019, \u2018e\u2019, \u2018i\u2019, \u2018o\u2019, and \u2018u\u2019.\nYou can also replicate the functionality of  using the below pattern. It will match any digits between 0 to 9.\nInstead of specifying all the digits, we can use  to specify only start and end digits. So, instead of , we can do:For example,  can be used to match any digits between 2 to 4 i.e. (2 or 3 or 4).You can even use the special characters we learned previously inside the brackets. For example, you can match any digit from 0 to 9 or whitespace as:Below, I have listed some useful common patterns and what they mean.Regex also has special handlers to make the pattern only match if it\u2019s at the start or end of the string.We can use the  anchor to match patterns only at the start of a line. For example:Similarly, we can use the  anchor after the character to match patterns only if it\u2019s the end of the line. For example:Consider a case where we want to exactly match the word \u201cMr. Stark\u201d.If we write a regex like , then it will have an unintended effect. Since we know dot has a special meaning in a regex.So, we should always escape the special metacharacters like ,  etc. if our goal is to match the exact character itself.Here is the list of metacharacters that you should remember to escape if you\u2019re using them directly.Now that we can pattern match any characters, we could repeat things and start building more complicated patterns.Using only what we have learned so far, a naive way would be to just repeat the pattern. For example, we can match two-digit numbers by just repeating the character-level pattern.Regex provides special quantifiers to specify different types of repetition for the character preceding it.We can use the  quantifier to specify the number of times a pattern should repeat.For example, the previous pattern for matching 2-digit number can be recreated as:You can also specify a range of repetitions using the same quantifier. For example, to match from 2-digit to 4-digit numbers, we could use the pattern:When applied to a sentence, it will match both 4-digit and 2-digit numbers.\nThere should not be any space between minimum and maximum count For example, \\d{2, 4} doesn't work.\nRegex also provides quantifiers \u201c*\u201d, \u201c+\u201d and \u201c?\u201d using which you can specify flexible repetition of a character.: \nThe  quantifier matches the previous character if it repeats 0 or 1 times. This can be useful to make certain parts optional. It is equivalent to .For example, let\u2019s say we want to match both the word \u201csound\u201d and \u201csounds\u201d where \u201cs\u201d is optional. Then, we can use the  quantifier that matches if a character repeats 0 or 1 times.\n: \nThe  quantifier matches the previous character if it repeats 1 or more times. It is equivalent to .For example, we could find numbers of any arbitrary length using the regex .: \nThe  quantifier matches the previous character if it repeats zero or more times. It is equivalent to .Python provides a module called \u201cre\u201d in the standard library to work with regular expression.To specify a regular expression in Python, we precede it with  to create raw strings.To understand why we precede with , let\u2019s try printing the expression  without .You can see how when we don\u2019t use raw string, the string  is treated as the escape character for tab by Python.Now let\u2019s convert it into raw string. We get back whatever we specified.To use  module, we can start by importing the  module as:This function allows us to get all the matches as a list of strings.This function searches for a pattern at the beginning of the string and returns the first occurrence as a match object. If the pattern is not found, it returns None.With the match object, we can get the matched text asIn a case where our pattern is not at the start of the sentence, we will not get any match.This function also finds the first occurrence of a pattern but the pattern can occur anywhere in the text. If the pattern is not found, it returns None.", "meta": {"title": "A Visual Guide to Regular Expression"}}
{"text": " is a free web application for visualizing high-dimensional data. It has built-in demos for visualizing word embeddings in NLP and image embeddings for MNIST in Computer Vision.I recently experimented with a way to load sentence embeddings along with the class labels into this tool and explore them interactively. In this blog post, I will explain the end-to-end process with an example dataset.To understand this use case, let\u2019s take a subset of 100 movie reviews from the SST-2 dataset which are labeled as positive and negative.The dataset has a column containing the text and a label indicating whether it\u2019s positive or negative opinion.We will introduce noise into our dataset by corrupting five of the responses with random text. It will act as an outlier for our example.Now, we will compute sentence embeddings for the headlines using the  package. First, let\u2019s install it using pip.Next, we will create a helper function to return a NumPy array of sentence embeddings given a list of sentences.Using the above function, we can generate sentence embeddings for our data as shown below.Embedding Projector requires two TSV files to load our custom embeddings.Let\u2019s first generate the  file for our sentence embeddings from the previous step.To generate , we simply save our original dataframe.We first go to .On the left-hand sidebar, click the  button.Then, for the first  button, upload the  file and for the second  button, upload the  file.After uploading both files, click outside and you should see the sentence embedding projection. The dimensions of embeddings are reduced to 3D by default using PCA.Let\u2019s switch to 2D by turning off the checkbox for \u2018Component #3\u2019 in the bottom part of sidebar.On the 2D visualization, we can see how the random text is far from other groups of text as an . On hovering the point, we see the text .We can enable color coding of the points by their actual labels (positive vs negative) by using the  dropdown in the left sidebar.Select the name of the column that contains your labels. In our example file, the column name is .The points themselves are interactive. You can see the actual sentence for each point by hovering over them.You can click on the point to show the metadata. We can see below on clicking a blue point that its label is \u201cpositive\u201d in the popup.So the blue points are positive and the red points are negative. When a point is selected, 100 nearest points in terms of cosine similarity are also highlighted.To get back to the original view, we can click on any empty white space.The color coding can be a useful heuristic for many use cases:The web app provides three standard dimensionality reduction techniques: , , and .You can choose the algorithm and their parameters from the bottom of the left sidebar.You can also use a custom keyword or full text as the axis using the  tab. This will apply a custom linear projection and can help us explore meaningful directions in the embedding space.For example, the Gmail team tried setting \u201cyeah\u201d on the left side and \u201cyes\u201d on the right side. When they projected encoder embeddings for email replies to this custom linear projection, they found replies in a casual tone (e.g. Here you go) on the left side and responses in a more formal tone clustered towards the right side.Thus, Embedding Projector is a very useful tool to better understand the datasets and models we work with.", "meta": {"title": "Interactive Analysis of Sentence Embeddings"}}
{"text": "Self Supervised Learning is an interesting research area where the goal is to learn rich representations from unlabeled data without any human annotation.This can be achieved by creatively formulating a problem such that you use parts of the data itself as labels and try to predict that. Such formulations are called pretext tasks.For example, you can setup a pretext task to predict the color version of the image given the grayscale version. Similarly, you could remove a part of the image and train a model to predict the part from the surrounding. There are many such .By pre-training on the pretext task, the hope is that the model will learn useful representations. Then, we can finetune the model to downstream tasks such as image classification, object detection, and semantic segmentation with only a small set of labeled training data.So pretext tasks can help us learn representations. But, this poses a question:How to determine how good a learned representation is?Currently, the standard way to gauge the representations is to evaluate it on a set of standard tasks and benchmark datasets.We can see that the above evaluation methods require us to use the same model architecture for both the pretext task and the target task.This poses some interesting challenges:For the pretext task, our goal is to learn on a large-scale unlabeled dataset and thus deeper models(e.g. ResNet) would help us learn better representations.But, for downstream tasks, we would prefer shallow models(e.g. AlexNet) for actual applications. Thus, we currently have to consider this limitation when designing the pretext task.It\u2019s harder to fairly compare which pre-text task is better if some methods used simpler architecture while other methods used deeper architecture.We can\u2019t compare the representations learned from pretext tasks to handcrafted features such as HOG.We may want to exploit several data domains such as sound, text, and videos in the pretext task but the target task may limit our design choices.Model trained on pretext task may learn extra knowledge that is not useful for generic visual recognition. Currently, the final task-specific layers are ignored and weights or features only up to certain convolutional layers are taken.Noroozi et al. proposed a simple idea to tackle these issues in their 2018 paper .The authors observed that in a good representation space, semantically similar data points should be close together.In regular supervised classification, the information that images are semantically similar is encoded through labels annotated by humans. A model trained on such labels would have a representation space that groups semantically similar images.Thus, with pre-text tasks in self-supervised learning, the objective is implicitly learning a metric that makes the same category images similar and different category images dissimilar. Hence we can provide a robust estimate of the learned representation if we could encode semantically related images to the same labels in some way.The authors propose a novel framework to transfer knowledge from a deep self-supervised model to a separate shallow downstream model. You can use different model architectures for the pretext task and downstream task.Cluster features from pretext task and assign cluster centers as pseudo-labels for unlabeled images. Then, re-train a small network with target task architecture on pseudo-labels to predict pseudo-labels and learn a novel representation.The end-to-end process is described below:Here we choose some deep network architecture and train it on some pretext task of our choice on some dataset. We can take features from some intermediate layer after the model is trained.\nFigure: Training on Pre-text Task ()For all the unlabeled images in the dataset, we compute the feature vectors from the pretext task model. Then, we run K-means clustering to group semantically similar images. The idea is that the cluster centers will be aligned with categories in ImageNet.\nFigure: Clustering Features ()In the paper, the authors ran K-means on a single Titan X GPU for 4 hours to cluster 1.3M images into 2000 categories.The cluster centers are treated as the pseudo-label. We can use either the same dataset as the above step or use a different dataset itself. Then, we compute the feature vectors for those images and find the closest cluster center for each image. This cluster center is used as the pseudo-label.\nFigure: Generating Pseudo-labels ()We take the model architecture that will be used for downstream tasks and train it to classify the unlabeled images into the pseudo-labels. Thus, the target architecture will learn a new representation such that it will map images that were originally close in the pre-trained feature space to close points.\nFigure: Re-training on pseudo-labels ()We saw how by clustering the features and then using pseudo-labels, we can bring the knowledge from any pretext task representations into a common reference model like AlexNet.As such, we can now easily compare different pretext tasks even if they are trained using different architectures and on different data domains. This also allows us to improve self-supervised methods by using deep models and challenging pretext tasks.To evaluate the idea quantitatively, the authors set up an experiment as described below:To evaluate their method, the authors took an old puzzle-like pretext task called \u201cJigsaw\u201d where we need to predict the permutation that was used to randomly shuffle a 3*3 square grid of image.\nImage Modified from They extended the task by randomly replacing 0 to 2 number of tiles with tile from another random image at some random locations. This increases the difficulty as now we need to solve the problem using only the remaining patches. The new pretext task is called \u201cJigsaw++\u201d.\nImage Modified from In the paper, they use 701 total permutations which had a minimum hamming distance of 3. They apply mean and standard deviation normalization at each image tile independently. They also make images gray-scale 70% of the time to prevent the network from cheating with low-level statistics.The authors used VGG-16 to solve the pretext task and learn representations. As VGG-16 has increased capacity, it can better handle the increased complexity of the \u201cJigsaw++\u201d task and thus extract better representation.The representations from VGG-16 are clustered and cluster centers are converted to pseudo-labels. Then, AlexNet is trained to classify the pseudo-labels.For downstream tasks, the convolutional layers for the AlexNet model are initialized with weights from pseudo-label classification and the fully connected layers were randomly initialized.\nThe pre-trained AlexNet is then finetuned on various benchmark datasets.Using a deeper network like VGG-16 leads to better representation and pseudo-labels and also better results in benchmark tasks. It got state of the art results on several benchmarks in 2018 and reduced the gap between supervised and self-supervised methods further.The authors tested their method on object classification and detection on PASCAL VOC 2007 dataset and semantic segmentation on PASCAL VOC 2012 dataset.In this, a linear classifier is trained on features extracted from AlexNet at different convolutional layers. For ImageNet, using VGG-16 and transferring knowledge to AlexNet using clustering gives a substantial boost of 2%.For a non-linear classifier, using VGG-16 and transferring knowledge to AlexNet using clustering gives the best performance on ImageNet.The network is not significantly affected by the number of clusters. The authors tested AlexNet trained on pseudo-labels from a different number of clusters on the task of object detection.Knowledge transfer is fundamentally different from knowledge distillation. Here, the goal is to only preserve the cluster association of images from the representation and transfer that to the target model. Unlike distillation, we don\u2019t do any regression to the exact output of the teacher.Yes, the method is flexible and you can pre-train on one dataset, cluster on another, and get pseudo-labels for the third one.The authors did an experiment where they trained clustering on representations for ImageNet and then calculated cluster centers on the \u201cPlaces\u201d dataset to get pseudo-labels. There was only a small reduction (-1.5%) in performance for object classification.Thus, Knowledge Transfer is a simple and efficient way to map representations from deep to shallow models.", "meta": {"title": "Knowledge Transfer in Self Supervised Learning"}}
{"text": "Keyword Extraction is one of the simplest ways to leverage text mining for providing business value. It can automatically identify the most representative terms in the document.Such extracted keywords can be used for various applications. They can be used to summarize the underlying theme of a large document with just a few terms. They are also valuable as metadata for indexing and tagging the documents. They can likewise be used for clustering similar documents. For instance, to showcase relevant advertisements on a webpage, we could extract keywords from the webpage, find matching advertisements for these keywords, and showcase those.In this post, I will provide an overview of the general pipeline of keyword extraction and explain the working mechanism of various unsupervised algorithms for this.For keyword extraction, all algorithms follow a similar pipeline as shown below. A document is preprocessed to remove less informative words like stop words, punctuation, and split into terms. Candidate keywords such as words and phrases are chosen.Then, a score is determined for each candidate keyword using some algorithm. The highest-ranking keywords are selected and post-processing such as removing near-duplicates is applied. Finally, the algorithm returns the top N ranking keywords as output.Unsupervised algorithms for keyword extraction don\u2019t need to be trained on the corpus and don\u2019t need any pre-defined rules, dictionary, or thesaurus. They can use statistical features from the text itself and as such can be applied to large documents easily without re-training. Most of these algorithms don\u2019t need any linguistic features except for stop word lists and so can be applied to multiple languages.Let\u2019s understand each algorithm by starting from simple methods and gradually adding complexity.This is a simple method which only takes into account how many times each term occurs.Let\u2019s understand it by applying it to an example document.In this step, we lowercase the text and remove low informative words such as stop words from the text.We split the remaining terms by space and punctuation symbols to get a list of possible keywords.We can count the number of times each term occurs to get a score for each term.We can sort the keywords in descending order based on the counts and take the top N keywords as the output.This method has an obvious drawback of only focusing on frequency. But, generic words are likely to be very frequent in any document but are not representative of the domain and topic of the document. We need some way to filter out generic terms.This method takes into account both how frequent the keyphrase is and also how rare it is across the documents.Let\u2019s understand how it works by going through the various steps of the pipeline:In this step, we lowercase the text and split the document into sentences.We generate 1-gram, 2-gram, and 3-grams candidate phrases from each sentence such that they don\u2019t contain any punctuations. These are our list of candidate phrases.Now, for each candidate keyword \u201cw\u201d, we calculate the TF-IDF score in the following steps.First, the term frequency(TF) is calculated simply by counting the occurrence of the word.Then, the inverse document frequency(IDF) is calculated by dividing the total number of documents by the number of documents that contain the word \u201cw\u201d and taking the log of that quantity.Finally, we get the  score for a term by multiplying the two quantities.We can sort the keywords in descending order based on their TF-IDF scores and take the top N keywords as the output.RAKE is a domain-independent keyword extraction method proposed in 2010. It uses word frequency and co-occurrence to identify the keywords. It is very useful for identifying relevant multi-word expressions.Let\u2019s apply RAKE on a toy example document to understand how it works:First, the stop words in the document are removed.We split the document at the stop word positions and punctuations to get content words. The words that occur consecutively without any stop word between them are taken as candidate keywords.For example, \u201cDeep Learning\u201d is treated as a single keyword.Next, the frequency of all the individual words in the candidate keywords are calculated. This finds words that occur frequently.Similarly, the word co-occurrence count is calculated and the degree for each word is the total sum. This metric identifies words that occur often in longer candidate keywords.Then, we divide the degree by the frequency for each word to get a final score. This score identifies words that occur more in longer candidate keywords than individually.Finally, we calculate the scores for our candidate keywords by adding the scores for their member words. The higher the score, the more useful a keyword is.Thus, the keywords are sorted in the descending order of their score value. We can select the top-N keywords from this list.We can use the  library to use it in Python as shown below.YAKE is another popular keyword extraction algorithm proposed in 2018. It outperforms TF-IDF and RAKE across many datasets and went on to win the best \u201cshort paper award\u201d at .YAKE uses statistical features to identify and rank the most important keywords. It doesn\u2019t need any linguistic information like NER or POS tagging and thus can be used with any language. It only requires a stop word list for the language.The sentences are split into terms using space and special character(line break, bracket, comma, period) as the delimiter.We decide the maximum length of the keyword to be generated. If we decide max length of 3, then 1-gram, 2-gram, and 3-gram candidate phrases are generated using a sliding window.Then, we remove phrases that contain punctuation marks. Also, phrases that begin and end with a stop word are removed.YAKE uses 5 features to quantify how good each word is.This feature considers the casing of the word. It gives more importance to capitalized words and acronyms such as \u201cNASA\u201d.First, we count the number of times the word starts with a capital letter when it is not the beginning word of the sentence. We also count the times when the word is in acronym form.Then, we take the maximum of the two counts and normalize it by the log of the total count.This feature gives more importance to words present at the beginning of the document. It\u2019s based on the assumption that relevant keywords are usually concentrated more at the beginning of a document.First, we get all the sentence positions where the word \u201cw\u201d occurs.Then, we compute the position feature by taking the median position and applying the following formula:This feature calculates the frequency of the words normalized by 1-standard deviation from the mean.This feature quantifies how related a word is to its context. For that, it counts how many different terms occur to the left or right of a candidate word. If the word occurs frequently with different words on the left or right side, it is more likely to be a stop word.where,This feature quantifies how often a candidate word occurs with different sentences. A word that often occurs in different sentences has a higher score.These 5 features are combined into a single score S(w) using the formula:where,Now, for each of our candidate keywords, a score is calculated using the following formula. The count of keyword penalizes less frequent keywords.It\u2019s pretty common to get similar candidates when extracting keyphrases. For example, we could have variations like:To eliminate such duplicates, the following process is applied:Thus, the chosen keyword list contains the final deduplicated keywords.Thus, we have a list of keywords along with their scores. A keyword is more important if it has a lower score.We can sort the keywords in ascending order and take the top N keywords as the output.To apply YAKE, we will use the  library. First, we need to install the library and its dependencies using the following command:Then, we can use YAKE to generate keywords of maximum length 2 as shown below.You get back a list of top-10 keywords and their scores. The highest ranked keyword has the lowest score.", "meta": {"title": "Unsupervised Keyphrase Extraction"}}
{"text": "When developing an NLP model, it\u2019s a standard practice to test how well a model generalizes to unseen examples by evaluating it on a held-out dataset. Suppose we reach our target performance metric of 95% on a held-out dataset and thus deploy the model to production based on this single metric.But, when real users start using it, the story could be completely different than what our 95% performance metric was saying. Our model might perform poorly even on simple variations of the training text.In contrast, the field of software engineering uses a suite of unit tests, integration tests, and end-to-end tests to evaluate all aspects of the product for failures. An application is deployed to production only after passing these rigorous tests. noticed this gap and took inspiration from software engineering to propose an evaluation methodology for NLP called . Their paper won the best overall paper award at ACL 2020.In this post, I will explain the overall concept of CheckList and the various components that it proposes for evaluating NLP models.To understand CheckList, let\u2019s first understand behavioral testing in the context of software engineering.Behavioral testing, also known as black-box testing, is a method where we test a piece of software based on its expected input and output. We don\u2019t need access to the actual implementation details.For example, let\u2019s say you have a function that adds two numbers together.We can evaluate this function by writing tests to compare it\u2019s output to the expected answer. We are not concerned with how this function was implemented internally.Even for a simple function such as addition, there are capabilities that it should satisfy. For example, the addition of a number with zero should yield the original number itself.CheckList proposes a general framework for writing behavioral tests for any NLP model and task.The core idea is based on a conceptual matrix that is composed of  as rows and  as columns. The intersecting cells contain multiple test examples generated from templates that we run and calculate the  for.By calculating the failure rates for various test types and capabilities, we can know exactly where our model is weak.Let\u2019s understand each part of this conceptual matrix in detail now.These are the columns in the previous matrix. There are 3 types of tests proposed in the CheckList framework:This test is similar to unit tests in software engineering. We build a collection of (text, expected label) pairs from scratch and test the model on this collection.For example, we are testing the negation capability of the model using an MFT test below.\nTemplate: I   the The goal of this test is to make sure the model is not taking any shortcuts and possesses linguistic capabilities.In this test, we perturb our existing training examples in a way that the label should not change. Then, the model is tested on this perturbed example and the model passes the test only if its prediction remains the same (i.e invariant).For example, changing the location from Chicago to Dallas should not change the original sentiment of a text.We can use different perturbation functions to test different capabilities. The paper mentions two examples:This test is similar to the invariance test but here we expect the model prediction to change after perturbation.For example, if we add a text \u201cYou are lame\u201d to the end of a text, the expectation is that sentiment of the original text will not move towards a positive direction.We can also write tests where we expect the target label to change. For example, consider the QQP task where we need to detect if two questions are duplicates or not.If we have a pair of duplicate questions and we change the location in one of the questions, then we expect the model to predict that they are not duplicates.These are the rows in the CheckList matrix. Each row contains a specific linguistic capability that applies to most NLP tasks.Let\u2019s understand examples of capabilities given in the original paper. The authors provide a lot of examples to help us build a mental model of how to test new capabilities relevant to our task and domain.We want to ensure the model has enough vocabulary knowledge and can differentiate words with a different part of speech and how it impacts the task at hand.For example, the paper shows the 3 test types for a sentiment analysis task.This can also be applied for the QQP task as shown below.It tests the capability of the model to understand named entities and whether it is important for the current task or not.We have examples of NER capability tests for sentiment analysis given below.We can also apply this to the QQP task.Here we want to test if the model understands the order of events in the text.Below are examples of tests we can devise to evaluate this capability for a sentiment model.Similarly, we can devise temporal capability tests for QQP data as well.This ensures the model understands negation and its impact on the output.Below are examples of tests we can devise to evaluate negation capabilities for a sentiment model.Similarly, we can devise negation capability tests for QQP data as well.This ensures the model understands the agent and the object in the text.Below are examples of tests we can devise to evaluate SRL capabilities for a sentiment model.Similarly, we can devise SRL capability tests for QQP data as well.This ensures that the model can handle small variations or perturbations to the input text such as typos and irrelevant changes.Below are examples of tests we can devise to evaluate robustness capabilities for a sentiment model.Similarly, we can devise robustness capability tests for QQP data as well.This ensures that the model has an understanding of synonyms and antonyms and how they affect the task at hand.Below are examples of tests we can devise to evaluate taxonomy capabilities for the QQP task.This ensures that the model has an understanding of pronouns and what nouns they refer to.Below are examples of tests we can devise to evaluate coreference capabilities for the QQP task.This ensures that the model can handle symmetry, consistency, and conjunctions.For example, in the QQP task, the order of the question shouldn\u2019t matter. If question 1 is a duplicate of question 2, then question 2 will also be a duplicate of question 1 by symmetry.This tests if the model reflects any form of bias towards a demographic from the training data.Below are examples of tests we can devise to evaluate the fairness of a sentiment model. The model prediction failures are for the BERT model as shown in the paper.The paper\u2019s authors have open-sourced a  that can generate test cases at scale based on the ideas above.The tool provides three approaches to write test cases:To generate templates, you can either brainstorm them from scratch or generalize patterns from your existing data.For example, if we had a text such as \u201c\u201d in our training data, we can generalize it as:Now, you can brainstorm possible fillers for the various template parts.By taking the cartesian products of all these possibilities, we can generate a lot of test cases.Instead of manually specifying fill-ins for the template, we can also use MLM models like ROBERTA and use masking to generate variants.For example, here we are using ROBERTA to suggest words for the mask and then we manually filter them into positive/negative/neutral.These fill-ins can be reused across multiple tests. The paper also suggests using WordNet to select only context-appropriate synonyms from ROBERTA.CheckList also provides out-of-box support for lexicons such as:CheckList also provides perturbation functions such as character swaps, contractions, name and location changes, and neutral word replacement.Thus, CheckList provides a general framework to perform a comprehensive and fine-grained evaluation of NLP models. This can help us better understand the state of NLP models beyond the leaderboard.", "meta": {"title": "Behavioral Testing of NLP models with CheckList"}}
{"text": "Semi-supervised learning methods for Computer Vision have been advancing quickly in the past few years. Current state-of-the-art methods are simplifying prior work in terms of architecture and loss function or introducing hybrid methods by blending different formulations.In this post, I will illustrate the key ideas of these recent methods for semi-supervised learning through diagrams.In this semi-supervised formulation, a model is trained on labeled data and used to predict pseudo-labels for the unlabeled data. The model is then trained on both ground truth labels and pseudo-labels simultaneously. proposed a very simple and efficient formulation called \u201cPseudo-label\u201d in 2013.The idea is to train a  simultaneously on a batch of both labeled and unlabeled images. The  is trained on labeled images in usual supervised manner with a cross-entropy loss. The same model is used to get predictions for a batch of unlabeled images and the  is used as the . Then, cross-entropy loss is calculated by comparing  predictions and the pseudo-label for the unlabeled images .The total loss is a weighted sum of the labeled and unlabeled loss terms.To make sure the model has learned enough from the labeled data, the \\(\\alpha_t\\) term is set to 0 during the initial 100 training steps. It is then gradually increased up to 600 training steps and then kept constant.\n proposed a semi-supervised method inspired by Knowledge Distillation called \u201cNoisy Student\u201d in 2019.The key idea is to train two separate models called  and . The  is first trained on the labeled images and then it is used to infer the pseudo-labels for the unlabeled images. These pseudo-labels can either be soft-label or converted to hard-label by . Then, the labeled and unlabeled images are combined together and a  is trained on this combined data. The images are augmented using RandAugment as a form of input noise. Also, model noise such as Dropout and Stochastic Depth are incorporated in the student model architecture.Once a  is trained, it becomes the new  and this process is repeated for three iterations.This paradigm uses the idea that  predictions on an unlabeled image should remain the same even after adding noise. We could use input noise such as Image Augmentation and Gaussian noise. Noise can also be incorporated in the architecture itself using Dropout.This model was proposed by  in a conference paper at ICLR 2017.The key idea is to create two random augmentations of an image for both labeled and unlabeled data. Then, a  is used to predict the label of both these images. The  of these two  is used as a . For labeled images, we also calculate the . The total loss is a weighted sum of these two loss terms. A weight  is applied to decide how much the consistency loss contributes in the overall loss.This method was also proposed by  in the same paper as the pi-model. It modifies the \u03c0-model by leveraging the  of predictions.The key idea is to use the  of past predictions as one view. To get another view, we augment the image as usual and a  is used to predict the label. The  of  and  is used as a . For labeled images, we also calculate the . The final loss is a weighted sum of these two loss terms. A weight  is applied to decide how much the consistency loss contributes in the overall loss.This method was proposed by . The general approach is similar to Temporal Ensembling but it uses Exponential Moving Average(EMA) of the model parameters instead of predictions.The key idea is to have two models called  and . The  model is a regular model with dropout. And the  model has the same architecture as the  model but its weights are set using an  of the weights of  model. For a labeled or unlabeled image, we create two random augmented versions of the image. Then, the  model is used to predict  for first image. And, the  model is used to predict the  for the second augmented image. The  of these two  is used as a . For labeled images, we also calculate the . The final loss is a weighted sum of these two loss terms. A weight  is applied to decide how much the consistency loss contributes in the overall loss.This method was proposed by . It uses the concept of adversarial attack for consistency regularization.The key idea is to generate an adversarial transformation of an image that will change the model prediction. To do so, first, an image is taken and an adversarial variant of it is created such that the KL-divergence between the model output for the original image and the adversarial image is maximized.Then we proceed as previous methods. We take a labeled/unlabeled image as first view and take its adversarial example generated in previous step as the second view. Then, the same  is used to predict  for both images. The  of these two  is used as a . For labeled images, we also calculate the . The final loss is a weighted sum of these two loss terms. A weight  is applied to decide how much the consistency loss contributes in the overall loss.This method was proposed by  and works for both images and text. Here, we will understand the method in the context of images.The key idea is to create an augmented version of a unlabeled image using AutoAugment. Then, a same  is used to predict the label of both these images. The  of these two  is used as a . For labeled images, we only calculate the  and don\u2019t calculate any . The final loss is a weighted sum of these two loss terms. A weight  is applied to decide how much the consistency loss contributes in the overall loss.This paradigm combines ideas from previous work such as self-training and consistency regularization along with additional components for performance improvement.This holistic method was proposed by .To understand this method, let\u2019s take a walk through each of the steps.i. For the labeled image, we create an augmentation of it. For the unlabeled image, we create K augmentations and get the model  on all K-images. Then, the  are  and  is applied to get a final pseudo-label. This pseudo-label will be used for all the K-augmentations.ii. The batches of augmented labeled and unlabeled images are combined and the whole group is shuffled. Then, the first N images of this group are taken as \\(W_L\\), and the remaining M images are taken as \\(W_U\\).iii. Now, Mixup is applied between the augmented labeled batch and group \\(W_L\\). Similarly, mixup is applied between the M augmented unlabeled group and the \\(W_U\\) group. Thus, we get the final labeled and unlabeled group.iv. Now, for the labeled group, we take model predictions and compute  with the ground truth mixup labels. Similarly, for the unlabeled group, we compute model predictions and compute  with the mixup pseudo labels. A weighted sum is taken of these two terms with \\(\\lambda\\) weighting the MSE loss.This method was proposed by  and combines pseudo-labeling and consistency regularization while vastly simplifying the overall method. It got state of the art results on a wide range of benchmarks.As seen, we train a supervised model on our labeled images with cross-entropy loss. For each unlabeled image,  and  are applied to get two images. The  is passed to our model and we get prediction over classes. The probability for the most confident class is compared to a . If it is above the , then we take that class as the ground label i.e. . Then, the  image is passed through our model to get a prediction over classes. This  is compared to ground truth  using cross-entropy loss. Both the losses are combined and the model is optimized.If you want to learn more about FixMatch, I have an  that goes over it in depth.Here is a high-level summary of the differences between all the above-mentioned methods.To evaluate the performance of these semi-supervised methods, the following datasets are commonly used. The authors simulate a low-data regime by using only a small portion(e.g. 40/250/4000/10000 examples) of the whole dataset as labeled and treating the remaining as the unlabeled set.Thus, we got an overview of how semi-supervised methods for Computer Vision have progressed over the years. This is a really important line of research that can have a direct impact on the industry.If you found this blog post useful, please consider citing it as:", "meta": {"title": "Semi-Supervised Learning in Computer Vision"}}
{"text": "Most software products we encounter today have some form of search functionality integrated into them. We search for content on Google, videos on YouTube, products on Amazon, messages on Slack, emails on Gmail, people on Facebook, and so on.As users, the workflow is pretty simple. We can search for items by writing our queries in a search box and the ranking model in their system gives us back the top-N most relevant results.How do we evaluate how good the top-N results are?In this post, I will answer the above question by explaining the common offline metrics used in learning to rank problems. These metrics are useful not only for evaluating search results but also for problems like keyword extraction and item recommendation.Let\u2019s take a simple toy example to understand the details and trade-offs of various evaluation metrics.We have a ranking model that gives us back 5-most relevant results for a certain query. The first, third, and fifth results were  as per our ground-truth annotation.Let\u2019s look at various metrics to evaluate this simple example.This metric quantifies how many items in the top-K results were relevant. Mathematically, this is given by:For our example, precision@1 = 1 as all items in the first 1 results is relevant.Similarly, precision@2 = 0.5 as only one of the top-2 results are relevant.Thus, we can calculate the precision score for all k values.A limitation of precision@k is that it doesn\u2019t consider the position of the relevant items. Consider two models A and B that have the same number of relevant results i.e. 3 out of 5.For model A, the first three items were relevant, while for model B, the last three items were relevant. Precision@5 would be the same for both of these models even though model A is better.This metric gives how many actual relevant results were shown out of all actual relevant results for the query. Mathematically, this is given by:For our example, recall@1 = 0.33 as only one of the 3 actual relevant items are present.Similarly, recall@3 = 0.67 as only two of the 3 actual relevant items are present.Thus, we can calculate the recall score for different K values.This is a combined metric that incorporates both Precision@k and Recall@k by taking their harmonic mean. We can calculate it as:Using the previously calculated values of precision and recall, we can calculate F1-scores for different K values as shown below.While precision, recall, and F1 give us a single-value metric, they don\u2019t consider the order in which the returned search results are sent. To solve that limitation, people have devised order-aware metrics given below:This metric is useful when we want our system to return the best relevant item and want that item to be at a higher position. Mathematically, this is given by:where:To calculate MRR, we first calculate the . It is simply the reciprocal of the rank of the first correct relevant result and the value ranges from 0 to 1.For our example, the reciprocal rank is \\(\\frac{1}{1}=1\\) as the first correct item is at position 1.Let\u2019s see another example where the only one relevant result is present at the end of the list i.e. position 5. It gets a lower reciprocal rank score of 0.2.Let\u2019s consider another example where none of the returned results are relevant. In such a scenario, the reciprocal rank will be 0.For multiple different queries, we can calculate the MRR by taking the mean of the reciprocal rank for each query.We can see that MRR doesn\u2019t care about the position of the remaining relevant results. So, if your use-case requires returning multiple relevant results in the best possible way, MRR is not a suitable metric.Average Precision is a metric that evaluates whether all of the ground-truth relevant items selected by the model are ranked higher or not. Unlike MRR, it considers all the relevant items.Mathematically, it is given by:where:For our example, we can calculate the AP based on our Precision@K values for different K.To illustrate the advantage of AP, let\u2019s take our previous example but place the 3 relevant results at the beginning. We can see that this gets a perfect AP score than the above example.If we want to evaluate average precision across multiple queries, we can use the MAP. It is simply the mean of the average precision for all queries. Mathematically, this is given bywhereLet\u2019s take another toy example where we annotated the items not just as relevant or not-relevant but instead used a grading scale between 0 to 5 where 0 denotes least relevant and 5 denotes the most relevant.\nWe have a ranking model that gives us back 5-most relevant results for a certain query. The first item had a relevance score of 3 as per our ground-truth annotation, the second item has a relevance score of 2 and so on.Let\u2019s understand the various metrics to evaluate this type of setup.This metric uses a simple idea to just sum up the relevance scores for top-K items. The total score is called cumulative gain. Mathematically, this is given by:For our example, CG@2 will be 5 because we add the first two relevance scores 3 and 2.Similarly, we can calculate the cumulative gain for all the K-values as:While simple, CG doesn\u2019t take into account the order of the relevant items. So, even if we swap a less-relevant item to the first position, the CG@2 will be the same.We saw how a simple cumulative gain doesn\u2019t take into account the position. But, we would normally want items with a high relevance score to be present at a better rank.Consider an example below. With the cumulative gain, we are simply adding the scores without taking into account their position.An item with a relevance score of 3 at position 1 is better than the same item with relevance score 3 at position 2.So, we need some way to penalize the scores by their position. DCG introduces a log-based penalty function to reduce the relevance score at each position. For 5 items, the penalty would beUsing this penalty, we can now calculate the discounted cumulative gain simply by taking the sum of the  . Mathematically, this is given by:To understand the behavior of the log-penalty, let\u2019s plot ranking position in x-axis and the percentage of relevance score i.e. \\(\\frac{1}{log_{2}(i+1)} * 100\\) in the y-axis. As seen, in position 1, we don\u2019t apply any penalty and score remains unchanged. But, the percentage of score kept decays exponentially from 100% in position 1 to 63% in position 2, 50% in position 3, and so on.Let\u2019s now calculate DCG for our example.Based on these penalized scores, we can now calculate DCG at various k values simply by taking their sum up to k.There is also an alternative formulation for DCG@K that gives more penalty if relevant items are ranked lower. This formulation is preferred more in industry.While DCG solves the issues with cumulative gain, it has a limitation. Suppose we a query Q1 with 3 results and query Q2 with 5 results. Then the query with 5 results Q2 will have a larger overall DCG score. But we can\u2019t say that query 2 was better than query 1.To allow a comparison of DCG across queries, we can use NDCG that normalizes the DCG values using the ideal order of the relevant items.Let\u2019s take our previous example where we had already calculated the DCG values at various K values.For our example, ideally, we would have wanted the items to be sorted in descending order of relevance scores.Let\u2019s calculate the ideal DCG(IDCG) for this order.Now we can calculate the NDCG@k for various k by diving DCG@k by IDCG@k as shown below:Thus, we get NDCG scores with a range between 0 and 1. A perfect ranking would get a score of 1. We can also compare NDCG@k scores of different queries since it\u2019s a normalized score.Thus, we learned about various evaluation metrics for both binary and graded ground-truth labels and how each metric improves upon the previous.", "meta": {"title": "Evaluation Metrics For Information Retrieval"}}
{"text": "While Flask has become the de-facto choice for API development in Machine Learning projects, there is a new framework called FastAPI that has been getting a lot of community traction.I recently decided to give FastAPI a spin by porting a production Flask project. It was very easy to pick up FastAPI coming from Flask and I was able to get things up and running in just a few hours.The added benefit of automatic data validation, documentation generation and baked-in best-practices such as pydantic schemas and python typing makes this a strong choice for future projects.In this post, I will introduce FastAPI by contrasting the implementation of various common use-cases in both Flask and FastAPI.\nAt the time of this writing, the Flask version is 1.1.2 and the FastAPI version is 0.58.1\nBoth Flask and FastAPI are available on PyPI. For conda, you need to use the  channel to install FastAPI while it\u2019s available in the default channel for Flask.Now you can run the development server using the below command. It runs on port 5000 by default.FastAPI defers serving to a production-ready server called . We can run it in development mode with a default port of 8000.For a production server,  is a common choice in Flask.FastAPI defers serving to a production-ready server called . We can start the server as:You can also start it in hot-reload mode by runningFurthermore, you can change the port as well.The number of workers can be controlled as well.You can use  to manage uvicorn as well using the following command. All regular gunicorn flags such as number of workers() work.You have individual decorator methods for each HTTP method.We want to get the user id from the URL e.g.  and then return the user id to the user.In FastAPI, we make use of type hints in Python to specify all the data types. For example, here we specify that  should be an integer. The variable in the URL path is also specified similar to f-strings.We want to allow the user to specify a search term by using a query string  in the URL.Let\u2019s take a toy example where we want to send a JSON POST request with a  key and get back a lowercased version.\nIf you simply replicate the functionality from Flask, you can do it as follows in FastAPI.But, this is where FastAPI introduces a new concept of creating Pydantic schema that maps to the JSON data being received. We can refactor the above example using pydantic as:As seen, instead of getting a dictionary, the JSON data is converted into an object of the schema . As such, we can access the data using data attributes such as . This also provides automatic validation of data types. If the user tries to send any data other than a string, they will be given an auto-generated validation error.Let\u2019s create an API to return the uploaded file name. The key used when uploading the file will be .\nFlask allows accessing the uploaded file via the request object.\nFastAPI uses function parameter to specify the file key.We want to access a text form field that\u2019s defined as shown below and echo the value.\nFlask allows accessing the form fields via the request object.\nWe use function parameter to define the key and data type for the form field.We can also make the form field optional as shown belowSimilarly, we can set a default value for the form field as shown below.We want to access a cookie called  from the request.\nFlask allows accessing the cookies via the request object.\nWe use parameter to define the key for the cookie.We want to decompose the views from a single app.py into separate files.\nIn Flask, we use a concept called blueprints to manage this. We would first create a blueprint for the user view as:Then, this view is registered in the main  file.\nIn FastAPI, the equivalent of a blueprint is called a router. First, we create a user router as:Then, we attach this router to the main app object as:\nFlask doesn\u2019t provide any input data validation feature out-of-the-box. It\u2019s common practice to either write custom validation logic or use libraries such as  or .FastAPI wraps pydantic into its framework and allow data validation by simply using a combination of pydantic schema and python type hints.This code will perform automatic validation to ensure  is a string and  is an integer. If any other data type is sent, it auto-generates validation error with a relevant message.Here are some examples of pydantic schema for common use-cases.You can learn more about Python Type hints from .\nFlask doesn\u2019t provide any built-in feature for documentation generation. There are extensions such as  or  to fill that gap but the workflow is comparatively complex.\nFastAPI automatically generates an interactive swagger documentation endpoint at  and a reference documentation at .For example, say we had a simple view given below that echoes what the user searched for.If you run the server and goto the endpoint , you will get an auto-generated swagger documentation.You can interactively try out the API from the browser itself.In addition to swagger, if you goto the endpoint , you will get an auto-generated reference documentation. There is information on parameters, request format, response format and status codes.\n\nFlask doesn\u2019t provide CORS support out of the box. We need to use extension such as  to configure CORS as shown below.\nFastAPI provides a  to handle CORS. We show an example of CORS below where we are allowing any origin to access our APIs.Thus, FastAPI is an excellent alternative to Flask for building robust APIs with best-practices baked in. You can refer to the  to learn more.", "meta": {"title": "FastAPI for Flask Users"}}
{"text": "Colab is one of the best products to come from Google. It has made GPUs freely accessible to learners and practitioners like me who otherwise wouldn\u2019t be able to afford a high-end GPU.While the interface is very easy to use, there are many lesser-known and undocumented features in colab. In this post, I will share those features that I\u2019ve discovered from basic usage and their official talks.It\u2019s a pretty common scenario that we have a bunch of cluttered untitled notebooks created when we try out temporary stuff on colab.\nTo solve this, you can bookmark the link given below. It will open a special  and any changes you make to that notebook are not saved to your main account.It\u2019s pretty common that we manually calculate the difference between start and end times of a piece of code to gauge the time taken.Colab provides an inbuilt feature to do this. After a cell is executed, just hover over the cell run icon and you will get an estimate of the execution time taken.You can also run only a part of the cell by selecting it and pressing the  button or using the keyboard shortcut .If you are familiar with keyboard shortcuts from Jupyter Notebook, they don\u2019t work directly in Colab. But I found a mental model to map between them.Just add  before whatever keyboard shortcut you were using in Jupyter. This rule of thumb works for the majority of common use-cases.Below are some notable exceptions to this rule for which either the shortcut is changed completely or kept the same.Similar to an IDE, you can go to a class definition by pressing  and then clicking a class name. For example, here we view the class definition of the Dense layer in Keras by pressing Ctrl and then clicking the  class name.The Google Colab team provides an official chrome extension to open notebooks on GitHub directly on colab. You can install it from .After installation, click the colab icon on any GitHub notebook to open it directly.Alternatively, you can also manually open any GitHub notebook by replacing  with .https:///fastai/course-v3/blob/master/nbs/dl1/00_notebook_tutorial.ipynbtohttps:///fastai/course-v3/blob/master/nbs/dl1/00_notebook_tutorial.ipynbAn even easier way is to replace  with . It will redirect you to a colab notebook.https:///fastai/course-v3/blob/master/nbs/dl1/00_notebook_tutorial.ipynbtohttps:///fastai/course-v3/blob/master/nbs/dl1/00_notebook_tutorial.ipynbWith a library called , you can easily expose a Flask web app running on colab to demo prototypes. First, you need to install  and .Then, you just need to pass your flask app object to  function and it will expose a ngrok endpoint when the server is started.You can try this out from the package author\u2019s  on Colab.You can easily switch between Tensorflow 1 and Tensorflow 2 using this magic flag. \nTo switch to Tensorflow 1.15.2, use this command:To switch to Tensorflow 2.2, run this command:You will need to restart the runtime for the effect to take place. Colab recommends using the pre-installed Tensorflow version instead of installing it from  for performance reasons.Colab also provides a magic command to use Tensorboard directly from the notebook. You just need to set the logs directory location using the  flag. You can learn to use it from the .Colab provides the following specs for their free and pro versions. Based on your use case, you can switch to the pro version at $10/month if you need a better runtime, GPU, and memory.You can view the GPU you have been assigned by running the following commandFor information on the CPU, you can run this commandSimilarly, you can view the RAM capacity by runningThere is no built-in interactive terminal in Colab. But you can use the  command to try out shell commands interactively. Just run this command and you will get an interactive input.Now, you can run any shell command in the given input box.To quit from the shell, just type  in the input box.Colab provides an indicator of RAM and disk usage. If you hover over the indicator, you will get a popup with the current usage and the total capacity.You can add a \u2018Open in Colab\u2019 badge to your  or jupyter notebooks using the following markdown code.\nIn the markdown code, we\u2019re loading an SVG image and then linking it to a colab notebook.Colab provides a notebook extension to add interactive sorting and filtering capabilities to pandas dataframes. To use it, run the following code.You can see the regular pandas dataframe and the interactive dataframe after loading the extension below.\n\nIf you use miniconda as your python environment manager, you can setup it on colab by running these commands at the top of your notebook.After the cell is executed, you can use conda to install packages as usual.Alternatively, you can use  package to install it easily.Then, run these python commands to install miniconda.You can use a library called  to easily create and sync colab notebooks with your local notebooks.There are use-cases when we need to start some web server or background tasks before we can execute our regular program.To run background tasks, use the  command followed by your regular shell command and add  to the end to run it in the background. This makes sure that you can run cells afterward in the notebook without your background task blocking it.If you\u2019re running a long task such as training a model, you can setup Colab to send a desktop notification once it\u2019s completed.To enable that, goto Tools \u2b95 Settings \u2b95 Site and enable  checkbox.You will get a popup to enable browser notification. Just accept it and colab will notify you on task completion even if you are on another tab, window or application.You can run javascript code by using the  magic command.You can run a full-fledged VSCode editor on Colab by following the method I have explained in another .You can save your own collections of useful snippets and access them easily in any colab notebook.Create a colab notebook called . To add each of your snippets, create a markdown cell and add name of the snippet as header. Below, the markdown cell, add a code cell with the snippet code.Copy the link of this notebook from the browser tab.Click  in your menu bar to open preference of colab.\nPaste the link into the  textbox and click save.You can start a JupyterLab instance on colab by running the following commands in a cell.Once executed, click the printed ngrok URL to access the JupyterLab interface.You can use R programming language in Google Colab by going to . It will open a new notebook with R set as the kernel instead of Python.", "meta": {"title": "Google Colab Tips for Power Users"}}
{"text": "In my , we explored a contrastive learning approach to zero-shot text classification. In this post, we will explore a different approach based on text generation. This approach was proposed by Puri et al. in their paper . The paper was also presented in the \u201c3rd Workshop on Meta-Learning\u201d at NeurIPS 2019.The goal of zero-shot text classification is to design a general and flexible approach that can generalize to new classification tasks without the need for task-specific classification heads.Build a text classification model that can classify classes on a new dataset it was never trained on.In the paper, the authors reformulate text classification as a text generation problem. Instead of classifying a text into X classes, the model needs to generate the correct class when given a text and the classes in a multiple-choice question answering format. Both the input and the output of the model are in natural language.Let\u2019s understand how the authors implemented this idea in a step-by-step process:As seen in the formulation above, we need to teach GPT-2 to pick the correct class when given the problem as a multiple-choice problem. The authors teach GPT-2 to do this by fine-tuning on a simple pre-training task called title prediction.In the original GPT-2 paper, the training data was prepared by scraping outbound web links that were submitted or commented on Reddit and had a minimum of 3 karma score.In the current paper, the authors build upon this idea with the  dataset. Since we can know the subreddit the link was posted in and the submission title the user used, this metadata can be collected and used as the supervision signal.\nFor multiple submissions of the same link, subreddits and submission titles can be aggregated. Thus, we have pairs of webpage text, submission title, and subreddit name as annotations.The authors found subreddit prediction didn\u2019t generalize well and so they use submission title in their experiments.To feed the annotated data into GPT-2, the authors prepared 26 different multiple-choice question format. A random question format is sampled during training.\nNow for each document, we randomly choose between 2 to 15 titles. One title is correct for that document while all others are random titles.We also add regularization by replacing a title with \u201cnone of the above\u201d 50% of the time. And the correct title is also replaced with \u201cnone of the above\u201d with a probability 1/(number of titles). Such noise can help train the model to choose \u201cnone of the above\u201d if none of the choices match the content.As shown below, the  are placed after the  as a comma-separated list.The question is prepended to the document to simulate a multiple-choice question answering task and a pre-trained GPT-2 language model is fine-tuned on this dataset to learn the submission title prediction task.From the previous step, we have a model that has been trained on a wide variety of titles from the web and thus simulates meta-learning with N-way text classification tasks.To test the zero-shot capabilities of the model, the authors tested it on 6 benchmark datasets without doing any finetuning.For each dataset, they perform the following steps:They convert the classes in each dataset into the same multiple-choice question format as pre-training and prepend it to the text. For example, for SST-2 dataset which contains movie reviews, the format would be:The question is prepended to the text and passed to GPT-2 as a prompt. Then we use greedy sampling to generate the output from GPT-2 and compare it with the actual class. Accuracy for each dataset is calculated.Even without access to the training data, the model was able to achieve up to 45% improvement in classification accuracy over random and majority class baselines.For sentiment datasets such as SST-2, Amazon-2, and Yelp-2, the larger size 335M GPT-2 model has a significant improvement over the random and majority class baselines. Zero-shot performance is still below direct finetuning and the SOTA held by XLNET.Increasing the model size from 117M to 355M parameters leads to better zero-shot performance on downstream tasks.When pretraining is done on the only 1/4th of the total data, it leads to a decrease in overall performance. This shows that pretraining across a diverse set of tasks is needed and a larger dataset provides that.For datasets like DBPedia, AGNews, and Yahoo Answer with many classes, the model performs noticeably better than random but struggles to break past 50% accuracy. The authors say this could be because the model can identify unlikely classes, but struggle to choose between most plausible options due to lack of any supervision. Also, performance is better with less data than with full dataset pretraining for them.The authors point out that there were controllability issues because GPT-2 was generating answers which were not a valid class. For example, for the yahoo answers dataset, valid classes are \u201ceducation & reference\u201d and \u201cscience and mathematics\u2019. But, the model sometimes mixed these two and generated \u2018education and mathematics\u2019. This problem diminished as the model size was increased to 355M and full data was used.The paper provides a good overview of the method and challenges of using generative language models for zero-shot classification and show that natural language could be a promising meta-learning strategy for text problems.", "meta": {"title": "Zero-shot Text Classification With Generative Language Models"}}
{"text": "I recently completed the UC Berkeley\u2019s  course. The course had an interesting  on the history of language modeling by Alec Radford, the author of GPT model.In one of his slides, Alec mentions how by simply observing a bunch of strings, language models tend to capture useful knowledge. He also mentions that maybe in the future, we could have an unsupervised language model that can be directly used on tasks without further fine-tuning. This talk was before GPT-3 was released and GPT-3 has shown the few-shot learning ability of language models.In this post, I will share my exploration of the simple examples he mentioned in the lecture with code and expand more on them.In language modeling, we want to learn a function that can observe a bunch of strings and then compute the probability for new strings. For example, the function can give us how likely this sentence is:There are many ways you could formulate this function. Here are some:We could use an RNN and variants to keep track of the previous context in a hidden state.Let\u2019s take GPT-2 as a language model and explore what it has learned by just observing a bunch of strings over the internet.We will use the  library to calculate the probability of a sentence using transformer-based language models.Let\u2019s create a scorer function that gives us a probability of a sentence using the GPT-2 language model.Now, we can use it for any sentence as shown below and it returns the probability.A language model has no prior knowledge of grammar rules and structure. But it has been exposed to a bunch of grammatically correct sentences in the large training corpus. Let\u2019s explore how much grammar it has picked up.The language model assigns a higher probability to sentence with the correct order of subject, verb, and object than an incorrect one.We have two similar sentences given below. Sentence 2 has a grammatical mistake.We would want our language model to assign more probability to the correct sentence 1. Let\u2019s verify if this is the case with GPT-2.The language model indeed assigns more probability to the gramatically correct sentence.The text corpus a language model is trained on contains lots of facts about the world. Can a language model pick that up? Let\u2019s see an example.Who does GPT-2 think is more probable to sit on a mat: cat or the hyena?It\u2019s the cat. This makes sense as cats are domesticated and hyena is a wild animal.Alec presents another idea where we find the conditional probability of positive/negative opinion following some text to perform sentiment analysis. For example, we could calculate the probability for \u201cSentiment: Positive.\u201d and \u201cSentiment: Negative.\u201d coming after a text and assign the sentiment as positive or negative respectively.Let\u2019s build a function to compute the two scores and return the sentiment based on whichever is higher.We can try with a few sentences.Since these models are trained on human-written text in the wild, they are bound to capture the inherent bias in these text. Here are some examples:The model finding it more probable for gender to be \u201che\u201d for doctor and scientist and \u201cshe\u201d for nurse.", "meta": {"title": "Exploring Knowledge Captured in Probability of Strings"}}
{"text": "Word Embeddings are one of the most interesting aspects of the Natural Language Processing field. When I first came across them, it was intriguing to see a simple recipe of unsupervised training on a bunch of text yield representations that show signs of syntactic and semantic understanding.In this post, we will explore a word embedding algorithm called \u201cFastText\u201d that was introduced by  and understand how it enhances the Word2Vec algorithm from 2013.Suppose we have the following words and we want to represent them as vectors so that they can be used in Machine Learning models.Ronaldo, Messi, DicaprioA simple idea could be to perform a one-hot encoding of the words, where each word gets a unique position.We can see that this sparse representation doesn\u2019t capture any relationship between the words and every word is isolated from each other.Maybe we could do something better. We know Ronaldo and Messi are footballers while Dicaprio is an actor. Let\u2019s use our world knowledge and create manual features to represent the words better.This is better than the previous one-hot-encoding because related items are closer in space.\nWe could keep on adding even more aspects as dimensions to get a more nuanced representation.But manually doing this for every possible word is not scalable. If we designed features based on our world knowledge of the relationship between words, can we replicate the same with a neural network?Can we have neural networks comb through a large corpus of text and generate word representations automatically?This is the intention behind the research in word-embedding algorithms.In 2013,  introduced an efficient method to learn vector representations of words from large amounts of unstructured text data. The paper was an execution of this idea from Distributional Semantics.You shall know a word by the company it keeps - J.R. Firth 1957Since similar words appear in a similar context, Mikolov et al. used this insight to formulate two tasks for representation learning.The first was called \u201c\u201d where need to predict the center words given the neighbor words. \n\nThe second task was called \u201c\u201d where we need to predict the neighbor words given a center word.\nRepresentations learned had interesting properties such as this popular example where arithmetic operations on word vectors seemed to retain meaning. \nWhile Word2Vec was a game-changer for NLP, we will see how there was still some room for improvement::\nIn Word2Vec, an embedding is created for each word. As such, it can\u2019t handle any words it has not encountered during its training.For example, words such as \u201c\u201d and \u201c\u201d are present in the vocabulary of Word2Vec. But if you try to get embedding for the compound word \u201c\u201d, you will get an .\n:\nFor words with same radicals such as \u201ceat\u201d and \u201ceaten\u201d, Word2Vec doesn\u2019t do any parameter sharing. Each word is learned uniquely based on the context it appears in. Thus, there is scope for utilizing the internal structure of the word to make the process more efficient.To solve the above challenges,  proposed a new embedding method called FastText. Their key insight was to use the internal structure of a word to improve vector representations obtained from the skip-gram method.The modification to the skip-gram method is applied as follows:For a word, we generate character n-grams of length 3 to 6 present in it.Then, we generate character n-grams of length n. For example, for the word \u201ceating\u201d, character n-grams of length 3 can be generated by sliding a window of 3 characters from the start of the angular bracket till the ending angular bracket is reached. Here, we shift the window one step each time.\nThus, we get a list of character n-grams for a word.\n\nExamples of different length character n-grams are given below:Since there can be huge number of unique n-grams, we apply hashing to bound the memory requirements. Instead of learning an embedding for each unique n-gram, we learn total B embeddings where B denotes the bucket size. The paper used a bucket of a size of 2 million.\n\nEach character n-gram is hashed to an integer between 1 to B. Though this could result in collisions, it helps control the vocabulary size. The paper uses the FNV-1a variant of the  function to hash character sequences to integer values.To understand the pre-training, let\u2019s take a simple toy example. We have a sentence with a center word \u201ceating\u201d and need to predict the context words \u201cam\u201d and \u201cfood\u201d.First, the embedding for the center word is calculated by taking a sum of vectors for the character n-grams and the whole word itself.\nFor the actual context words, we directly take their word vector from the embedding table without adding the character n-grams.\nNow, we collect negative samples randomly with probability proportion to the square root of the unigram frequency. For one actual context word, 5 random negative words are sampled.\nWe take dot product between the center word and the actual context words and apply sigmoid function to get a match score between 0 and 1.Based on the loss, we update the embedding vectors with SGD optimizer to bring actual context words closer to the center word but increase distance to the negative samples.\nFastText improves performance on syntactic word analogy tasks significantly for morphologically rich language like Czech and German.FastText has degraded performance on semantic analogy tasks compared to Word2Vec.\nFastText is 1.5 times slower to train than regular skipgram due to added overhead of n-grams.Using sub-word information with character-ngrams has better performance than CBOW and skip-gram baselines on word-similarity task. Representing out-of-vocab words by summing their sub-words has better performance than assigning null vectors.To train your own embeddings, you can either use the  or use the  available in gensim.Pre-trained word vectors trained on Common Crawl and Wikipedia for 157 languages are available  and variants of English word vectors are available .", "meta": {"title": "A Visual Guide to FastText Word Embeddings"}}
{"text": "With transformer models such as BERT and friends taking the NLP research community by storm, it might be tempting to just throw the latest and greatest model at a problem and declare it done. However, in industry, we have compute and memory limitations to consider and might not even have a dedicated GPU for inference.Thus, it\u2019s useful to keep simple and efficient models in your NLP problem-solving toolbox.  proposed one such model called \u201cUniversal Sentence Encoder\u201d.In this post, I will explain the core idea behind \u201cUniversal Sentence Encoder\u201d and how it learns fixed-length sentence embeddings from a mixed corpus of supervised and unsupervised data.We want to learn a model that can map a sentence to a . This vector encodes the meaning of the sentence and thus can be used for downstream tasks such as searching for similar documents.A naive technique to get sentence embedding is to average the embeddings of words in a sentence and use the average as the representation of the whole sentence. This approach has some challenges.Let\u2019s understand these challenges with some code examples using the spacy library. We first install spacy and create an  object to load the medium version of their model.\nIf we calculate the cosine similarity of documents given below using averaged word vectors, the similarity is pretty high even if the second sentence has a single word  and doesn\u2019t have the same meaning as the first sentence.\nIn this example, we swap the order of words in a sentence resulting in a sentence with a different meaning. Yet, the similarity obtained from averaged word vectors is 100%.We could fix some of these challenges with hacky manual feature engineering like skipping stop-words, weighting the words by their TF-IDF scores, adding n-grams to respect order when averaging, concatenating embeddings, stacking max pooling and averaged embeddings and so on.A different line of thought is training an end-to-end model to get us sentence embeddings:What if we could train a neural network to figure out how to best combine the word embeddings?On a high level, the idea is to design an  that summarizes any given sentence to a  sentence embedding. We use this same embedding to solve  and based on the  it makes on those, we update the sentence embedding. Since the same embedding has to work on multiple generic tasks, it will capture only the most informative features and discard noise. The intuition is that this will result in an generic embedding that transfers universally to wide variety of NLP tasks such as relatedness, clustering, paraphrase detection and text classification.Let\u2019s now dig deeper into each component of Universal Sentence Encoder.First, the sentences are converted to lowercase and tokenized into tokens using the Penn Treebank(PTB) tokenizer.This is the component that encodes a sentence into fixed-length 512-dimension embedding. In the paper, there are two architectures proposed based on trade-offs in accuracy vs inference speed.In this variant, we use the encoder part of the original transformer architecture. The architecture consists of 6 stacked transformer layers. Each layer has a self-attention module followed by a feed-forward network.The self-attention process takes word order and surrounding context into account when generating each word representation. The output context-aware word embeddings are added element-wise and divided by the square root of the length of the sentence to account for the sentence-length difference. We get a 512-dimensional vector as output sentence embedding.This encoder has better accuracy on downstream tasks but higher memory and compute resource usage due to complex architecture. Also, the compute time scales dramatically with the length of sentence as self-attention has \\(O(n^{2})\\) time complexity with the length of the sentence. But for short sentences, it is only moderately slower.In this simpler variant, the encoder is based on the architecture proposed by . First, the embeddings for word and bi-grams present in a sentence are averaged together. Then, they are passed through 4-layer feed-forward deep DNN to get 512-dimensional sentence embedding as output. The embeddings for word and bi-grams are learned during training.It has slightly reduced accuracy compared to the transformer variant, but the inference time is very efficient. Since we are only doing feedforward operations, the compute time is of linear complexity in terms of length of the input sequence.To learn the sentence embeddings, the encoder is shared and trained across a range of unsupervised tasks along with supervised training on the SNLI corpus. The tasks are as follows:The idea with original skip-thought paper from  was to use the current sentence to predict the previous and next sentence.\nIn USE, the same core idea is used but instead of LSTM encoder-decoder architecture, only an encoder based on transformer or DAN is used. USE was trained on this task using the Wikipedia and News corpus.In this task, we need to predict the correct response for a given input among a list of correct responses and other randomly sampled responses. This task is inspired by  who proposed a scalable email reply prediction architecture. This also powered the \u201cSmart Reply\u201d feature in \u201cInbox by Gmail\u201d.\nSource: The USE authors use a corpus scraped from web question-answering pages and discussion forums and formulate this task using a sentence encoder. The input sentence is encoded into a vector u. The response is also encoded by the same encoder and response embeddings are passed through a DNN to get vector v. This is done to model the difference in meaning of input and response. The dot product of this two vectors gives the relevance of an input to response.Training is done by taking a batch of K randomly shuffled input-response pairs. In each batch, for a input, its response pair is taken as the correct response and the remaining responses are treated as incorrect. Then, the dot product scores are calculated and converted to probabilities using a softmax function. Model is trained to maximize the log likelihood of the correct response for each input.In this task, we need to predict if a hypothesis entails, contradicts, or is neutral to a premise. The authors used the 570K sentence pairs from  corpus to train USE on this task.The sentence pairs are encoded using shared Transformer/DAN encoders and the output 512-dim embeddings u1 and u2 are obtained. Then, they are concatenated along with their L1 distance and their dot product(angle). This concatenated vector is passed through fully-connected layers and softmax is applied to get probability for entailment/contradiction/neutral classes.\nThe idea to learn sentence embedding based on SNLI seems to be inspired by the  paper though the authors don\u2019t cite it.Once the model is trained using the above tasks, we can use it to map any sentence into fixed-length 512 dimension sentence embedding. This can be used for semantic search, paraphrase detection, clustering, smart-reply, text classification, and many other NLP tasks.One caveat with the USE paper was that it doesn\u2019t have a section on comparison with other competing sentence embedding methods over standard benchmarks. The paper seems to be written from an engineering perspective based on learnings from products such as Inbox by Gmail and Google Books.The pre-trained models for \u201cUniversal Sentence Encoder\u201d are available via Tensorflow Hub. You can use it to get embeddings as well as use it as a pre-trained model in Keras. You can refer to my article on  to learn how to use it.Thus, Universal Sentence Encoder is a strong baseline to try when comparing the accuracy gains of newer methods against the compute overhead. I have personally used it for semantic search, retrieval, and text clustering and it provides a decent balance of accuracy and inference speed.", "meta": {"title": "Universal Sentence Encoder Visually Explained"}}
{"text": "Keras provides a powerful abstraction for recurrent layers such as RNN, GRU, and LSTM for Natural Language Processing. When I first started learning about them from the documentation, I couldn\u2019t clearly understand how to prepare input data shape, how various attributes of the layers affect the outputs, and how to compose these layers with the provided abstraction.Having learned it through experimentation, I wanted to share my understanding of the API with visualizations so that it\u2019s helpful for anyone else having troubles.Let\u2019s take a simple example of encoding the meaning of a whole sentence using an RNN layer in Keras.\nCredits: Marvel StudiosTo use this sentence in an RNN, we need to first convert it into numeric form. We could either use one-hot encoding, pretrained word vectors, or learn word embeddings from scratch. For simplicity, let\u2019s assume we used some word embedding to convert each word into 2 numbers.Now, to pass these words into an RNN, we treat each word as a time-step and the embedding as features. Let\u2019s build an RNN layer to pass these into\nAs seen above, here is what the various parameters means and why they were set as such:Thus for a whole sentence, we get a vector of size 4 as output from the RNN layer as shown in the figure. You can verify this by printing the shape of the output from the layer.As seen, we create a random batch of input data with 1 sentence having 3 words and each word having an embedding of size 2. After passing through the LSTM layer, we get back a representation of size 4 for that one sentence.This can be combined with a Dense layer to build an architecture for something like sentiment analysis or text classification.Keras provides a  parameter to control output from the RNN cell. If we set it to , what it means is that the output from each unfolded RNN cell is returned instead of only the last cell.As seen above, we get an  of size  for each word in the sentence.This can be verified by the below code where we send one sentence with 3 words and embedding of size 2 for each word. As seen, the layer gives us back 3 outputs with a vector of size 4 for each word.Suppose we want to recognize entities in a text. For example, in our text \u201cI am \u201d, we want to identify  as a .\nWe have already seen how to get output for each word in the sentence in the previous section. Now, we need some way to apply classification on the output vector from the RNN cell on each word. For simple cases such as text classification, you know how we use the  layer with  activation as the last layer.Similar to that, we can apply  layer on  from the RNN layer through a wrapper layer called TimeDistributed(). It will apply the  layer on  and give us class probability scores for the entities.As seen, we take a 3 word sentence and classify output of RNN for each word into 4 classes using . These classes can be the entities like name, person, location etc.We can also stack multiple recurrent layers one after another in Keras.We can understand the behavior of the code with the following figure:\nSince the second layer needs inputs from the first layer, we set return_sequence=True for the first SimpleRNN layer. For the second layer, we usually set it to False if we are going to just be doing text classification. If out task is NER prediction, we can set it to True in the final layer as well.\n", "meta": {"title": "A Visual Guide to Recurrent Layers in Keras"}}
{"text": "I recently came across an interesting thread on Twitter discussing a hypothetical scenario where research papers are published on GitHub and subsequent papers are diffs over the original paper. Information overload has been a real problem in ML with so many new papers coming every month.If you could represent a paper as a code diff, many papers could be compressed down to <50 lines :) The diff would also be more intuitive to read and eval standardized.Some ideas are so different that this wouldn\u2019t apply, but I think it would work well for the majority. \nThis post is a fun experiment showcasing how the commit history could look like for the BERT paper and some of its subsequent variants.\ncommit Author: Devlin et al.Date: Thu Oct 11 00:50:01 2018 +0000-Transformer Decoder+Masked Language Modeling+Next Sentence Prediction+WordPiece 30Kcommit Author: Lample et al.Date: Tue Jan 22 13:22:34 2019 +0000+Translation Language Modeling(TLM)+Causal Language Modeling(CLM)commit Author: Lee et al.Date: Fri Jan 25 05:57:24 2019 +0000+PubMed Abstracts data+PubMed Central Full Texts datacommit Author: Liu et al.Date: Thu Jan 31 18:07:25 2019 +0000+Multi-task Learningcommit Author: Beltagy et al.Date: Tue Mar 26 05:11:46 2019 +0000-BERT WordPiece Vocabulary-English Wikipedia-BookCorpus+1.14M Semantic Scholar Papers(Biomedial + Computer Science)+ScispaCy segmentation+SciVOCAB WordPiece Vocabularycommit Author: Yang et al.Date: Wed Jun 19 17:35:48 2019 +0000-Masked Language Modeling-BERT Transformer+Permutation Language Modeling+Transformer-XL+Two-stream self-attention+SentencePiece Tokenizercommit Author: Joshi et al.Date: Wed Jul 24 15:43:40 2019 +0000-Random Token Masking-Next Sentence Prediction-Bi-sequence Training+Continuous Span Masking+Span-Boundary Objective(SBO)+Single-Sequence Trainingcommit Author: Liu et al.Date: Fri Jul 26 17:48:29 2019 +0000-Next Sentence Prediction-Static Masking of Tokens+Dynamic Masking of Tokens+Byte Pair Encoding(BPE) 50K+Large batch size+CC-NEWS(76G) dataset+OpenWebText(38G) dataset+Stories(31G) datasetcommit Author: Reimers et al.Date: Tue Aug 27 08:50:17 2019 +0000+Siamese Network Structure+Finetuning on SNLI and MNLIcommit Author: Lan et al.Date: Thu Sep 26 07:06:13 2019 +0000-Next Sentence Prediction+Sentence Order Prediction+Cross-layer Parameter Sharing+Factorized Embeddings+SentencePiece Tokenizercommit Author: Sanh et al.Date: Wed Oct 2 17:56:28 2019 +0000-Next Sentence Prediction-Token-Type Embeddings-[CLS] pooling+Knowledge Distillation+Cosine Embedding Loss+Dynamic Maskingcommit Author: Martin et al.Date: Sun Nov 10 10:46:37 2019 +0000-BERT-English+ROBERTA+French OSCAR dataset(138GB)+Whole-word Masking(WWM)+SentencePiece Tokenizercommit Author: Le et al.Date: Wed Dec 11 14:59:32 2019 +0000-BERT-English+ROBERTA+fastBPE+Stochastic Depth+French dataset(71GB)+FLUE(French Language Understanding Evaluation) benchmark", "meta": {"title": "A Commit History of BERT and its Forks"}}
{"text": "While Computer Vision is making  on self-supervised learning only in the last few years, self-supervised learning has been a first-class citizen in NLP research for quite a while. Language Models have existed since the 90\u2019s even before the phrase \u201cself-supervised learning\u201d was termed. The Word2Vec paper from 2013 popularized this paradigm and the field has rapidly progressed applying these self-supervised methods across many problems.At the core of these self-supervised methods lies a framing called \u201c\u201d that allows us to use the data itself to generate labels and use supervised methods to solve unsupervised problems. These are also referred to as \u201c\u201d or \u201c\u201d. The representations learned by performing this task can be used as a starting point for our downstream supervised tasks.\nIn this post, I will provide an overview of the various pretext tasks that researchers have designed to learn representations from text corpus without explicit data labeling. The focus of the article will be on the task formulation rather than the architectures implementing them.In this formulation, we take a small chunk of the text of a certain window size and our goal is to predict the center word given the surrounding words.For example, in the below image, we have a window of size of one and so we have one word each on both sides of the center word. Using these neighboring words, we need to predict the center word.This formulation has been used in the famous \u201c\u201d approach of the  paper.In this formulation, we take a span of the text of a certain window size and our goal is to predict the surrounding words given the center word.This formulation has been implemented in the famous \u201c\u201d approach of the  paper.In this formulation, we take three consecutive sentences and design a task in which given the center sentence, we need to generate the previous sentence and the next sentence. It is similar to the previous skip-gram method but applied to sentences instead of words.This formulation has been used in the  paper.In this formulation, we take large corpus of unlabeled text and setup a task to predict the next word given the previous words. Since we already know what word should come next from the corpus, we don\u2019t need manually-annotated labels.For example, we could setup the task as left-to-right language modeling by predicting  given the previous words.We can also formulate this as predicting the  given the future words. The direction will be from right to left.This formulation has been used in many papers ranging from n-gram models to neural network models such as (Bengio et al., 2003) to .In this formulation, words in a text are randomly masked and the task is to predict them. Compared to the auto-regressive formulation, we can use context from both previous and next words when predicting the masked word.This formulation has been used in the ,  and  papers. Compared to the auto-regressive formulation, in this task, we predict only a small subset of masked words and so the amount of things learned from each sentence is lower.In this formulation, we take two consecutive sentences present in a document and another sentence from a random location in the same document or a different document.Then, the task is to classify whether two sentences can come one after another or not.It was used in the  paper to improve performance on downstream tasks that requires an understanding of sentence relations such as Natural Language Inference(NLI) and Question Answering. However, later works have questioned its effectiveness.In this formulation, we take pairs of consecutive sentences from the document. Another pair is also created where the positions of the two sentences are interchanged.The goal is to classify if a pair of sentences are in the correct order or not.It was used in the  paper to replace the \u201cNext Sentence Prediction\u201d task.In this formulation, we take a continuous span of text from the corpus and break the sentences present there. Then, the sentences positions are shuffled randomly and the task is to recover the original order of the sentences.It has been used in the  paper as one of the pre-training tasks.In this formulation, a random token in the document is chosen as the rotation point. Then, the document is rotated such that this token becomes the starting word. The task is to recover the original sentence from this rotated version.It has been used in the  paper as one of the pre-training tasks. The intuition is that this will train the model to identify the start of a document.This formulation was used in the  paper and exploits the idea that we use emoji to express the emotion of the thing we are tweeting. As shown below, we can use the emoji present in the tweet as the label and formulate a supervised task to predict the emoji when given the text.Authors of  used this concept to perform pre-training of a model on 1.2 billion tweets and then fine-tuned it on emotion-related downstream tasks like sentiment analysis, hate speech detection and insult detection.This pretext task was proposed in the  paper. The pre-training task was specifically designed to improve performance on the downstream task of abstractive summarization.The idea is to take a input document and mask the important sentences. Then, the model has to generate the missing sentences concatenated together.Source: ", "meta": {"title": "Self Supervised Representation Learning in NLP"}}
{"text": "The recent release of GPT-3 got me interested in the state of zero-shot learning and few-shot learning in NLP. While most of the zero-shot learning research is concentrated in Computer Vision, there has been some interesting work in the NLP domain as well.I will be writing a series of blog posts to cover existing research on zero-shot learning in NLP. In this first post, I will explain the paper  by Pushp et al. This paper from December 2017 was the first work to propose a zero-shot learning paradigm for text classification.Zero-Shot Learning is the ability to detect classes that the model has never seen during training. It resembles our ability as humans to generalize and identify new things without explicit supervision.For example, let\u2019s say we want to do  and  classification. Normally, we will train/fine-tune a new model for each dataset. In contrast, with zero-shot learning, you can perform tasks such as sentiment and news classification directly without any task-specific training.\nIn the paper, the authors propose a simple idea for zero-shot classification. Instead of classifying texts into X classes, they re-formulate the task as a binary classification to determine if a text and a class are related or not. \nLet\u2019s understand their formulation and end-to-end process in more detail now.The authors crawled 4.2 million  from the web and used the  for the news article as the . After crawling, they got total  as the labels. We can see how troublesome it would have been if we had to train a supervised model on .\n\nEach  was truncated to 28 words and anything shorter was padded.The paper uses word2vec pre-trained on Google News as the word embeddings for both the sentences as well as the labels.The paper proposes three different architecture to learn the relation between sentence and label embeddings.In this architecture, we take the mean of word embeddings in the sentence as the sentence embedding and concatenate it with the . This vector is then passed through a  to classify if the sentence and label are related or not.In this architecture, instead of taking the mean, the word embeddings are passed through an LSTM and the  of the network is treated as the sentence vector. It is concatenated with the  and then passed through a  to classify if the sentence and label are related or not.In this architecture, the embedding of each word in the sentence is concatenated with the . This combined embedding is passed through an LSTM and the  of the network is taken. It is then passed through a  to classify if the sentence and label are related or not.Using the crawled news headlines dataset, each headline is paired with 50% actual labels and 50% randomly selected unrelated labels. Then the model is trained using above 3 architectures with a binary cross-entropy loss with Adam optimizer.In the paper, they achieve the highest accuracy of 74% on the binary classification task with Architecture 3, followed by 72.6% on architecture 2 and 72% on architecture 1 on the separated test set of the news headlines dataset.Now, taking the trained model that can compute relatedness score of sentences with labels, the authors tested its generalization capability to unseen datasets and labels.The authors tested this process on the entire dataset and achieved 61.73%, 63% and 64.21% accuracy. In comparison, the supervised methods achieve 94.75% accuracy. The result is still interesting because without even training on a single sample, it achieves better than random accuracy.The paper proposes some really simple but clever techniques to learn the relationship between sentences and labels and achieves better than random accuracy on unseen datasets and labels. Since this was proposed in the pre-transformer era, it can be interesting to try these ideas with recent models.", "meta": {"title": "Zero Shot Learning for Text Classification"}}
{"text": "Unlike Computer Vision where using image data augmentation is standard practice, augmentation of text data in NLP is pretty rare. Trivial operations for images such as rotating an image a few degrees or converting it into grayscale doesn\u2019t change its semantics. This presence of semantically invariant transformation made augmentation an essential toolkit in Computer Vision research.I was curious if there were attempts at developing augmentation techniques for NLP and explored the existing literature. In this post, I will give an overview of the current approaches for text data augmentation based on my findings.This line of work tries to substitute words present in a text without changing the meaning of the sentence.In this technique, we take a random word from the sentence and replace it with its synonym using a Thesaurus. For example, we could use the  database for English to look up the synonyms and then perform the replacement. It is a manually curated database with relations between words. used this technique in their 2015 paper \u201cCharacter-level Convolutional Networks for Text Classification\u201d.  used a similar strategy to generate additional 10K training examples for their sentence similarity model. This technique was also used by  as one of the techniques in their pool of four random augmentations in the \u201cEasy Data Augmentation\u201d paper.For implementation, NLTK provides a programmatic  to WordNet. You can also use the . Additionally, there is a database called  containing millions of paraphrases that you can download and use programmatically.In this approach, we take pre-trained word embeddings such as Word2Vec, GloVe, FastText, Sent2Vec, and use the nearest neighbor words in the embedding space as the replacement for some word in the sentence. used this technique with GloVe embeddings in their paper \u201c\u201d to improve the generalization of their language model on downstream tasks.  used it to augment tweets needed to learn a topic model.For example, you can replace the word with the 3-most similar words and get three variations of the text.It\u2019s easy to use packages like Gensim to access pre-trained word vectors and get the nearest neighbors. For example, here we find the synonyms for the word \u2018awesome\u2019 using word vectors trained on tweets.You will get back the 5 most similar words along with the cosine similarities.Transformer models such as BERT, ROBERTA, and ALBERT have been trained on a large amount of text using a pretext task called \u201cMasked Language Modeling\u201d where the model has to predict masked words based on the context.This can be used to augment some text. For example, we could use a pre-trained BERT model, mask some parts of the text, and ask the BERT model to predict the token for the mask.Thus, we can generate variations of a text using the mask predictions. Compared to previous approaches, the generated text is more grammatically coherent as the model takes context into account when making predictions.This is easy to implement with open-source libraries such as  by Hugging Face. You can set the token you want to replace with  and generate predictions.However, one caveat of this method is that deciding which part of the text to mask is not trivial. You will have to use heuristics to decide the mask, otherwise, the generated text might not retain the meaning of the original sentence. use this idea for generating adversarial examples for text classification.\nFigure: MLM augmentation via replacement or insertion ()This augmentation method was proposed by  in the Unsupervised Data Augmentation paper. The basic idea is that words that have  are uninformative and thus can be replaced without affecting the ground-truth labels of the sentence.The words that replace the original word are chosen by calculating TF-IDF scores of words over the whole document and taking the lowest ones. You can refer to the  for this in the original paper.In this approach, we leverage machine translation to paraphrase a text while retraining the meaning.  used this method to augment the unlabeled text and learn a semi-supervised model on IMDB dataset with only 20 labeled examples. Their model outperformed the previous state-of-the-art model trained on 25,000 labeled examples.The back-translation process is as follows:You can also run back-translation using different languages at once to generate more variations. As shown below, we translate an English sentence to a target language and back again to English for three target languages: French, Mandarin, and Italian.This technique was also used in the  for the \u201cToxic Comment Classification Challenge\u201d on Kaggle. The winner used it for both training-data augmentations as well as during test-time where the predicted probabilities for English sentence along with back-translation using three languages(French, German, Spanish) were averaged to get the final prediction.For the implementation of back-translation, you can use TextBlob. Alternatively, you can also use  to apply Google Translate for free. You can also use  for back-translation.These are simple pattern matching transformations applied using regex and was introduced by  in his paper.In the paper, he gives an example of transforming verbal forms from contraction to expansion and vice versa. We can generate augmented texts by applying this.Since the transformation should not change the meaning of the sentence, we can see that this can fail in case of expanding ambiguous verbal forms like:To resolve this, the paper proposes that we allow ambiguous contractions but skip ambiguous expansion.You can find a list of contractions for the English language . For expansion, you can use the  library in Python.The idea of these methods is to inject noise in the text so that the model trained is robust to perturbations.In this method, we add spelling errors to some random word in the sentence. These spelling errors can be added programmatically or using a mapping of common spelling errors such as  for English.This method tries to simulate common errors that happen when typing on a QWERTY layout keyboard due to keys that are very near to each other. The errors are injected based on keyboard distance.This method has been used by  and the  paper. The idea is to perform replacement with words sampled from the unigram frequency distribution. This frequency is basically how many times each word occurs in the training corpus.This method has been proposed by  in their paper. The idea is to replace a random word with a placeholder token. The paper uses \u201c_\u201d as the placeholder token. In the paper, they use it as a way to avoid overfitting on specific contexts as well as a smoothing mechanism for the language model. The technique helped improve perplexity and BLEU scores.This is a naive technique where we shuffle sentences present in a training text to create an augmented version.This technique was proposed by  in their paper \u201cEasy Data Augmentation\u201d. In this technique, we first choose a random word from the sentence that is not a stop word. Then, we find its synonym and insert that into a random position in the sentence.This technique was also proposed by  in their paper \u201cEasy Data Augmentation\u201d. The idea is to randomly swap any two words in the sentence.This technique was also proposed by  in their paper \u201cEasy Data Augmentation\u201d. In this, we randomly remove each word in the sentence with some probability p.This technique was introduced by  in his paper on sentiment analysis for TASS 2019. It is inspired by the chromosome crossover operation that happens in genetics. \nIn the method, a tweet is divided into two halves and two random tweets of the same polarity(i.e. positive/negative) have their halves swapped. The hypothesis is that even though the result will be ungrammatical and semantically unsound, the new text will still preserve the sentiment.This technique had no impact on the accuracy but helped with the F1 score in the paper showing that it helps minority classes such as the Neutral class with fewer tweets.This technique has been used in the paper by . The idea is to parse and generate the dependency tree of the original sentence, transform it using rules, and generate a paraphrased sentence.\nFor example, one transformation that doesn\u2019t change the meaning of the sentence is the transformation from active voice to the passive voice of sentence and vice versa.Mixup is a simple yet effective image augmentation technique introduced by  in 2017. The idea is to combine two random images in a mini-batch in some proportion to generate synthetic examples for training. For images, this means combining image pixels of two different classes. It acts as a form of regularization during training.Bringing this idea to NLP,  modified Mixup to work with text. They propose two novel approaches for applying Mixup to text:In this method, two random sentences in a mini-batch are taken and they are zero-padded to the same length. Then, their word embeddings are combined in some proportion. The resulting word embedding is passed to the usual flow for text classification. The cross-entropy loss is calculated for both the labels of the original text in the given proportion.In this method, two sentences are taken and they are zero-padded to the same length. Then, their word embeddings are passed through LSTM/CNN encoder and we take the last hidden state as sentence embedding. These embeddings are combined in a certain proportion and then passed to the final classification layer. The cross-entropy loss is calculated based on both the labels of original sentences in the given proportion.This line of work tries to generate additional training data while preserving the class label.This technique was first proposed by Anaby-Tavor et al. in their paper . A recent paper from  evaluated this idea across multiple transformer-based pre-trained models. The problem formulation is as follows:Libraries like  and  provide simple and consistent API to apply the above NLP data augmentation methods in Python. They are framework agnostic and can be easily integrated into your pipeline.My takeaway from the literature review is that many of these NLP augmentation methods are very task-specific and their impact on performance has been studied for some particular use-cases only. It would be an interesting research to systematically compare these methods and analyze their impact on performance for many tasks.If you found this blog post useful, please consider citing it as:", "meta": {"title": "A Visual Survey of Data Augmentation in NLP"}}
{"text": "Deep Learning has shown very promising results in the area of Computer Vision. But when applying it to practical domains such as medical imaging, lack of labeled data is a major hurdle.In practical settings, labeling data is a time consuming and expensive process. Though, you have a lot of images, only a small portion of them can be labeled due to resource constraints. In such settings, we could wonder:How can we leverage the remaining unlabeled images along with the labeled images to improve the performance of our modelThe answer lies in a field called semi-supervised learning. FixMatch is a recent semi-supervised approach by  from Google Brain that improved the state of the art in semi-supervised learning(SSL). It is a simpler combination of previous methods such as UDA and ReMixMatch.In this post, we will understand FixMatch and also see how it got 78% median accuracy and 84% maximum accuracy on CIFAR-10 with just 10 labeled images.Suppose we\u2019re doing a cat vs dog classification where we have limited labeled data and a lot of unlabelled images of cats and dogs.Our usual  approach would be to just train a classifier on labeled images and ignore the unlabelled images.We know that a model should be able to handle perturbations of an image to improve generalization. So, instead of ignoring unlabeled images, we could instead apply the below approach:What if we create augmented versions of unlabeled images and make the supervised model predict those images. Since it\u2019s the same image, the predicted labels should be the same for both.Thus, even without knowing their correct labels, we can use the unlabeled images as a part of our training pipeline. This is the core idea behind FixMatch and many preceding papers it builds upon.With the intuition clear, let\u2019s see how FixMatch is applied in practice. The overall pipeline is summarized by the following figure:As seen, we train a supervised model on our labeled images with cross-entropy loss. For each unlabeled image,  and  are applied to get two images. The  is passed to our model and we get prediction over classes.The probability for the most confident class is compared to a . If it is above the , then we take that class as the ground label i.e. . Then, the  image is passed through our model to get a prediction over classes. This  is compared to ground truth  using cross-entropy loss. Both the losses are combined and the model is tuned.FixMatch borrows this idea from UDA and ReMixMatch to apply different augmentation i.e weak augmentation on unlabeled image for the pseudo-label generation and strong augmentation on unlabeled image for prediction.\nFor weak augmentation, the paper uses a standard flip-and-shift strategy. It includes two simple augmentations:This augmentation is applied with a probability of 50%. This is skipped for the SVHN dataset since those images contain digits for which horizontal flip is not relevant. In PyTorch, this can be performed using  as:This augmentation is applied up to 12.5%. In PyTorch, this can be implemented using the following code where 32 is the size of the image needed:These include augmentations that output heavily distorted versions of the input images. FixMatch applies either RandAugment or CTAugment and then applies CutOut augmentation.This augmentation randomly removes a square part of the image and fills it with gray or black color.PyTorch doesn\u2019t have a built-in implementation of Cutout but we can reuse its  transformation to apply the CutOut effect.Previous SSL work used , which trained a Reinforcement Learning algorithm to find augmentations that leads to the best accuracy on a proxy task(e.g. CIFAR-10). This is problematic since we require some labeled dataset to learn the augmentation and also due to resource requirements associated with RL.So, FixMatch uses one among two variants of AutoAugment:\nThe idea of Random Augmentation(RandAugment) is very simple.\nCTAugment was an augmentation technique introduced in the ReMixMatch paper and uses ideas from control theory to remove the need for Reinforcement Learning in AutoAugment. Here\u2019s how it works:Thus, we see that unlike RandAugment, CTAugment can learn magnitude for each transformation dynamically during training. So, we don\u2019t need to optimize it on some supervised proxy task and it has no sensitive hyperparameters to optimize.Thus, this is very suitable for the semi-supervised setting where labeled data is scarce.The paper uses wider and shallower variants of ResNet called  as the base architecture.The exact variant used is Wide-Resnet-28-2 with a depth of 28 and a widening factor of 2. This model is two times wider than the ResNet. It has a total of 1.5 million parameters. The model is stacked with an output layer with nodes equal to the number of classes needed(e.g. 2 classes for cat/dog classification).We prepare batches of the labeled images of size B and unlabeled images of batch size \\(\\color{#774cc3}{\\mu} B\\). Here \\(\\color{#774cc3}{\\mu}\\) is a hyperparameter that decides the relative size of labeled: unlabeled images in a batch. For example, \\(\\color{#774cc3}{\\mu}=2\\) means that we use twice the number of unlabeled images compared to labeled images.The paper tried increasing values of \\(\\color{#774cc3}{\\mu}\\) and found that as we increased the number of unlabeled images, the error rate decreases. The paper uses \\(\\color{#774cc3}{\\mu} = 7\\) for evaluation datasets.Source: FixMatch paper\nFor the supervised part of the pipeline which is trained on s, we use the regular  for classification task. The total loss for a batch is defined by \\(l_s\\) and is calculated by taking average of  for  in the batch.\nFor the unlabeled images, first we apply  to the  and get the  by applying . This is the  that will be compared with output of model on strongly augmented image.\nNow, the same  is  and it\u2019s output is compared to our  to compute . The total unlabeled batch loss is denoted by \\(l_u\\) and given by:Here \\(\\color{#d11e77}{\\tau}\\) denotes the  above which we take a pseudo-label. This loss is similar to the pseudo-labeling loss. The difference is that we\u2019re using weak augmentation to generate labels and strong augmentation for loss.\nWe finally combine these two losses to get a total loss that we optimize to improve our model. \\(\\lambda_u\\) is a fixed scalar hyperparameter that decides how much both the unlabeled image loss contribute relative to the labeled loss.An interesting result comes from \\(\\lambda_u\\). Previous works have shown that increasing weight during training is good. But, in FixMatch, this is present in the algorithm itself.Since initially, the model is not confident on labeled data, so its output predictions on unlabeled data will be below the threshold. As such, the model will be trained only on labeled data. But as the training progress, the model becomes more confident in labeled data and as such, predictions on unlabeled data will also start to cross the threshold. As such, the loss will soon start incorporating predictions on unlabeled images as well. This gives us a free form of curriculum learning.Intuitively, this is similar to how we\u2019re taught in childhood. In the early years, we learn easy concepts such as alphabets and what they represent before moving on to complex topics like word formation, sentence formation, and then essays.The authors performed a really interesting experiment on the CIFAR-10 dataset. They trained a model on CIFAR-10 using only 10 labeled images i.e. 1 labeled example of each class.The authors ran evaluations on datasets commonly used for SSL such as CIFAR-10, CIFAR-100, SVHN, STL-10, and ImageNet.\nFixMatch achieves the state of the art results on CIFAR-10 and SVHN benchmarks. They use 5 different folds for each dataset.\nOn CIFAR-100, ReMixMatch is a bit superior to FixMatch. To understand why the authors borrowed various components from ReMixMatch to FixMatch and measured their impact on performance.They found that the  component which encourages the model to emit all classes with equal probability was the cause. So, when they combined FixMatch with DA, they achieved a 40.14% error rate compared to a 44.28% error rate of ReMixMatch.\nSTL-10 dataset consists of 100,000 unlabeled images and 5000 labeled images. We need to predict 10 classes(airplane, bird, car, cat, deer, dog, horse, monkey, ship, truck.). It is a more representative evaluation for semi-supervised learning because its unlabeled set has out-of-distribution images.FixMatch achieves the lowest error rate with CTAugment when evaluated on 5-folds of 1000 labeled images each among all methods.\nThe authors also evaluate the model on ImageNet to verify if it works on large and complex datasets. They take 10% of the training data as labeled images and all remaining 90% as unlabeled. Also, the architecture used is ResNet-50 instead of WideResNet and RandAugment is used as a strong augmentation.They achieve a top-1 error rate of \\(28.54\\pm0.52%\\) which is \\(2.68\\%\\) better than UDA. The top-5 error rate is \\(10.87\\pm0.28\\%\\).The official implementation of FixMatch in Tensorflow by the paper authors is available .Unofficial implementations of FixMatch paper in PyTorch are available on GitHub (, , ). They use RandAugment and are evaluated on CIFAR-10 and CIFAR-100.The paper is available here: .If you found this blog post useful, please consider citing it as:", "meta": {"title": "The Illustrated FixMatch for Semi-Supervised Learning"}}
{"text": "PyTorch has emerged as one of the go-to deep learning frameworks in recent years. This popularity can be attributed to its easy to use API and it being more \u201cpythonic\u201d.PyTorch leverages numerous native features of Python to give us a consistent and clean API. In this article, I will explain those native features in detail. Learning these will help you better understand why you do things a certain way in PyTorch and make better use of what it has to offer.Layers such as  are some of the basic constructs in PyTorch that we use to build our models. You import the layer and apply them to tensors.Here we are able to call layer on some tensor , so it must be a function right? Is  returning a function? Let\u2019s verify it by checking the type.Surprise!  is actually a class and layer an object of that class.\u201cWhat! How could we call it then? Aren\u2019t only functions supposed to be callable?\u201dNope, we can create callable objects as well. Python provides a native way to make objects created from classes callable by using magic functions.\nLet\u2019s see a simple example of a class that doubles a number.Here we add a magic method  in the class to double any number passed to it. Now, you can create an object out of this class and call it on some number.Alternatively, the above code can be combined in the single line itself.This works because everything in Python is an object. See an example of a function below that doubles a number.Even functions invoke the method behind the scenes.Let\u2019s see an example of a model that applies a single fully connected layer to MNIST images to get 10 outputs.The following code should be familiar to you. We are computing output of this model on some tensor x.We know calling the model directly on some tensor executes the  function on it. How does that work?It\u2019s the same reason in previous example. We\u2019re inheriting the class . Internally,  has a  magic method that calls the . So, when we override  method later, it\u2019s executed.Thus, we were able to call the model directly on tensors.In PyTorch, it is common to create a custom class inheriting from the  class to prepare our training and test datasets. Have you ever wondered why we define methods with obscure names like  and  in it?These methods are builtin magic methods of Python. You know how we can get the length of iterables like list and tuples using  function.Python allows defining a  on our custom class so that  works on it. For example,Similarly, you know how we can access elements of list and tuples using index notation.Python allows a  magic method to allow such functionality for custom classes. For example,With the above concept, now you can easily understand the builtin dataset like MNIST and what you can do with them.Let\u2019s create a dataloader for a training dataset of MNIST digits.Now, let\u2019s try accessing first batch from the data loader directly without looping. If we try to access it via index, we get an exception.You might have been used to doing it in this way.Have you ever wondered why do we wrap trainloader by  and then call ? Let\u2019s demystify this.Consider a list  with 3 elements. In Python, we can create an iterator out of  using the  function.Iterators are used because they allow lazy loading such that only one element is loaded in memory at a time.We get each element and when we reach the end of the list, we get a  exception.This pattern matches our usual machine learning workflow where we take small batches of data at a time in memory and do the forward and backward pass. So,  also incorporates this pattern in PyTorch.To create iterators out of classes in Python, we need to define magic methods  and Here, the  function calls the  magic method of the class returning that same object. Then, the  function calls the  magic method of the class to return next element present in our data.In PyTorch, the implementation of DataLoader implements this pattern as follows:So, they decouple the iterator creation part and the actual data loading part.Thus, we get images and labels for a single batch.Thus, we saw how PyTorch borrows several advanced concepts from native Python itself in its API design. I hope the article was helpful to demystify how these concepts work behind the scenes and will help you become a better PyTorch user.", "meta": {"title": "The Python Magic Behind PyTorch"}}
{"text": "Many self-supervised methods use  to generate surrogate labels and formulate an unsupervised learning problem as a supervised one. Some examples include rotation prediction, image colorization, jigsaw puzzles etc. However, such pretext tasks are domain-dependent and require expertise to design them. is a self-supervised method proposed by Caron et al. of Facebook AI Research that brings a different approach.\nThis method doesn\u2019t require domain-specific knowledge and can be used to learn deep representations for scenarios where annotated data is scarce.DeepCluster combines two pieces: unsupervised clustering and deep neural networks. It proposes an end-to-end method to jointly learn parameters of a deep neural network and the cluster assignments of its representations. The features are generated and clustered iteratively to get both a trained model and labels as output artifacts.Let\u2019s now understand how the deep cluster pipeline works with an interactive diagram.\nAs seen in the figure above, unlabeled images are taken and  are applied to them. Then, an  architecture such as  or  is used as the feature extractor. Initially, the  is initialized with randomly weights and we take the  from layer before the final classification head. Then,  is used to reduce the dimension of the  along with whitening and . Finally, the processed features are passed to  to get cluster assignment for each image.These cluster assignments are used as the  and the  is trained to predict these clusters. Cross-entropy loss is used to gauge the performance of the model. The model is trained for 100 epochs with the  step occurring once per epoch. Finally, we can take the  learned and use it for downstream tasks.Let\u2019s see how DeepCluster is applied in practice with a step by step example of the whole pipeline from the input data to the output labels:\nWe take unlabeled images from the ImageNet dataset which consist of 1.3 million images uniformly distributed into 1000 classes. These images are prepared in mini-batches of 256.\nThe training set of N images can be denoted mathematically by:Transformations are applied to the images so that the features learned is invariant to augmentations. Two different augmentations are done, one when training model to learn representations and one when sending the image representations to the clustering algorithm:When model representations are to be sent for clustering, random augmentations are not used. The image is simply resized to 256*256 and the center crop is applied to get 224*224 image. Then normalization is applied.\nIn PyTorch, this can be implemented as:When the model is trained on image and labels, then we use random augmentations. The image is cropped to a random size and aspect ratio and then resized to 224*224. Then, the image is horizontally flipped with a 50% chance. Finally, we normalize the image with ImageNet mean and std.\n\nIn PyTorch, this can be implemented as:Once we get the normalized image, we convert it into grayscale. Then, we increase the local contrast of the image using the Sobel filters.\nBelow is a simplified snippet adapted from the author\u2019s implementation . We can apply it on the augmented image  we got above.To perform clustering, we need to decide the number of clusters. This will be the number of classes the model will be trained on.\nBy default, ImageNet has 1000 classes, but the paper uses 10,000 clusters as this gives more fine-grained grouping of the unlabeled images. For example, if you previously had a grouping of cats and dogs and you increase clusters, then groupings of breeds of the cat and dog could be created.The paper primarily uses AlexNet architecture consisting of  and 3 fully connected layers. The Local Response Normalization layers are removed and Batch Normalization is applied instead. Dropout is also added. The filter size used is from 2012 competition: 96, 256, 384, 384, 256.Alternatively, the paper has also tried replacing AlexNet by VGG-16 with batch normalization to see impact on performance.To generating initial labels for the model to train on, we initialize AlexNet with random weights and the last fully connected layer FC3 removed. We perform a forward pass on the model on images and take the feature vector coming from the second fully connected layer FC2 of the model on an image. This feature vector has a dimension of 4096.This process is repeated for all images in the batch for the whole dataset. Thus, if we have N total images, we will have an image-feature matrix of [N, 4096].Before performing clustering, dimensionality reduction is applied to the image-feature matrix.For dimensionality reduction, Principal Component Analysis(PCA) is applied to the features to reduce them from 4096 dimensions to 256 dimensions. The values are also whitened. \nThe paper uses the  library to perform this at scale. Faiss provides an efficient implementation of PCA which can be applied for some image-feature matrix  as:Then, L2 normalization is applied to the values we get after PCA.Thus, we finally get a matrix of  for total N images. Now, K-means clustering is applied to the pre-processed features to get images and their corresponding clusters. These clusters will act as the pseudo-labels on which the model will be trained.The paper use Johnson\u2019s implementation of K-means from the paper . It is available in the faiss library. Since clustering has to be run on all the images, it takes one-third of the total training time.After clustering is done, new batches of images are created such that images from each cluster has an equal chance of being included. Random augmentations are applied to these images.Once we have the images and clusters, we train our ConvNet model like regular supervised learning. We use a batch size of 256 and use cross-entropy loss to compare model predictions to the ground truth cluster label. The model learns useful representations.The model is trained for 500 epochs. The clustering step is run once at the start of each epoch to generate pseudo-labels for the whole dataset. Then, the regular training of ConvNet using cross-entropy loss is continued for all the batches.\nThe paper uses SGD optimizer with momentum of 0.9, learning rate of 0.05 and weight decay of \\(10^{-5}\\). They trained it on Pascal P100 GPU.The official implementation of Deep Cluster in PyTorch by the paper authors is available on . They also provide  for AlexNet and Resnet-50 architectures.If you found this blog post useful, please consider citing it as:", "meta": {"title": "A Visual Exploration of DeepCluster"}}
{"text": "In the past year, several methods for self-supervised learning of image representations have been proposed. A recent trend in the methods is using Contrastive Learning (, , ) which have given very promising results.However, as we had seen in our  on self-supervised learning, there exist many other problem formulations for self-supervised learning. One promising approach is:Combine clustering and representation learning together to learn both features and labels simultaneously.A paper  presented at ICLR 2020 by Asano et al. of the Visual Geometry Group(VGG), University of Oxford has a new take on this approach and achieved the state of the art results in various benchmarks.The most interesting part is that we can  with this method and then use those labels independently with any model architecture and regular supervised learning methods. Self-Labelling is a very practical idea for industries and domains with scarce labeled data. Let\u2019s understand how it works.At a very high level, the Self-Labelling method works as follows:But, how will you generate labels for images in the first place without a trained model? This sounds like the chicken-and-egg problem where if the chicken came first, what did it hatch from and if the egg came first, who laid the egg?The solution to the problem is to use a randomly initialized network to bootstrap the first set of image labels. This has been shown to work empirically in the  paper.The authors of DeepCluster used a randomly initialized  and evaluated it on ImageNet. Since the ImageNet dataset has 1000 classes, if we randomly guessed the classes, we would get an baseline accuracy of  = . But, a randomly initialized AlexNet was shown to achieve  accuracy on ImageNet. This means that a randomly-initialized network possesses some faint signal in its weights.Thus, we can use labels obtained from a randomly initialized network to kick start the process which can be refined later.Let\u2019s now understand how the self-labelling pipeline works.\n\nAs seen in the figure above, we first generate labels for  unlabeled images using a randomly initialized model. Then, the  algorithm is applied to cluster the unlabeled images and get a new set of labels. The  is again trained on these new set of labels and optimized with cross-entropy loss.  algorithm is run once in a while during the course of training to optimize and get new set of labels. This process is repeated for a number of epochs and we get the final labels and a .Let\u2019s see how this method is implemented in practice with a step by step example of the whole pipeline from the input data to the output labels:First of all, we get N unlabeled images \\(I_1, ..., I_N\\) and take batches of them from some dataset. In the paper, batches of 256 unlabeled images are prepared from the ImageNet dataset.\nWe apply augmentations to the unlabeled images so that the self-labelling function learned is transformation invariant. The paper first randomly crops the image into size . Then, the image is converted into grayscale with a probability of 20%. Color Jitter is applied to this image. Finally, the horizontal flip is applied 50% of the time. After the transformations are applied, the image is normalized with a mean of and a standard deviation of .This can be implemented in PyTorch for some image as:We then need to choose the number of clusters(K) we want to group our data in. By default, ImageNet has 1000 classes so we could use 1000 clusters. The number of clusters is dependent on the data and can be chosen either by using domain knowledge or by comparing the number of clusters against model performance. This is denoted by:The paper experimented with the number of clusters ranging from 1000(1k) to 10,000(10k) and found the ImageNet performance improves till 3000 but slightly degrades when using more clusters than that. So the papers use 3000 clusters and as a result 3000 classes for the model.\nA ConvNet architecture such as  or  is used as the feature extractor. This  is denoted by \\(\\color{#885e9c}{\\phi(} I \\color{#885e9c}{)}\\)\nand maps an image I to  \\(m \\in R^D\\) with dimension D.Then, a  is used which is simply a single  that converts the feature vectors into class scores. These scores are converted into probabilities using the softmax operator.\nThe above model is initialized with random weights and we do a forward pass through the model to get class predictions for each image in the batch. These predicted classes are assumed as the initial labels.\nUsing these initial labels, we want to find a better distribution of images into clusters. To do that, the paper uses a novel approach quite different than K-means clustering that was used in DeepCluster. The authors apply the concept of optimal transport from operations research to tackle this problem.Let\u2019s first understand the optimal transport problem with a simple real-world example:Now, that we understand the problem, let\u2019s see how it applies in our case of cluster allocation. The authors have formulated the problem of assigning the unlabeled images into clusters as an optimal transport problem in this way::\nThe unlabeled images should be divided equally into the K clusters. This is referred to as the equipartition condition in the paper.:\nThe cost of allocating each image to a cluster is given by the model performance when trained using these clusters as the labels. Intuitively, this means the mistake model is making when we assign an unlabeled image to some cluster. If it is high, then that means our current label assignment is not ideal and so we should change it in the optimization step.We find the optimal matrix Q using a fast-variant of the Sinkhorn-Knopp algorithm. This algorithm involves a single matrix-vector multiplication and scales linearly with the number of images N. In the paper, they were able to reach convergence on ImageNet dataset within 2 minutes when using GPU to accelerate the process. For the algorithm and derivation of Sinkhorn-Knopp, please refer to the  paper. There is also an excellent blogpost by Michiel Stock that explains Optimal Transport .\nSince we have updated labels Q, we can now take predictions of the model on the images and compare it to their corresponding cluster labels with a classification cross-entropy loss. The model is trained for a fixed number of epochs and as the cross-entropy loss decrease, the internal representation learned improves.\nThe optimization of labels at step 6 is scheduled to occur at most once an epoch. The authors experimented with not using self-labelling algorithm at all to doing the Sinkhorn-Knopp optimization once per epoch. The best result was achieved at 80.\nThis shows that self-labeling is giving us a significant increase in performance compared to  (only random-initialization and augmentation).The labels obtained for images from self-labelling can be used to train another network from scratch using standard supervised training.In the paper, they took labels assigned by SeLa with AlexNet and retrained another AlexNet network from scratch with those labels using only 90-epochs to get the same accuracy.They did another interesting experiment where 3000 labels obtained by applying SeLa to ResNet-50 was used to train AlexNet model from scratch. They got  accuracy which was higher than  accuracy obtained by training AlexNet from scratch directly. This shows how labels can be transferred between architectures.The authors have published their generated labels for the ImageNet dataset. These can be used to train a supervised model from scratch.The author have also setup an interactive demo  to look at all the clusters found from ImageNet.\nThe paper got state of the art results on CIFAR-10, CIFAR-100 and SVHN datasets beating best previous method . An interesting result is very small improvement() on SVHN, which the authors say is because the difference between supervised baseline of 96.1 and AND\u2019s 93.7 is already small (<3%).\n\nThe authors also evaluated it using weighted KNN and an embedding size of 128 and outperformed previous methods by 2%.\n \nThe paper has an assumption that images are equally distributed over classes. So, to test the impact on the algorithm when it\u2019s trained on unbalanced datasets, the authors prepared three datasets out of CIFAR-10:The official implementation of Self-Labelling in PyTorch by the paper authors is available . They also provide  for AlexNet and Resnet-50.If you found this blog post useful, please consider citing it as:", "meta": {"title": "A Visual Guide to Self-Labelling Images"}}
{"text": "The end of 2019 saw a huge surge in the number of self-supervised learning research papers using contrastive learning. In December 2019,  from Facebook AI Research proposed a new method  (pronounced as \u201cpearl\u201d) for learning image representations.In this article, I will explain the rationale behind the paper and how it advances the self-supervised representation learning scene further for images. We will also see how this compares to the current SOTA approach \u201c\u201d (as of March 16, 2020) which improves shortcomings of PIRL.A number of interesting  have been proposed to learn image representations in recent times. Many of these use the idea of setting up a pretext task exploiting some  to get labels. This includes , , , ,  among many others.The pretext task is set up such that representations are learned for a transformed image to predict some property of transformation. For example, for a rotation prediction task, we randomly rotate the image by say 90 degrees and then ask the network to predict the rotation angle.As such, the image representations learned can overfit to this objective of rotation angle prediction and not generalize well on downstream tasks. The representations will be  with the transformation. It will only encode essential information to predict rotation angle and could discard useful semantic information.PIRL proposes a method to tackle the problem of representations being covariant with transformation. It proposes a problem formulation such that representations generated for both the original image and transformed image are similar. There are two goals:Intuitively this makes sense because even if an image was rotated, it doesn\u2019t change the semantic meaning that this image is still a \u201c\u201dPIRL defines a generic framework to implement this idea. First, you take a original image I, apply a transformation borrowed from some pretext task(e.g. rotation prediction) to get transformed image \\(I^t\\). Then, both the images are passed through ConvNet \\(\\theta\\) with shared weights to get representations \\(V_I\\) and \\(V_{I^T}\\). The representation \\(V_I\\) of original image is passed through a projection head f(.) to get representation \\(f(V_I)\\). Similarly, a separate projection head g(.) is used to get representation \\(g(V_{I^T})\\) for transformed image. These representations are tuned with a loss function such that representations of \\(I\\) and \\(I^t\\) are similar, while making it different from other random image representations \\(I'\\) stored in a memory bank.Let\u2019s assume we have a training corpus containing 3 RGB images for simplicity.Here is how PIRL works on these images step by step:To solve this problem, PIRL proposes to use a memory bank which caches representations of all images and use that during training. This allows us to use a large number of negative pairs without increasing batch size.In our example, the PIRL model is initialized with random weights. Then, a foward pass is done for all images in training data and the representation \\(f(V_I)\\) for each image is stored in memory bank.\nNow, we take mini-batches from the training data. Let\u2019s assume we take a batch of size 2 in our case.\nFor each image in batch, we apply the transformation based on the pretext task used. Here, we show the transformation for pretext task of geometric rotation prediction.Thus, for 2 images in our batch, we get two pairs and total four images.\nNow, for each image, the image and its counterpart transformation are passed through a network to get representations. The paper uses ResNet-50 as the base ConvNet encoder and we get back 2048-dimensional representation.\nThe representations obtained from encoder are passed through a single linear layer to project the representation from 2048 dimension to 128 dimension. Separate linear layers f(.) and g(.) are used for the original and transformed image respectively. We get final representation \\(f(V_I)\\) for original image and \\(g(V_{I^T})\\) for transformed image.\nCurrently, for each image, we have representations for original and transformed versions of it.\nOur goal is to produce similar representations for both while producing different representations for other images.Now, we calculate the loss in the following steps:\na. \nCosine similarity is used as a similarity measure of any two representations. Below, we are comparing the similarity of a cat image and it\u2019s rotated counterpart. It is denoted by \\(s()\\)b. \nWe use a Noise Contrastive Estimator(NCE) function to compute the similarity score of two representations normalized by all negative images.\nFor a cat image and it\u2019s rotated counterpart, the noise contrastive estimator is denoted by:Mathematically, we compute NCE over representations from the projection heads instead of representations from ResNet-50. The formulation is:The loss for a pair of images is calculated using cross-entropy loss as:Since we already have representation of image and negative images in memory bank, we use that instead of computed representation as:where \\(f(V_I)\\) is replaced by \\(m_I\\) and \\(f(V_{I'})\\) is replaced by \\(m_{I'}\\).In ideal case, similarity of image and it\u2019s transformation is highest i.e. 1 while similarity with any negative images is zero. So, loss becomes zero in that case.We see how above loss only compares \\(I\\) to \\(I^t\\) and compares \\(I^t\\) to \\(I'\\). It doesn\u2019t compare \\(I\\) and \\(I'\\). To do that, we introduce another another loss term and combine both these losses using following formulation.With this formulation, we compare image to its transformation, transformation to negative image and original image to negative image as well.Based on these losses, the encoder and projection heads improve over time and better representations are obtained. The representations for images in the memory bank for the current batch are also updated by applying exponential moving average.After the model is trained, then the projection heads \\(f(.)\\) and \\(g(.)\\) are removed and the ResNet-50 encoder is used for downstream tasks. You can either freeze the ResNet-50 model and use this as a feature extractor or you can finetune the whole network for your downstream task.The authors state two promising areas for improving PIRL and learn better image representations:An implementation of PIRL in PyTorch by Arkadiusz Kwasigroch is available . Models are available for both rotation as well as jigsaw task. It tests the performance of the network on a small dataset of 2000 skin lesions images and gets an AUC score of 0.7 compared to a random initialization score of 0.55.If you found this blog post useful, please consider citing it as:", "meta": {"title": "The Illustrated PIRL: Pretext-Invariant Representation Learning"}}
{"text": "When working on Natural Language Processing applications such as Text Classification, collecting enough labeled examples for each category manually can be difficult. In this article, I will go over an interesting technique to augment your existing text data automatically called back translation.The key idea of back translation is very simple. We create augmented version of a sentence using the following steps:\nFigure: Back TranslationWe need a machine translation service to perform the translation to a different language and back to English. Google Translate is the most popular service for this purpose, but you need to get an API key to use it and it is a paid service.Luckily, Google provides a handy feature in their Google Sheets web app, which we can leverage for our purpose.Let\u2019s assume we are building a sentiment analysis model and our dataset has sentences and their associated labels. We can load it into Google Sheets by importing the Excel/CSV file directly.Add a new column and use the  function to translate from English to French and back to English.The command to place in the column isOnce the command is placed, press Enter and you will see the translation.Now, select the first cell of \u201cBacktranslated\u201d column and drag the small square at the bottom right side below to apply this formula over the whole columnThis should apply to all your training texts and you will get back the augmented version.For texts where the original text and what get back from  are the same, we can filter them out programmatically by comparing the original text column and the augmented column. Then, only keep responses that have  value in the  column.You can download your data as a CSV file and augment your existing training data.Here is a  demonstrating all the four steps above. You can refer to that and make a copy of it to test things out.Back translation offers an interesting approach when you\u2019ve small training data but want to improve the performance of your model.", "meta": {"title": "Back Translation for Text Augmentation with Google Sheets"}}
{"text": "I first got introduced to self-supervised learning in a  by Yann Lecun, where he introduced the \u201ccake analogy\u201d to illustrate the importance of self-supervised learning. In the talk, he said:\u201cIf intelligence is a cake, the bulk of the cake is self-supervised learning, the icing on the cake is supervised learning, and the cherry on the cake is reinforcement learning (RL).\u201dThough the analogy is , we have seen the impact of self-supervised learning in the Natural Language Processing field where recent developments (Word2Vec, Glove, ELMO, BERT) have embraced self-supervision and achieved state of the art results.Curious to know the current state of self-supervised learning in the Computer Vision field, I read up on existing literature on self-supervised learning applied to computer vision through a  by Jing et. al.In this post, I will explain what is self-supervised learning and summarize the patterns of problem formulation being used in self-supervised learning with visualizations.To apply supervised learning with deep neural networks, we need enough labeled data. To acquire that, human annotators manually label data which is both a time consuming and expensive process. There are also fields such as the medical field where getting enough data is a challenge itself. Thus, a major bottleneck in current supervised learning paradigm is the label generation part.Self supervised learning is a method that poses the following question to formulate an unsupervised learning problem as a supervised one:Can we design the task in such a way that we can generate virtually unlimited labels from our existing images and use that to learn the representations?In self-supervised learning, we replace the human annotation block by  some property of data to set up a pseudo-supervised task. For example, here instead of labeling images as cat/dog, we could instead rotate them by 0/90/180/270 degrees and train a model to predict rotation. We can generate virtually unlimited training data from millions of images we have freely available on the internet.Figure: End to End Workflow of Self-Supervised LearningOnce we learn representations from these millions of images, we can use transfer learning to fine-tune it on some supervised task like image classification of cats vs dogs with very few examples.Let\u2019s now understand the various approaches researchers have proposed to exploit image and video properties and apply self-supervised learning for representation learning.Formulation:What if we prepared pairs of (grayscale, colorized) images by applying grayscale to millions of images we have freely available?We could use an encoder-decoder architecture based on a fully convolutional neural network and compute the L2 loss between the predicted and actual color images.To solve this task, the model has to learn about different objects present in the image and related parts so that it can paint those parts in the same color. Thus, representations learned are useful for downstream tasks.\n\n |  | Formulation:What if we prepared training pairs of (small, upscaled) images by downsampling millions of images we have freely available?GAN based models such as  are popular for this task. A generator takes a low-resolution image and outputs a high-resolution image using a fully convolutional network. The actual and generated images are compared using both mean-squared-error and content loss to imitate human-like quality comparison. A binary-classification discriminator takes an image and classifies whether it\u2019s an actual high-resolution image(1) or a fake generated superresolution image(0). This interplay between the two models leads to generator learning to produce images with fine details.Both generator and discriminator learn semantic features that can be used for downstream tasks.:\nFormulation:What if we prepared training pairs of (corrupted, fixed) images by randomly removing part of images?Similar to superresolution, we can leverage a GAN-based architecture where the Generator can learn to reconstruct the image while discriminator separates real and generated images.For downstream tasks,  have shown that semantic features learned by such a generator give 10.2% improvement over random initialization on the  semantic segmentation challenge while giving <4% improvements over classification and object detection.:\nFormulation:What if we predict one channel of the image from the other channel and combine them to reconstruct the original image?Zhang et al. used this idea in their paper called \u201cSplit-Brain Autoencoder\u201d. To understand the idea of the paper, let\u2019s take an example of a color image of tomato.Example adapted from \u201cSplit-Brain Autoencoder\u201d paperFor this color image, we can split it into grayscale and color channels. Then, for the grayscale channel, we predict the color channel and for the color channel part, we predict the grayscale channel. The two predicted channels \\(X_1\\) and \\(X_2\\) are combined to get back a reconstruction of the original image. We can compare this reconstruction to the original color image to get a loss and improve the model.This same setup can be applied for images with depth as well where we use the color channels and the depth channels from a RGB-HHA image to predict each other and compare output image and original image.Example adapted from \u201cSplit-Brain Autoencoder\u201d paper:\nFormulation:What if we prepared training pairs of (shuffled, ordered) puzzles by randomly shuffling patches of images?Even with only 9 patches, there can be 362880 possible puzzles. To overcome this, only a subset of possible permutations is used such as 64 permutations with the highest hamming distance.\nSuppose we use a permutation that changes the image as shown below. Let\u2019s use the permutation number 64 from our total available 64 permutations.\nNow, to recover back the original patches, \nproposed a neural network called context-free network (CFN) as shown below. Here, the individual patches are passed through the same siamese convolutional layers that have shared weights. Then, the features are combined in a fully-connected layer. In the output, the model has to predict which permutation was used from the 64 possible classes. If we know the permutation, we can solve the puzzle.\nTo solve the Jigsaw puzzle, the model needs to learn to identify how parts are assembled in an object, relative positions of different parts of objects and shape of objects. Thus, the representations are useful for downstream tasks in classification and detection.:\nFormulation:What if we prepared training pairs of (image-patch, neighbor) by randomly taking an image patch and one of its neighbors around it from large, unlabeled image collection?To solve this pre-text task,  used an architecture similar to that of a jigsaw puzzle. We pass the patches through two siamese ConvNets to extract features, concatenate the features and do a classification over 8 classes denoting the 8 possible neighbor positions.\n:\nFormulation:What if we prepared training pairs of (rotated-image, rotation-angle) by randomly rotating images by (0, 90, 180, 270) from large, unlabeled image collection?To solve this pre-text task,  propose an architecture where a rotated image is passed through a ConvNet and the network has to classify it into 4 classes(0/90/270/360 degrees).\nThough a very simple idea, the model has to understand location, types and pose of objects in an image to solve this task and as such, the representations learned are useful for downstream tasks.:\nFormulation:What if we prepared training pairs of (image, cluster-number) by performing clustering on large, unlabeled image collection?To solve this pre-text task,  propose an architecture called deep clustering. Here, the images are first clustered and the clusters are used as classes. The task of the ConvNet is to predict the cluster label for an input image.\n:Formulation:What if we prepared training pairs of (image, properties) by generating synthetic images using game engines and adapting it to real images?To solve this pre-text task,  propose an architecture where weight-shared ConvNets are trained on both synthetic and real images and then a discriminator learns to classify whether ConvNet features fed to it is of a synthetic image or a real image. Due to adversarial nature, the shared representations between real and synthetic images get better.\nFormulation:What if we prepared training pairs of (video frames, correct/incorrect order) by shuffling frames from videos of objects in motion?To solve this pre-text task,  propose an architecture where video frames are passed through weight-shared ConvNets and the model has to figure out whether the frames are in the correct order or not. In doing so, the model learns not just spatial features but also takes into account temporal features.:If you found this blog post useful, please consider citing it as:", "meta": {"title": "The Illustrated Self-Supervised Learning"}}
{"text": "Tensorflow 2.0 introduced Keras as the default high-level API to build models. Combined with pretrained models from Tensorflow Hub, it provides a dead-simple way for transfer learning in NLP to create good models out of the box.To illustrate the process, let\u2019s take an example of classifying if the title of an article is clickbait or not.We will use the dataset from the paper  available .Since the goal of this article is to illustrate transfer learning, we will directly load an already pre-processed dataset into a pandas dataframe.The dataset consists of page titles and labels. The label is 1 if the title is clickbait.Let\u2019s split the data into 70% training data and 30% validation data.Now, we install tensorflow and tensorflow-hub using pip.To use text data as features for models, we need to convert it into a numeric form. Tensorflow Hub provides various  for converting the sentences into embeddings such as BERT, NNLM and Wikiwords.Universal Sentence Encoder is one of the popular module for generating sentence embeddings. It gives back a 512 fixed-size vector for the text.\nBelow is an example of how we can use tensorflow hub to capture embeddings for the sentence \u201cHello World\u201d.In Tensorflow 2.0, using these embeddings in our models is a piece of cake thanks to the new  module. Let\u2019s design a tf.keras model for the binary classification task of clickbait detection.First import the required libraries.Then, we create a sequential model that will encapsulate our layers.The first layer will be a hub.KerasLayer from where we can loading models available at . We will be loading .Here are what the different parameters used mean:Next, we add a Dense layer with single node to output probability of clickbait between 0 and 1.In summary, we have a model that takes text data, projects it into 512-dimension embedding and passed that through a feedforward neural network with sigmoid activation to give a clickbait probability.Alternatively, we can implement the exact above architecture using the tf.keras functional API as well.The output of the model summary isThe number of trainable parameters is  because we\u2019re finetuning Universal Sentence Encoder.Since we\u2019re performing a binary classification task, we use a binary cross entropy loss along with ADAM optimizer and accuracy as the metric.Now, let\u2019s train the model forWe reach a training accuracy of 99.62% and validation accuracy of 98.46% with only 2 epochs.Let\u2019s test the model on a few examples.Thus, with a combination of Tensorflow Hub and tf.keras, we can leverage transfer learning easily and build high-performance models for any of our downstream tasks.", "meta": {"title": "Transfer Learning in NLP with Tensorflow Hub and Keras"}}
{"text": "In recent years,  have been proposed for learning image representations, each getting better than the previous. But, their performance was still below the supervised counterparts.This changed when  proposed a new framework in their research paper \u201c\u201d. The SimCLR paper not only improves upon the previous state-of-the-art self-supervised learning methods but also beats the supervised learning method on ImageNet classification when scaling up the architecture.In this article, I will explain the key ideas of the framework proposed in the research paper using diagrams.As a kid, I remember we had to solve such puzzles in our textbook.The way a child would solve it is by looking at the picture of the animal on the left side, know its a cat, then search for a cat on the right side.\u201cSuch exercises were prepared for the child to be able to recognize an object and contrast that to other objects. Can we similarly teach machines?\u201dIt turns out that we can through a technique called . It attempts to teach machines to distinguish between similar and dissimilar things.To model the above exercise for a machine instead of a child, we see that we require 3 things:We would require example pairs of images that are similar and images that are different for training a model.The supervised school of thought would require a human to manually annotate such pairs. To automate this, we could leverage . But how do we formulate it?We need some mechanism to get representations that allow the machine to understand an image.We need some mechanism to compute the similarity of two images.The paper proposes a framework called \u201c\u201d for modeling the above problem in a self-supervised manner. It blends the concept of  with a few novel ideas to learn visual representations without human supervision.The idea of SimCLR framework is very simple. An image is taken and random transformations are applied to it to get a pair of two augmented images \\(x_i\\) and \\(x_j\\). Each image in that pair is passed through an encoder to get representations. Then a non-linear fully connected layer is applied to get representations z. The task is to maximize the similarity between these two representations \\(z_i\\) and \\(z_j\\) for the same image.Let\u2019s explore the various components of the SimCLR framework with an example. Suppose we have a training corpus of millions of unlabeled images.First, we generate batches of size N from the raw images. Let\u2019s take a batch of size N = 2 for simplicity. In the paper, they use a large batch size of 8192.The paper defines a random transformation function T that takes an image and applies a combination of .For each image in this batch, a random transformation function is applied to get a pair of 2 images. Thus, for a batch size of 2, we get 2*N = 2*2 = 4 total images.Each augmented image in a pair is passed through an encoder to get image representations. The encoder used is generic and replaceable with other architectures. The two encoders shown below have shared weights and we get vectors \\(h_i\\) and \\(h_j\\).In the paper, the authors used  architecture as the ConvNet encoder. The output is a 2048-dimensional vector h.The representations \\(h_i\\) and \\(h_j\\) of the two augmented images are then passed through a series of non-linear  layers to apply non-linear transformation and project it into a representation \\(z_i\\) and \\(z_j\\). This is denoted by \\(g(.)\\) in the paper and called projection head.Thus, for each augmented image in the batch, we get embedding vectors \\(z\\) for it.From these embedding, we calculate the loss in following steps:Now, the similarity between two augmented versions of an image is calculated using cosine similarity. For two augmented images \\(x_i\\) and \\(x_j\\), the cosine similarity is calculated on its projected representations \\(z_i\\) and \\(z_j\\).whereThe pairwise cosine similarity between each augmented image in a batch is calculated using the above formula. As shown in the figure, in an ideal case, the similarities between augmented images of cats will be high while the similarity between cat and elephant images will be lower.SimCLR uses a contrastive loss called \u201c\u201d (). Let see intuitively how it works.First, the augmented pairs in the batch are taken one by one.Next, we apply the softmax function to get the probability of these two images being similar.This softmax calculation is equivalent to getting the probability of the second augmented cat image being the most similar to the first cat image in the pair. Here, all remaining images in the batch are sampled as a dissimilar image (negative pair). Thus, we don\u2019t need specialized architecture, memory bank or queue need by previous approaches like ,  or .Then, the loss is calculated for a pair by taking the negative of the log of the above calculation. This formulation is the Noise Contrastive Estimation(NCE) Loss.We calculate the loss for the same pair a second time as well where the positions of the images are interchanged.Finally, we compute loss over all the pairs in the batch of size N=2 and take an average.Based on the loss, the encoder and projection head representations improves over time and the representations obtained place similar images closer in the space.Once the SimCLR model is trained on the contrastive learning task, it can be used for transfer learning. For this, the representations from the encoder are used instead of representations obtained from the projection head. These representations can be used for downstream tasks like ImageNet Classification.SimCLR outperformed previous self-supervised methods on ImageNet. The below image shows the top-1 accuracy of linear classifiers trained on representations learned with different self-supervised methods on ImageNet. The gray cross is supervised ResNet50 and SimCLR is shown in bold.Source: The official implementation of SimCLR in Tensorflow by the paper authors is available on . They also provide  for 1x, 2x, and 3x variants of the ResNet50 architectures using Tensorflow Hub.There are various unofficial SimCLR PyTorch implementations available that have been tested on small datasets like  and .Thus, SimCLR provides a strong framework for doing further research in this direction and improve the state of self-supervised learning for Computer Vision.If you found this blog post useful, please consider citing it as:", "meta": {"title": "The Illustrated SimCLR Framework"}}
{"text": "Consider a sentence given below. As humans, when we encounter the word \u201c\u201d, we could:The basic premise of the latest developments in NLP is to give machines the ability to learn such representations.In 2018, Google released BERT that attempted to learn representations based on a few novel ideas:Language modeling involves predicting the word given its context as a way to learn representation. Traditionally, this involved predicting the next word in the sentence when given previous words.\nBERT instead used a  objective, in which we randomly mask words in document and try to predict them based on surrounding context.\nCredits: The idea with \u201cNext Sentence Prediction\u201d is to detect whether two sentences are coherent when placed one after another or not.\nFor this, consecutive sentences from the training data are used as a positive example. For a negative example, some sentence is taken and a random sentence from another document is placed next to it. BERT model is trained on this task to identify if two sentences can occur next to each other.To solve the above two tasks, BERT uses stacked layers of transformer blocks as encoders. Word vectors are passed through the layers to capture the meaning and yield a vector of size 768 for the base model.\nJay Alammar has an  that illustrates the internals of transformers in more depth.BERT, when released, yielded state of art results on many NLP tasks on leaderboards. But, the model was very large which resulted in some issues. The \u201cALBERT\u201d paper highlights these issues in two categories:\nConsider a simple neural network with one input node, two hidden nodes and an output node. Even such a simple neural network will have 7 parameters to learn due to weights and bias per node.\nBERT-large, being a complex model, has 340 million parameters because of its 24 hidden layers and lots of nodes in the feed-forward network and attention heads. If you wanted to build upon the work on BERT and bring improvements to it, you would require large compute requirements to train from scratch and iterate on it.\n\nThese compute requirements mainly involve GPUs and TPUs, but such devices have a memory limitation. So, there is a limit to the size of the models.One popular approach to this problem in distributed training. Let\u2019s take an example of data parallelism on BERT-large, where training data is divided into two machines. The model is trained on two machines on chunks of data. As shown in the figure, you can notice how a large number of parameters to transfer during the synchronization of gradients can slow down the training process. The same bottleneck applies for the model parallelism as well where we store different parts of the model(parameters) on different machines.\n\nFigure: Communication overhead in distributed training\nThe recent trend in the NLP research community is using larger and larger models to get state-of-the-art performance on leaderboards. ALBERT shows that this can have diminishing returns.In the paper, the authors performed an interesting experiment.If larger models lead to better performance, why not double the hidden layer units of the largest available BERT model(BERT-large) from 1024 units to 2048 units?They call it \u201cBERT-xlarge\u201d. Surprisingly, the larger model performs worse than the BERT-large model on both the Language Modeling task as well as when tested on a reading comprehension test (RACE).\nFrom the plots given in the original paper, we can see how the performance degrades. BERT-xlarge is performing worse than BERT-large even though it is larger and has more parameters.\n\nCredits: ALBERT paperALBERT attacks these problems by building upon on BERT with a few novel ideas:\nBERT large model had 24 layers while it\u2019s base version had 12-layers. As we add more layers, we increase the number of parameters exponentially.\nTo solve this problem, ALBERT uses the concept of cross-layer parameter sharing. To illustrate, let\u2019s see the example of a 12-layer BERT-base model. Instead of learning unique parameters for each of the 12 layers, we only learn parameters for the first block and reuse the block in the remaining 11 layers.We can share parameters for either feed-forward layer only, the attention parameters only or share the parameters of the whole block itself. The paper shares the parameters for the whole block.Compared to the 110 million parameters of BERT-base, the ALBERT model only has 31 million parameters while using the same number of layers and 768 hidden units. The effect on accuracy is minimal for embedding size of 128. A major drop in accuracy is due to feed-forward network parameter sharing. The effect of sharing attention parameters is minimal.\nFigure: Effect of cross-layer parameter strategy on performanceBERT introduced a binary classification loss called \u201c\u201d. This was specifically created to improve performance on downstream tasks that use sentence pairs like \u201cNatural Language Inference\u201d. The basic process is:Papers like  and  have shed light on the ineffectiveness of NSP and found its impact on the downstream tasks unreliable. On eliminating the NSP task, the performance across several tasks improved.So, ALBERT proposes an alternative task called . The key idea is:This forces the model to learn finer-grained distinction about discourse-level coherence properties.ALBERT conjectures that NSP was ineffective because it\u2019s not a difficult task when compared to masked language modeling. In a single task, it mixes both topic prediction and coherence prediction. The topic prediction part is easy to learn because it overlaps with the masked language model loss. Thus, NSP will give higher scores even when it hasn\u2019t learned coherence prediction.SOP improves performance on downstream multi-sentence encoding tasks (SQUAD 1.1, 2.0, MNLI, SST-2, RACE).\nHere we can see how a model trained on NSP is only giving scores slightly better than the random baseline on the SOP task, but model trained on SOP can solve the NSP task quite effectively. This provides evidence that SOP leads to better learning representation. \nIn BERT, the embeddings used (word piece embeddings) size was linked to the hidden layer sizes of the transformer blocks. Word piece embeddings learned from the one-hot encoding representations of a vocabulary of size 30,000 was used. These are projected directly to the hidden space of the hidden layer.Let\u2019s say we have a vocabulary of size 30K, word-piece embedding of dimension E=768 and a hidden layer of size H=768. If we increase hidden units in the block, then we need to add a new dimension to each embedding as well. This problem is prevalent in XLNET and ROBERTA as well.\nALBERT solves this problem by factorizing the large vocabulary embedding matrix into two smaller matrices. This separates the size of the hidden layers from the size of the vocabulary embeddings. This allows us to grow the hidden size without significantly increasing the parameter size of the vocabulary embeddings.\nWe project the One Hot Encoding vector into the lower dimension embedding space of E=100 and then this embedding space into the hidden space H=768.ALBERT marks an important step towards building language models that not only get SOTA on the leaderboards but are also feasible for real-world applications.If you found this blog post useful, please consider citing it as:", "meta": {"title": "Visual Paper Summary: ALBERT (A Lite BERT)"}}
{"text": "Text Language Identification is the process of predicting the language of a given piece of text. You might have encountered it when Chrome shows a popup to translate a webpage when it detects that the content is not in English. Behind the scenes, Chrome is using a model to predict the language of text used on a webpage.When working with a dataset for NLP, the corpus may contain a mixed set of languages. Here, language identification can be useful to either filter out a few languages or to translate the corpus to a single language and then use it for your downstream tasks.In this post, I will explain the working mechanism and usage of various language detection libraries. is an open-source library in Python for word embeddings and text classification. It is built for production use cases rather than research and hence is optimized for performance and size. It extends the  model with ideas such as using  and .For our purpose of language identification, we can use the pre-trained fasttext language identification models. The model was trained on a dataset drawn from , , and . The basic idea is to prepare training data of (text, language) pairs and then train a classifier on it.The benchmark below shows that these pre-trained language detection models are better than , another popular python language detection library. Fasttext has better accuracy and also the inference time is very fast. It supports a wide variety of languages including French, German, English, Spanish, Chinese.The model returns two tuples. One of them is an array of language labels and the other is the confidence for each sentence. Here  is the  code for French. The model is 96.56% confident that the language is French.Fasttext returns the ISO code for the most probable one among the 170 languages. You can refer to the page on  codes to find language for each symbol.Google also provides a compact pretrained model for language identification called . It supports 107 languages.To use it, first install  from pip as:After installation, you can initialize the model as shown below.Once loaded, the model can be used to predict the language of a text as shown below:From the returned result, you can get the language BCP-47 style language code. The mapping of code to language is available .You can also get the confidence of the model from the result.You can also get the reliability of the prediction from the result object.Instead of predicting a single language,  also provides a method to get confidence over multiple languages.For example, we can get the top-2 predicted languages as:Thus, we learned how pretrained models can be used for language detection in Python. This is very useful to filter out non-English responses in NLP projects and handle them.", "meta": {"title": "Identify the Language of Text using Python"}}
{"text": "If you are migrating to Django from another MVC framework, chances are you already know SQL.In this post, I will be illustrating how to use Django ORM by drawing analogies to equivalent SQL statements. Connecting a new topic to your existing knowledge will help you learn to use the ORM faster.Let us consider a simple base model for a person with attributes name, age, and gender.To implement the above entity, we would model it as a table in SQL.The same table is modeled in Django as a class which inherits from the base Model class. The ORM creates the equivalent table under the hood.The most used data types are:The various queries we can use are:\nSQL:Django:\nSQL:Django:\nSQL:Django:\nSQL:Django:\nSQL:Django:\nSQL:Django:\nSQL:Django:\nSQL:Django:\nSQL:Django:\nSQL:Django:SQL:Django:SQL:Django:SQL:Django:SQL:Django:\nSQL:Django:\nSQL:Django:SQL:Django:\nSQL:Django:\nSQL:Django:\nSQL:Django:\nSQL:Django:\nSQL:Django:\nSQL:Django:\nSQL:Django:\nSQL:Django:\nSQL:Django:\nSQL:Django: \nSQL:Django:Consider a foreign key relationship between books and publisher.\nSQL:Django:\nSQL:Django:", "meta": {"title": "Django ORM if you already know SQL"}}
{"text": "I recently worked on a Django web application which was deployed on a Linux Server with SSH access. Deployments\nwere done manually and the CI/CD pipeline was still in planning.We had to perform these tasks every time to deploy the latest changes.I found the process repetitive and researched on whether it was possible to automate the commands I need to run\nafter SSHing into the server.Turns out, we can write a shell script to automate the task.Create a new shell script file  with the following content. Modify it to point to your PEM file, username, and IP address.The above code prints \u2018Hello World\u2019 on the remote server.You can write any shell commands between the two  and it will be run on the remote server.\nAdd the sequence of commands you currently run manually on the server to this script.For the Django project, I wrote the following commands that pulls the latest code and restarts the services.Run the below command to change permissions of the script and make it executable.Run the script and it will perform all the deployment.This simple script has saved me a lot of time until the CI/CD process is in place.", "meta": {"title": "How to Automate Manual Steps after SSH"}}
{"text": "In this article, I will go over the most frequent tasks related to file paths and show how you can refactor the old approach of using  module to the new cleaner way using  module.In pathlib, we can use the division operator to separate the paths elegantly.In new versions of python, you can directly pass a pathlib  to the  function.In older versions, you can either convert the path to a string using  or use the  method.", "meta": {"title": "Migrating from OS.PATH to PATHLIB Module in Python"}}
{"text": "When working with Machine Learning projects, you will come across a wide variety of equations that you need to implement in code. Mathematical notations capture a concept so eloquently but unfamiliarity with them makes them obscure.In this post, I\u2019ll be explaining the most common math notations by connecting it with its analogous concept in Python. Once you learn them, you will be able to intuitively grasp the intention of an equation and be able to implement it in code.This symbol is taking the value at i\\(^{th}\\) index of a vector.This can be extended for 2D vectors and so on.\n\\(x_{ij}\\)This symbol finds the sum of all elements in a vector for a given range. Both lower and upper limits are inclusive. In Python, it is equivalent to looping over a vector from index 0 to index N-1. Notice how we\u2019re using the previously explained \\(x_i\\) symbol to get the value at index.The above code can even be shortened using built-in functions in Python asHere we reuse the sigma notation and divide by the number of elements to get an average.The above code can even be shortened in Python asThis symbol finds the product of all elements in a vector for a given range. In Python, it is equivalent to looping over a vector from index 0 to index N-1 and multiplying them.The pipe symbol can mean different things based on where it\u2019s applied.This symbol denotes the absolute value of a number i.e. without a sign.The norm is used to calculate the magnitude of a vector. In Python, this means squaring each element of an array, summing them and then taking the square root.This symbol checks if an element is part of a set. In Python, this would be equivalent toThis denotes a function which takes a domain X and maps it to range Y. In Python, it\u2019s equivalent to taking a pool of values X, doing some operation on it to calculate pool of values Y.You will encounter the following symbols in place of X and Y. Here are what they mean: means input and outputs are real numbers and can take any value (integer, float, irrational, rational).\nIn Python, this is equivalent to any value except complex numbers.You will also encounter symbols such as\\(R^d\\) means d-dimensional vector of real numbers.Let\u2019s assume d = 2. In Python, an example can be a function that takes 2-D array and returns it\u2019s sum. It will be mapping a \\(R^d\\) to \\(R\\)This is basically exchanging the rows and columns.\nIn Python, this would be equivalent toOutput would be a list with exchanged rows and columns.It means multiplying the corresponding elements in two tensors. In Python, this would be equivalent to multiplying the corresponding elements in two lists.Output isIt gives the sum of the products of the corresponding entries of the two sequences of numbers.The hat gives the unit vector. This means dividing each component in a vector by it\u2019s length(norm).This makes the magnitude of the vector 1 and only keeps the direction.This denotes the factorial of a number. It is the product of numbers starting from 1 to that number. In Python, it can be calculated asThe same thing can also be calculated using built-in function.The output is", "meta": {"title": "Math Symbols Explained with Python"}}
{"text": "In Windows, we can use Alt+F4 keyboard shortcut to shutdown. But Linux doesn\u2019t have such feature out of the box. After switching to Ubuntu, I struggled trying to make a keyboard shortcut for shutting down the computer.So I started reading about the packages related to shutdown and discovered a method that works flawlessly. We utilize a package called  that\u2019s present by default in the /sbin directory.: You can make a shortcut for restart as well, follow the same tutorial except in step 3, use code  .You have a fully functioning keyboard shortcut to shutdown Linux just like Windows. Press Ctrl+Alt+K and your system is off. Please let me know if it worked for you in the comments.", "meta": {"title": "Shutdown Ubuntu With A Keyboard Shortcut"}}
