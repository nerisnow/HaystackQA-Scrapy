In recent years,  have been proposed for learning image representations, each getting better than the previous. But, their performance was still below the supervised counterparts.This changed when  proposed a new framework in their research paper “”. The SimCLR paper not only improves upon the previous state-of-the-art self-supervised learning methods but also beats the supervised learning method on ImageNet classification when scaling up the architecture.In this article, I will explain the key ideas of the framework proposed in the research paper using diagrams.As a kid, I remember we had to solve such puzzles in our textbook.The way a child would solve it is by looking at the picture of the animal on the left side, know its a cat, then search for a cat on the right side.“Such exercises were prepared for the child to be able to recognize an object and contrast that to other objects. Can we similarly teach machines?”It turns out that we can through a technique called . It attempts to teach machines to distinguish between similar and dissimilar things.To model the above exercise for a machine instead of a child, we see that we require 3 things:We would require example pairs of images that are similar and images that are different for training a model.The supervised school of thought would require a human to manually annotate such pairs. To automate this, we could leverage . But how do we formulate it?We need some mechanism to get representations that allow the machine to understand an image.We need some mechanism to compute the similarity of two images.The paper proposes a framework called “” for modeling the above problem in a self-supervised manner. It blends the concept of  with a few novel ideas to learn visual representations without human supervision.The idea of SimCLR framework is very simple. An image is taken and random transformations are applied to it to get a pair of two augmented images \(x_i\) and \(x_j\). Each image in that pair is passed through an encoder to get representations. Then a non-linear fully connected layer is applied to get representations z. The task is to maximize the similarity between these two representations \(z_i\) and \(z_j\) for the same image.Let’s explore the various components of the SimCLR framework with an example. Suppose we have a training corpus of millions of unlabeled images.First, we generate batches of size N from the raw images. Let’s take a batch of size N = 2 for simplicity. In the paper, they use a large batch size of 8192.The paper defines a random transformation function T that takes an image and applies a combination of .For each image in this batch, a random transformation function is applied to get a pair of 2 images. Thus, for a batch size of 2, we get 2*N = 2*2 = 4 total images.Each augmented image in a pair is passed through an encoder to get image representations. The encoder used is generic and replaceable with other architectures. The two encoders shown below have shared weights and we get vectors \(h_i\) and \(h_j\).In the paper, the authors used  architecture as the ConvNet encoder. The output is a 2048-dimensional vector h.The representations \(h_i\) and \(h_j\) of the two augmented images are then passed through a series of non-linear  layers to apply non-linear transformation and project it into a representation \(z_i\) and \(z_j\). This is denoted by \(g(.)\) in the paper and called projection head.Thus, for each augmented image in the batch, we get embedding vectors \(z\) for it.From these embedding, we calculate the loss in following steps:Now, the similarity between two augmented versions of an image is calculated using cosine similarity. For two augmented images \(x_i\) and \(x_j\), the cosine similarity is calculated on its projected representations \(z_i\) and \(z_j\).whereThe pairwise cosine similarity between each augmented image in a batch is calculated using the above formula. As shown in the figure, in an ideal case, the similarities between augmented images of cats will be high while the similarity between cat and elephant images will be lower.SimCLR uses a contrastive loss called “” (). Let see intuitively how it works.First, the augmented pairs in the batch are taken one by one.Next, we apply the softmax function to get the probability of these two images being similar.This softmax calculation is equivalent to getting the probability of the second augmented cat image being the most similar to the first cat image in the pair. Here, all remaining images in the batch are sampled as a dissimilar image (negative pair). Thus, we don’t need specialized architecture, memory bank or queue need by previous approaches like ,  or .Then, the loss is calculated for a pair by taking the negative of the log of the above calculation. This formulation is the Noise Contrastive Estimation(NCE) Loss.We calculate the loss for the same pair a second time as well where the positions of the images are interchanged.Finally, we compute loss over all the pairs in the batch of size N=2 and take an average.Based on the loss, the encoder and projection head representations improves over time and the representations obtained place similar images closer in the space.Once the SimCLR model is trained on the contrastive learning task, it can be used for transfer learning. For this, the representations from the encoder are used instead of representations obtained from the projection head. These representations can be used for downstream tasks like ImageNet Classification.SimCLR outperformed previous self-supervised methods on ImageNet. The below image shows the top-1 accuracy of linear classifiers trained on representations learned with different self-supervised methods on ImageNet. The gray cross is supervised ResNet50 and SimCLR is shown in bold.Source: The official implementation of SimCLR in Tensorflow by the paper authors is available on . They also provide  for 1x, 2x, and 3x variants of the ResNet50 architectures using Tensorflow Hub.There are various unofficial SimCLR PyTorch implementations available that have been tested on small datasets like  and .Thus, SimCLR provides a strong framework for doing further research in this direction and improve the state of self-supervised learning for Computer Vision.If you found this blog post useful, please consider citing it as:In recent years,  have been proposed for learning image representations, each getting better than the previous. But, their performance was still below the supervised counterparts.This changed when  proposed a new framework in their research paper “”. The SimCLR paper not only improves upon the previous state-of-the-art self-supervised learning methods but also beats the supervised learning method on ImageNet classification when scaling up the architecture.In this article, I will explain the key ideas of the framework proposed in the research paper using diagrams.As a kid, I remember we had to solve such puzzles in our textbook.The way a child would solve it is by looking at the picture of the animal on the left side, know its a cat, then search for a cat on the right side.“Such exercises were prepared for the child to be able to recognize an object and contrast that to other objects. Can we similarly teach machines?”It turns out that we can through a technique called . It attempts to teach machines to distinguish between similar and dissimilar things.To model the above exercise for a machine instead of a child, we see that we require 3 things:We would require example pairs of images that are similar and images that are different for training a model.The supervised school of thought would require a human to manually annotate such pairs. To automate this, we could leverage . But how do we formulate it?We need some mechanism to get representations that allow the machine to understand an image.We need some mechanism to compute the similarity of two images.The paper proposes a framework called “” for modeling the above problem in a self-supervised manner. It blends the concept of  with a few novel ideas to learn visual representations without human supervision.The idea of SimCLR framework is very simple. An image is taken and random transformations are applied to it to get a pair of two augmented images \(x_i\) and \(x_j\). Each image in that pair is passed through an encoder to get representations. Then a non-linear fully connected layer is applied to get representations z. The task is to maximize the similarity between these two representations \(z_i\) and \(z_j\) for the same image.Let’s explore the various components of the SimCLR framework with an example. Suppose we have a training corpus of millions of unlabeled images.First, we generate batches of size N from the raw images. Let’s take a batch of size N = 2 for simplicity. In the paper, they use a large batch size of 8192.The paper defines a random transformation function T that takes an image and applies a combination of .For each image in this batch, a random transformation function is applied to get a pair of 2 images. Thus, for a batch size of 2, we get 2*N = 2*2 = 4 total images.Each augmented image in a pair is passed through an encoder to get image representations. The encoder used is generic and replaceable with other architectures. The two encoders shown below have shared weights and we get vectors \(h_i\) and \(h_j\).In the paper, the authors used  architecture as the ConvNet encoder. The output is a 2048-dimensional vector h.The representations \(h_i\) and \(h_j\) of the two augmented images are then passed through a series of non-linear  layers to apply non-linear transformation and project it into a representation \(z_i\) and \(z_j\). This is denoted by \(g(.)\) in the paper and called projection head.Thus, for each augmented image in the batch, we get embedding vectors \(z\) for it.From these embedding, we calculate the loss in following steps:Now, the similarity between two augmented versions of an image is calculated using cosine similarity. For two augmented images \(x_i\) and \(x_j\), the cosine similarity is calculated on its projected representations \(z_i\) and \(z_j\).whereThe pairwise cosine similarity between each augmented image in a batch is calculated using the above formula. As shown in the figure, in an ideal case, the similarities between augmented images of cats will be high while the similarity between cat and elephant images will be lower.SimCLR uses a contrastive loss called “” (). Let see intuitively how it works.First, the augmented pairs in the batch are taken one by one.Next, we apply the softmax function to get the probability of these two images being similar.This softmax calculation is equivalent to getting the probability of the second augmented cat image being the most similar to the first cat image in the pair. Here, all remaining images in the batch are sampled as a dissimilar image (negative pair). Thus, we don’t need specialized architecture, memory bank or queue need by previous approaches like ,  or .Then, the loss is calculated for a pair by taking the negative of the log of the above calculation. This formulation is the Noise Contrastive Estimation(NCE) Loss.We calculate the loss for the same pair a second time as well where the positions of the images are interchanged.Finally, we compute loss over all the pairs in the batch of size N=2 and take an average.Based on the loss, the encoder and projection head representations improves over time and the representations obtained place similar images closer in the space.Once the SimCLR model is trained on the contrastive learning task, it can be used for transfer learning. For this, the representations from the encoder are used instead of representations obtained from the projection head. These representations can be used for downstream tasks like ImageNet Classification.SimCLR outperformed previous self-supervised methods on ImageNet. The below image shows the top-1 accuracy of linear classifiers trained on representations learned with different self-supervised methods on ImageNet. The gray cross is supervised ResNet50 and SimCLR is shown in bold.Source: The official implementation of SimCLR in Tensorflow by the paper authors is available on . They also provide  for 1x, 2x, and 3x variants of the ResNet50 architectures using Tensorflow Hub.There are various unofficial SimCLR PyTorch implementations available that have been tested on small datasets like  and .Thus, SimCLR provides a strong framework for doing further research in this direction and improve the state of self-supervised learning for Computer Vision.If you found this blog post useful, please consider citing it as:In recent years,  have been proposed for learning image representations, each getting better than the previous. But, their performance was still below the supervised counterparts.This changed when  proposed a new framework in their research paper “”. The SimCLR paper not only improves upon the previous state-of-the-art self-supervised learning methods but also beats the supervised learning method on ImageNet classification when scaling up the architecture.In this article, I will explain the key ideas of the framework proposed in the research paper using diagrams.As a kid, I remember we had to solve such puzzles in our textbook.The way a child would solve it is by looking at the picture of the animal on the left side, know its a cat, then search for a cat on the right side.“Such exercises were prepared for the child to be able to recognize an object and contrast that to other objects. Can we similarly teach machines?”It turns out that we can through a technique called . It attempts to teach machines to distinguish between similar and dissimilar things.To model the above exercise for a machine instead of a child, we see that we require 3 things:We would require example pairs of images that are similar and images that are different for training a model.The supervised school of thought would require a human to manually annotate such pairs. To automate this, we could leverage . But how do we formulate it?We need some mechanism to get representations that allow the machine to understand an image.We need some mechanism to compute the similarity of two images.The paper proposes a framework called “” for modeling the above problem in a self-supervised manner. It blends the concept of  with a few novel ideas to learn visual representations without human supervision.The idea of SimCLR framework is very simple. An image is taken and random transformations are applied to it to get a pair of two augmented images \(x_i\) and \(x_j\). Each image in that pair is passed through an encoder to get representations. Then a non-linear fully connected layer is applied to get representations z. The task is to maximize the similarity between these two representations \(z_i\) and \(z_j\) for the same image.Let’s explore the various components of the SimCLR framework with an example. Suppose we have a training corpus of millions of unlabeled images.First, we generate batches of size N from the raw images. Let’s take a batch of size N = 2 for simplicity. In the paper, they use a large batch size of 8192.The paper defines a random transformation function T that takes an image and applies a combination of .For each image in this batch, a random transformation function is applied to get a pair of 2 images. Thus, for a batch size of 2, we get 2*N = 2*2 = 4 total images.Each augmented image in a pair is passed through an encoder to get image representations. The encoder used is generic and replaceable with other architectures. The two encoders shown below have shared weights and we get vectors \(h_i\) and \(h_j\).In the paper, the authors used  architecture as the ConvNet encoder. The output is a 2048-dimensional vector h.The representations \(h_i\) and \(h_j\) of the two augmented images are then passed through a series of non-linear  layers to apply non-linear transformation and project it into a representation \(z_i\) and \(z_j\). This is denoted by \(g(.)\) in the paper and called projection head.Thus, for each augmented image in the batch, we get embedding vectors \(z\) for it.From these embedding, we calculate the loss in following steps:Now, the similarity between two augmented versions of an image is calculated using cosine similarity. For two augmented images \(x_i\) and \(x_j\), the cosine similarity is calculated on its projected representations \(z_i\) and \(z_j\).whereThe pairwise cosine similarity between each augmented image in a batch is calculated using the above formula. As shown in the figure, in an ideal case, the similarities between augmented images of cats will be high while the similarity between cat and elephant images will be lower.SimCLR uses a contrastive loss called “” (). Let see intuitively how it works.First, the augmented pairs in the batch are taken one by one.Next, we apply the softmax function to get the probability of these two images being similar.This softmax calculation is equivalent to getting the probability of the second augmented cat image being the most similar to the first cat image in the pair. Here, all remaining images in the batch are sampled as a dissimilar image (negative pair). Thus, we don’t need specialized architecture, memory bank or queue need by previous approaches like ,  or .Then, the loss is calculated for a pair by taking the negative of the log of the above calculation. This formulation is the Noise Contrastive Estimation(NCE) Loss.We calculate the loss for the same pair a second time as well where the positions of the images are interchanged.Finally, we compute loss over all the pairs in the batch of size N=2 and take an average.Based on the loss, the encoder and projection head representations improves over time and the representations obtained place similar images closer in the space.Once the SimCLR model is trained on the contrastive learning task, it can be used for transfer learning. For this, the representations from the encoder are used instead of representations obtained from the projection head. These representations can be used for downstream tasks like ImageNet Classification.SimCLR outperformed previous self-supervised methods on ImageNet. The below image shows the top-1 accuracy of linear classifiers trained on representations learned with different self-supervised methods on ImageNet. The gray cross is supervised ResNet50 and SimCLR is shown in bold.Source: The official implementation of SimCLR in Tensorflow by the paper authors is available on . They also provide  for 1x, 2x, and 3x variants of the ResNet50 architectures using Tensorflow Hub.There are various unofficial SimCLR PyTorch implementations available that have been tested on small datasets like  and .Thus, SimCLR provides a strong framework for doing further research in this direction and improve the state of self-supervised learning for Computer Vision.If you found this blog post useful, please consider citing it as:In recent years,  have been proposed for learning image representations, each getting better than the previous. But, their performance was still below the supervised counterparts.This changed when  proposed a new framework in their research paper “”. The SimCLR paper not only improves upon the previous state-of-the-art self-supervised learning methods but also beats the supervised learning method on ImageNet classification when scaling up the architecture.In this article, I will explain the key ideas of the framework proposed in the research paper using diagrams.As a kid, I remember we had to solve such puzzles in our textbook.The way a child would solve it is by looking at the picture of the animal on the left side, know its a cat, then search for a cat on the right side.“Such exercises were prepared for the child to be able to recognize an object and contrast that to other objects. Can we similarly teach machines?”It turns out that we can through a technique called . It attempts to teach machines to distinguish between similar and dissimilar things.To model the above exercise for a machine instead of a child, we see that we require 3 things:We would require example pairs of images that are similar and images that are different for training a model.The supervised school of thought would require a human to manually annotate such pairs. To automate this, we could leverage . But how do we formulate it?We need some mechanism to get representations that allow the machine to understand an image.We need some mechanism to compute the similarity of two images.The paper proposes a framework called “” for modeling the above problem in a self-supervised manner. It blends the concept of  with a few novel ideas to learn visual representations without human supervision.The idea of SimCLR framework is very simple. An image is taken and random transformations are applied to it to get a pair of two augmented images \(x_i\) and \(x_j\). Each image in that pair is passed through an encoder to get representations. Then a non-linear fully connected layer is applied to get representations z. The task is to maximize the similarity between these two representations \(z_i\) and \(z_j\) for the same image.Let’s explore the various components of the SimCLR framework with an example. Suppose we have a training corpus of millions of unlabeled images.First, we generate batches of size N from the raw images. Let’s take a batch of size N = 2 for simplicity. In the paper, they use a large batch size of 8192.The paper defines a random transformation function T that takes an image and applies a combination of .For each image in this batch, a random transformation function is applied to get a pair of 2 images. Thus, for a batch size of 2, we get 2*N = 2*2 = 4 total images.Each augmented image in a pair is passed through an encoder to get image representations. The encoder used is generic and replaceable with other architectures. The two encoders shown below have shared weights and we get vectors \(h_i\) and \(h_j\).In the paper, the authors used  architecture as the ConvNet encoder. The output is a 2048-dimensional vector h.The representations \(h_i\) and \(h_j\) of the two augmented images are then passed through a series of non-linear  layers to apply non-linear transformation and project it into a representation \(z_i\) and \(z_j\). This is denoted by \(g(.)\) in the paper and called projection head.Thus, for each augmented image in the batch, we get embedding vectors \(z\) for it.From these embedding, we calculate the loss in following steps:Now, the similarity between two augmented versions of an image is calculated using cosine similarity. For two augmented images \(x_i\) and \(x_j\), the cosine similarity is calculated on its projected representations \(z_i\) and \(z_j\).whereThe pairwise cosine similarity between each augmented image in a batch is calculated using the above formula. As shown in the figure, in an ideal case, the similarities between augmented images of cats will be high while the similarity between cat and elephant images will be lower.SimCLR uses a contrastive loss called “” (). Let see intuitively how it works.First, the augmented pairs in the batch are taken one by one.Next, we apply the softmax function to get the probability of these two images being similar.This softmax calculation is equivalent to getting the probability of the second augmented cat image being the most similar to the first cat image in the pair. Here, all remaining images in the batch are sampled as a dissimilar image (negative pair). Thus, we don’t need specialized architecture, memory bank or queue need by previous approaches like ,  or .Then, the loss is calculated for a pair by taking the negative of the log of the above calculation. This formulation is the Noise Contrastive Estimation(NCE) Loss.We calculate the loss for the same pair a second time as well where the positions of the images are interchanged.Finally, we compute loss over all the pairs in the batch of size N=2 and take an average.Based on the loss, the encoder and projection head representations improves over time and the representations obtained place similar images closer in the space.Once the SimCLR model is trained on the contrastive learning task, it can be used for transfer learning. For this, the representations from the encoder are used instead of representations obtained from the projection head. These representations can be used for downstream tasks like ImageNet Classification.SimCLR outperformed previous self-supervised methods on ImageNet. The below image shows the top-1 accuracy of linear classifiers trained on representations learned with different self-supervised methods on ImageNet. The gray cross is supervised ResNet50 and SimCLR is shown in bold.Source: The official implementation of SimCLR in Tensorflow by the paper authors is available on . They also provide  for 1x, 2x, and 3x variants of the ResNet50 architectures using Tensorflow Hub.There are various unofficial SimCLR PyTorch implementations available that have been tested on small datasets like  and .Thus, SimCLR provides a strong framework for doing further research in this direction and improve the state of self-supervised learning for Computer Vision.If you found this blog post useful, please consider citing it as:In recent years,  have been proposed for learning image representations, each getting better than the previous. But, their performance was still below the supervised counterparts.This changed when  proposed a new framework in their research paper “”. The SimCLR paper not only improves upon the previous state-of-the-art self-supervised learning methods but also beats the supervised learning method on ImageNet classification when scaling up the architecture.In this article, I will explain the key ideas of the framework proposed in the research paper using diagrams.As a kid, I remember we had to solve such puzzles in our textbook.The way a child would solve it is by looking at the picture of the animal on the left side, know its a cat, then search for a cat on the right side.“Such exercises were prepared for the child to be able to recognize an object and contrast that to other objects. Can we similarly teach machines?”It turns out that we can through a technique called . It attempts to teach machines to distinguish between similar and dissimilar things.To model the above exercise for a machine instead of a child, we see that we require 3 things:We would require example pairs of images that are similar and images that are different for training a model.The supervised school of thought would require a human to manually annotate such pairs. To automate this, we could leverage . But how do we formulate it?We need some mechanism to get representations that allow the machine to understand an image.We need some mechanism to compute the similarity of two images.The paper proposes a framework called “” for modeling the above problem in a self-supervised manner. It blends the concept of  with a few novel ideas to learn visual representations without human supervision.The idea of SimCLR framework is very simple. An image is taken and random transformations are applied to it to get a pair of two augmented images \(x_i\) and \(x_j\). Each image in that pair is passed through an encoder to get representations. Then a non-linear fully connected layer is applied to get representations z. The task is to maximize the similarity between these two representations \(z_i\) and \(z_j\) for the same image.Let’s explore the various components of the SimCLR framework with an example. Suppose we have a training corpus of millions of unlabeled images.First, we generate batches of size N from the raw images. Let’s take a batch of size N = 2 for simplicity. In the paper, they use a large batch size of 8192.The paper defines a random transformation function T that takes an image and applies a combination of .For each image in this batch, a random transformation function is applied to get a pair of 2 images. Thus, for a batch size of 2, we get 2*N = 2*2 = 4 total images.Each augmented image in a pair is passed through an encoder to get image representations. The encoder used is generic and replaceable with other architectures. The two encoders shown below have shared weights and we get vectors \(h_i\) and \(h_j\).In the paper, the authors used  architecture as the ConvNet encoder. The output is a 2048-dimensional vector h.The representations \(h_i\) and \(h_j\) of the two augmented images are then passed through a series of non-linear  layers to apply non-linear transformation and project it into a representation \(z_i\) and \(z_j\). This is denoted by \(g(.)\) in the paper and called projection head.Thus, for each augmented image in the batch, we get embedding vectors \(z\) for it.From these embedding, we calculate the loss in following steps:Now, the similarity between two augmented versions of an image is calculated using cosine similarity. For two augmented images \(x_i\) and \(x_j\), the cosine similarity is calculated on its projected representations \(z_i\) and \(z_j\).whereThe pairwise cosine similarity between each augmented image in a batch is calculated using the above formula. As shown in the figure, in an ideal case, the similarities between augmented images of cats will be high while the similarity between cat and elephant images will be lower.SimCLR uses a contrastive loss called “” (). Let see intuitively how it works.First, the augmented pairs in the batch are taken one by one.Next, we apply the softmax function to get the probability of these two images being similar.This softmax calculation is equivalent to getting the probability of the second augmented cat image being the most similar to the first cat image in the pair. Here, all remaining images in the batch are sampled as a dissimilar image (negative pair). Thus, we don’t need specialized architecture, memory bank or queue need by previous approaches like ,  or .Then, the loss is calculated for a pair by taking the negative of the log of the above calculation. This formulation is the Noise Contrastive Estimation(NCE) Loss.We calculate the loss for the same pair a second time as well where the positions of the images are interchanged.Finally, we compute loss over all the pairs in the batch of size N=2 and take an average.Based on the loss, the encoder and projection head representations improves over time and the representations obtained place similar images closer in the space.Once the SimCLR model is trained on the contrastive learning task, it can be used for transfer learning. For this, the representations from the encoder are used instead of representations obtained from the projection head. These representations can be used for downstream tasks like ImageNet Classification.SimCLR outperformed previous self-supervised methods on ImageNet. The below image shows the top-1 accuracy of linear classifiers trained on representations learned with different self-supervised methods on ImageNet. The gray cross is supervised ResNet50 and SimCLR is shown in bold.Source: The official implementation of SimCLR in Tensorflow by the paper authors is available on . They also provide  for 1x, 2x, and 3x variants of the ResNet50 architectures using Tensorflow Hub.There are various unofficial SimCLR PyTorch implementations available that have been tested on small datasets like  and .Thus, SimCLR provides a strong framework for doing further research in this direction and improve the state of self-supervised learning for Computer Vision.If you found this blog post useful, please consider citing it as: