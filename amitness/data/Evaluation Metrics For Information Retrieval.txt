Most software products we encounter today have some form of search functionality integrated into them. We search for content on Google, videos on YouTube, products on Amazon, messages on Slack, emails on Gmail, people on Facebook, and so on.As users, the workflow is pretty simple. We can search for items by writing our queries in a search box and the ranking model in their system gives us back the top-N most relevant results.How do we evaluate how good the top-N results are?In this post, I will answer the above question by explaining the common offline metrics used in learning to rank problems. These metrics are useful not only for evaluating search results but also for problems like keyword extraction and item recommendation.Let’s take a simple toy example to understand the details and trade-offs of various evaluation metrics.We have a ranking model that gives us back 5-most relevant results for a certain query. The first, third, and fifth results were  as per our ground-truth annotation.Let’s look at various metrics to evaluate this simple example.This metric quantifies how many items in the top-K results were relevant. Mathematically, this is given by:For our example, precision@1 = 1 as all items in the first 1 results is relevant.Similarly, precision@2 = 0.5 as only one of the top-2 results are relevant.Thus, we can calculate the precision score for all k values.A limitation of precision@k is that it doesn’t consider the position of the relevant items. Consider two models A and B that have the same number of relevant results i.e. 3 out of 5.For model A, the first three items were relevant, while for model B, the last three items were relevant. Precision@5 would be the same for both of these models even though model A is better.This metric gives how many actual relevant results were shown out of all actual relevant results for the query. Mathematically, this is given by:For our example, recall@1 = 0.33 as only one of the 3 actual relevant items are present.Similarly, recall@3 = 0.67 as only two of the 3 actual relevant items are present.Thus, we can calculate the recall score for different K values.This is a combined metric that incorporates both Precision@k and Recall@k by taking their harmonic mean. We can calculate it as:Using the previously calculated values of precision and recall, we can calculate F1-scores for different K values as shown below.While precision, recall, and F1 give us a single-value metric, they don’t consider the order in which the returned search results are sent. To solve that limitation, people have devised order-aware metrics given below:This metric is useful when we want our system to return the best relevant item and want that item to be at a higher position. Mathematically, this is given by:where:To calculate MRR, we first calculate the . It is simply the reciprocal of the rank of the first correct relevant result and the value ranges from 0 to 1.For our example, the reciprocal rank is \(\frac{1}{1}=1\) as the first correct item is at position 1.Let’s see another example where the only one relevant result is present at the end of the list i.e. position 5. It gets a lower reciprocal rank score of 0.2.Let’s consider another example where none of the returned results are relevant. In such a scenario, the reciprocal rank will be 0.For multiple different queries, we can calculate the MRR by taking the mean of the reciprocal rank for each query.We can see that MRR doesn’t care about the position of the remaining relevant results. So, if your use-case requires returning multiple relevant results in the best possible way, MRR is not a suitable metric.Average Precision is a metric that evaluates whether all of the ground-truth relevant items selected by the model are ranked higher or not. Unlike MRR, it considers all the relevant items.Mathematically, it is given by:where:For our example, we can calculate the AP based on our Precision@K values for different K.To illustrate the advantage of AP, let’s take our previous example but place the 3 relevant results at the beginning. We can see that this gets a perfect AP score than the above example.If we want to evaluate average precision across multiple queries, we can use the MAP. It is simply the mean of the average precision for all queries. Mathematically, this is given bywhereLet’s take another toy example where we annotated the items not just as relevant or not-relevant but instead used a grading scale between 0 to 5 where 0 denotes least relevant and 5 denotes the most relevant.
We have a ranking model that gives us back 5-most relevant results for a certain query. The first item had a relevance score of 3 as per our ground-truth annotation, the second item has a relevance score of 2 and so on.Let’s understand the various metrics to evaluate this type of setup.This metric uses a simple idea to just sum up the relevance scores for top-K items. The total score is called cumulative gain. Mathematically, this is given by:For our example, CG@2 will be 5 because we add the first two relevance scores 3 and 2.Similarly, we can calculate the cumulative gain for all the K-values as:While simple, CG doesn’t take into account the order of the relevant items. So, even if we swap a less-relevant item to the first position, the CG@2 will be the same.We saw how a simple cumulative gain doesn’t take into account the position. But, we would normally want items with a high relevance score to be present at a better rank.Consider an example below. With the cumulative gain, we are simply adding the scores without taking into account their position.An item with a relevance score of 3 at position 1 is better than the same item with relevance score 3 at position 2.So, we need some way to penalize the scores by their position. DCG introduces a log-based penalty function to reduce the relevance score at each position. For 5 items, the penalty would beUsing this penalty, we can now calculate the discounted cumulative gain simply by taking the sum of the  . Mathematically, this is given by:To understand the behavior of the log-penalty, let’s plot ranking position in x-axis and the percentage of relevance score i.e. \(\frac{1}{log_{2}(i+1)} * 100\) in the y-axis. As seen, in position 1, we don’t apply any penalty and score remains unchanged. But, the percentage of score kept decays exponentially from 100% in position 1 to 63% in position 2, 50% in position 3, and so on.Let’s now calculate DCG for our example.Based on these penalized scores, we can now calculate DCG at various k values simply by taking their sum up to k.There is also an alternative formulation for DCG@K that gives more penalty if relevant items are ranked lower. This formulation is preferred more in industry.While DCG solves the issues with cumulative gain, it has a limitation. Suppose we a query Q1 with 3 results and query Q2 with 5 results. Then the query with 5 results Q2 will have a larger overall DCG score. But we can’t say that query 2 was better than query 1.To allow a comparison of DCG across queries, we can use NDCG that normalizes the DCG values using the ideal order of the relevant items.Let’s take our previous example where we had already calculated the DCG values at various K values.For our example, ideally, we would have wanted the items to be sorted in descending order of relevance scores.Let’s calculate the ideal DCG(IDCG) for this order.Now we can calculate the NDCG@k for various k by diving DCG@k by IDCG@k as shown below:Thus, we get NDCG scores with a range between 0 and 1. A perfect ranking would get a score of 1. We can also compare NDCG@k scores of different queries since it’s a normalized score.Thus, we learned about various evaluation metrics for both binary and graded ground-truth labels and how each metric improves upon the previous.Most software products we encounter today have some form of search functionality integrated into them. We search for content on Google, videos on YouTube, products on Amazon, messages on Slack, emails on Gmail, people on Facebook, and so on.As users, the workflow is pretty simple. We can search for items by writing our queries in a search box and the ranking model in their system gives us back the top-N most relevant results.How do we evaluate how good the top-N results are?In this post, I will answer the above question by explaining the common offline metrics used in learning to rank problems. These metrics are useful not only for evaluating search results but also for problems like keyword extraction and item recommendation.Let’s take a simple toy example to understand the details and trade-offs of various evaluation metrics.We have a ranking model that gives us back 5-most relevant results for a certain query. The first, third, and fifth results were  as per our ground-truth annotation.Let’s look at various metrics to evaluate this simple example.This metric quantifies how many items in the top-K results were relevant. Mathematically, this is given by:For our example, precision@1 = 1 as all items in the first 1 results is relevant.Similarly, precision@2 = 0.5 as only one of the top-2 results are relevant.Thus, we can calculate the precision score for all k values.A limitation of precision@k is that it doesn’t consider the position of the relevant items. Consider two models A and B that have the same number of relevant results i.e. 3 out of 5.For model A, the first three items were relevant, while for model B, the last three items were relevant. Precision@5 would be the same for both of these models even though model A is better.This metric gives how many actual relevant results were shown out of all actual relevant results for the query. Mathematically, this is given by:For our example, recall@1 = 0.33 as only one of the 3 actual relevant items are present.Similarly, recall@3 = 0.67 as only two of the 3 actual relevant items are present.Thus, we can calculate the recall score for different K values.This is a combined metric that incorporates both Precision@k and Recall@k by taking their harmonic mean. We can calculate it as:Using the previously calculated values of precision and recall, we can calculate F1-scores for different K values as shown below.While precision, recall, and F1 give us a single-value metric, they don’t consider the order in which the returned search results are sent. To solve that limitation, people have devised order-aware metrics given below:This metric is useful when we want our system to return the best relevant item and want that item to be at a higher position. Mathematically, this is given by:where:To calculate MRR, we first calculate the . It is simply the reciprocal of the rank of the first correct relevant result and the value ranges from 0 to 1.For our example, the reciprocal rank is \(\frac{1}{1}=1\) as the first correct item is at position 1.Let’s see another example where the only one relevant result is present at the end of the list i.e. position 5. It gets a lower reciprocal rank score of 0.2.Let’s consider another example where none of the returned results are relevant. In such a scenario, the reciprocal rank will be 0.For multiple different queries, we can calculate the MRR by taking the mean of the reciprocal rank for each query.We can see that MRR doesn’t care about the position of the remaining relevant results. So, if your use-case requires returning multiple relevant results in the best possible way, MRR is not a suitable metric.Average Precision is a metric that evaluates whether all of the ground-truth relevant items selected by the model are ranked higher or not. Unlike MRR, it considers all the relevant items.Mathematically, it is given by:where:For our example, we can calculate the AP based on our Precision@K values for different K.To illustrate the advantage of AP, let’s take our previous example but place the 3 relevant results at the beginning. We can see that this gets a perfect AP score than the above example.If we want to evaluate average precision across multiple queries, we can use the MAP. It is simply the mean of the average precision for all queries. Mathematically, this is given bywhereLet’s take another toy example where we annotated the items not just as relevant or not-relevant but instead used a grading scale between 0 to 5 where 0 denotes least relevant and 5 denotes the most relevant.
We have a ranking model that gives us back 5-most relevant results for a certain query. The first item had a relevance score of 3 as per our ground-truth annotation, the second item has a relevance score of 2 and so on.Let’s understand the various metrics to evaluate this type of setup.This metric uses a simple idea to just sum up the relevance scores for top-K items. The total score is called cumulative gain. Mathematically, this is given by:For our example, CG@2 will be 5 because we add the first two relevance scores 3 and 2.Similarly, we can calculate the cumulative gain for all the K-values as:While simple, CG doesn’t take into account the order of the relevant items. So, even if we swap a less-relevant item to the first position, the CG@2 will be the same.We saw how a simple cumulative gain doesn’t take into account the position. But, we would normally want items with a high relevance score to be present at a better rank.Consider an example below. With the cumulative gain, we are simply adding the scores without taking into account their position.An item with a relevance score of 3 at position 1 is better than the same item with relevance score 3 at position 2.So, we need some way to penalize the scores by their position. DCG introduces a log-based penalty function to reduce the relevance score at each position. For 5 items, the penalty would beUsing this penalty, we can now calculate the discounted cumulative gain simply by taking the sum of the  . Mathematically, this is given by:To understand the behavior of the log-penalty, let’s plot ranking position in x-axis and the percentage of relevance score i.e. \(\frac{1}{log_{2}(i+1)} * 100\) in the y-axis. As seen, in position 1, we don’t apply any penalty and score remains unchanged. But, the percentage of score kept decays exponentially from 100% in position 1 to 63% in position 2, 50% in position 3, and so on.Let’s now calculate DCG for our example.Based on these penalized scores, we can now calculate DCG at various k values simply by taking their sum up to k.There is also an alternative formulation for DCG@K that gives more penalty if relevant items are ranked lower. This formulation is preferred more in industry.While DCG solves the issues with cumulative gain, it has a limitation. Suppose we a query Q1 with 3 results and query Q2 with 5 results. Then the query with 5 results Q2 will have a larger overall DCG score. But we can’t say that query 2 was better than query 1.To allow a comparison of DCG across queries, we can use NDCG that normalizes the DCG values using the ideal order of the relevant items.Let’s take our previous example where we had already calculated the DCG values at various K values.For our example, ideally, we would have wanted the items to be sorted in descending order of relevance scores.Let’s calculate the ideal DCG(IDCG) for this order.Now we can calculate the NDCG@k for various k by diving DCG@k by IDCG@k as shown below:Thus, we get NDCG scores with a range between 0 and 1. A perfect ranking would get a score of 1. We can also compare NDCG@k scores of different queries since it’s a normalized score.Thus, we learned about various evaluation metrics for both binary and graded ground-truth labels and how each metric improves upon the previous.Most software products we encounter today have some form of search functionality integrated into them. We search for content on Google, videos on YouTube, products on Amazon, messages on Slack, emails on Gmail, people on Facebook, and so on.As users, the workflow is pretty simple. We can search for items by writing our queries in a search box and the ranking model in their system gives us back the top-N most relevant results.How do we evaluate how good the top-N results are?In this post, I will answer the above question by explaining the common offline metrics used in learning to rank problems. These metrics are useful not only for evaluating search results but also for problems like keyword extraction and item recommendation.Let’s take a simple toy example to understand the details and trade-offs of various evaluation metrics.We have a ranking model that gives us back 5-most relevant results for a certain query. The first, third, and fifth results were  as per our ground-truth annotation.Let’s look at various metrics to evaluate this simple example.This metric quantifies how many items in the top-K results were relevant. Mathematically, this is given by:For our example, precision@1 = 1 as all items in the first 1 results is relevant.Similarly, precision@2 = 0.5 as only one of the top-2 results are relevant.Thus, we can calculate the precision score for all k values.A limitation of precision@k is that it doesn’t consider the position of the relevant items. Consider two models A and B that have the same number of relevant results i.e. 3 out of 5.For model A, the first three items were relevant, while for model B, the last three items were relevant. Precision@5 would be the same for both of these models even though model A is better.This metric gives how many actual relevant results were shown out of all actual relevant results for the query. Mathematically, this is given by:For our example, recall@1 = 0.33 as only one of the 3 actual relevant items are present.Similarly, recall@3 = 0.67 as only two of the 3 actual relevant items are present.Thus, we can calculate the recall score for different K values.This is a combined metric that incorporates both Precision@k and Recall@k by taking their harmonic mean. We can calculate it as:Using the previously calculated values of precision and recall, we can calculate F1-scores for different K values as shown below.While precision, recall, and F1 give us a single-value metric, they don’t consider the order in which the returned search results are sent. To solve that limitation, people have devised order-aware metrics given below:This metric is useful when we want our system to return the best relevant item and want that item to be at a higher position. Mathematically, this is given by:where:To calculate MRR, we first calculate the . It is simply the reciprocal of the rank of the first correct relevant result and the value ranges from 0 to 1.For our example, the reciprocal rank is \(\frac{1}{1}=1\) as the first correct item is at position 1.Let’s see another example where the only one relevant result is present at the end of the list i.e. position 5. It gets a lower reciprocal rank score of 0.2.Let’s consider another example where none of the returned results are relevant. In such a scenario, the reciprocal rank will be 0.For multiple different queries, we can calculate the MRR by taking the mean of the reciprocal rank for each query.We can see that MRR doesn’t care about the position of the remaining relevant results. So, if your use-case requires returning multiple relevant results in the best possible way, MRR is not a suitable metric.Average Precision is a metric that evaluates whether all of the ground-truth relevant items selected by the model are ranked higher or not. Unlike MRR, it considers all the relevant items.Mathematically, it is given by:where:For our example, we can calculate the AP based on our Precision@K values for different K.To illustrate the advantage of AP, let’s take our previous example but place the 3 relevant results at the beginning. We can see that this gets a perfect AP score than the above example.If we want to evaluate average precision across multiple queries, we can use the MAP. It is simply the mean of the average precision for all queries. Mathematically, this is given bywhereLet’s take another toy example where we annotated the items not just as relevant or not-relevant but instead used a grading scale between 0 to 5 where 0 denotes least relevant and 5 denotes the most relevant.
We have a ranking model that gives us back 5-most relevant results for a certain query. The first item had a relevance score of 3 as per our ground-truth annotation, the second item has a relevance score of 2 and so on.Let’s understand the various metrics to evaluate this type of setup.This metric uses a simple idea to just sum up the relevance scores for top-K items. The total score is called cumulative gain. Mathematically, this is given by:For our example, CG@2 will be 5 because we add the first two relevance scores 3 and 2.Similarly, we can calculate the cumulative gain for all the K-values as:While simple, CG doesn’t take into account the order of the relevant items. So, even if we swap a less-relevant item to the first position, the CG@2 will be the same.We saw how a simple cumulative gain doesn’t take into account the position. But, we would normally want items with a high relevance score to be present at a better rank.Consider an example below. With the cumulative gain, we are simply adding the scores without taking into account their position.An item with a relevance score of 3 at position 1 is better than the same item with relevance score 3 at position 2.So, we need some way to penalize the scores by their position. DCG introduces a log-based penalty function to reduce the relevance score at each position. For 5 items, the penalty would beUsing this penalty, we can now calculate the discounted cumulative gain simply by taking the sum of the  . Mathematically, this is given by:To understand the behavior of the log-penalty, let’s plot ranking position in x-axis and the percentage of relevance score i.e. \(\frac{1}{log_{2}(i+1)} * 100\) in the y-axis. As seen, in position 1, we don’t apply any penalty and score remains unchanged. But, the percentage of score kept decays exponentially from 100% in position 1 to 63% in position 2, 50% in position 3, and so on.Let’s now calculate DCG for our example.Based on these penalized scores, we can now calculate DCG at various k values simply by taking their sum up to k.There is also an alternative formulation for DCG@K that gives more penalty if relevant items are ranked lower. This formulation is preferred more in industry.While DCG solves the issues with cumulative gain, it has a limitation. Suppose we a query Q1 with 3 results and query Q2 with 5 results. Then the query with 5 results Q2 will have a larger overall DCG score. But we can’t say that query 2 was better than query 1.To allow a comparison of DCG across queries, we can use NDCG that normalizes the DCG values using the ideal order of the relevant items.Let’s take our previous example where we had already calculated the DCG values at various K values.For our example, ideally, we would have wanted the items to be sorted in descending order of relevance scores.Let’s calculate the ideal DCG(IDCG) for this order.Now we can calculate the NDCG@k for various k by diving DCG@k by IDCG@k as shown below:Thus, we get NDCG scores with a range between 0 and 1. A perfect ranking would get a score of 1. We can also compare NDCG@k scores of different queries since it’s a normalized score.Thus, we learned about various evaluation metrics for both binary and graded ground-truth labels and how each metric improves upon the previous.Most software products we encounter today have some form of search functionality integrated into them. We search for content on Google, videos on YouTube, products on Amazon, messages on Slack, emails on Gmail, people on Facebook, and so on.As users, the workflow is pretty simple. We can search for items by writing our queries in a search box and the ranking model in their system gives us back the top-N most relevant results.How do we evaluate how good the top-N results are?In this post, I will answer the above question by explaining the common offline metrics used in learning to rank problems. These metrics are useful not only for evaluating search results but also for problems like keyword extraction and item recommendation.Let’s take a simple toy example to understand the details and trade-offs of various evaluation metrics.We have a ranking model that gives us back 5-most relevant results for a certain query. The first, third, and fifth results were  as per our ground-truth annotation.Let’s look at various metrics to evaluate this simple example.This metric quantifies how many items in the top-K results were relevant. Mathematically, this is given by:For our example, precision@1 = 1 as all items in the first 1 results is relevant.Similarly, precision@2 = 0.5 as only one of the top-2 results are relevant.Thus, we can calculate the precision score for all k values.A limitation of precision@k is that it doesn’t consider the position of the relevant items. Consider two models A and B that have the same number of relevant results i.e. 3 out of 5.For model A, the first three items were relevant, while for model B, the last three items were relevant. Precision@5 would be the same for both of these models even though model A is better.This metric gives how many actual relevant results were shown out of all actual relevant results for the query. Mathematically, this is given by:For our example, recall@1 = 0.33 as only one of the 3 actual relevant items are present.Similarly, recall@3 = 0.67 as only two of the 3 actual relevant items are present.Thus, we can calculate the recall score for different K values.This is a combined metric that incorporates both Precision@k and Recall@k by taking their harmonic mean. We can calculate it as:Using the previously calculated values of precision and recall, we can calculate F1-scores for different K values as shown below.While precision, recall, and F1 give us a single-value metric, they don’t consider the order in which the returned search results are sent. To solve that limitation, people have devised order-aware metrics given below:This metric is useful when we want our system to return the best relevant item and want that item to be at a higher position. Mathematically, this is given by:where:To calculate MRR, we first calculate the . It is simply the reciprocal of the rank of the first correct relevant result and the value ranges from 0 to 1.For our example, the reciprocal rank is \(\frac{1}{1}=1\) as the first correct item is at position 1.Let’s see another example where the only one relevant result is present at the end of the list i.e. position 5. It gets a lower reciprocal rank score of 0.2.Let’s consider another example where none of the returned results are relevant. In such a scenario, the reciprocal rank will be 0.For multiple different queries, we can calculate the MRR by taking the mean of the reciprocal rank for each query.We can see that MRR doesn’t care about the position of the remaining relevant results. So, if your use-case requires returning multiple relevant results in the best possible way, MRR is not a suitable metric.Average Precision is a metric that evaluates whether all of the ground-truth relevant items selected by the model are ranked higher or not. Unlike MRR, it considers all the relevant items.Mathematically, it is given by:where:For our example, we can calculate the AP based on our Precision@K values for different K.To illustrate the advantage of AP, let’s take our previous example but place the 3 relevant results at the beginning. We can see that this gets a perfect AP score than the above example.If we want to evaluate average precision across multiple queries, we can use the MAP. It is simply the mean of the average precision for all queries. Mathematically, this is given bywhereLet’s take another toy example where we annotated the items not just as relevant or not-relevant but instead used a grading scale between 0 to 5 where 0 denotes least relevant and 5 denotes the most relevant.
We have a ranking model that gives us back 5-most relevant results for a certain query. The first item had a relevance score of 3 as per our ground-truth annotation, the second item has a relevance score of 2 and so on.Let’s understand the various metrics to evaluate this type of setup.This metric uses a simple idea to just sum up the relevance scores for top-K items. The total score is called cumulative gain. Mathematically, this is given by:For our example, CG@2 will be 5 because we add the first two relevance scores 3 and 2.Similarly, we can calculate the cumulative gain for all the K-values as:While simple, CG doesn’t take into account the order of the relevant items. So, even if we swap a less-relevant item to the first position, the CG@2 will be the same.We saw how a simple cumulative gain doesn’t take into account the position. But, we would normally want items with a high relevance score to be present at a better rank.Consider an example below. With the cumulative gain, we are simply adding the scores without taking into account their position.An item with a relevance score of 3 at position 1 is better than the same item with relevance score 3 at position 2.So, we need some way to penalize the scores by their position. DCG introduces a log-based penalty function to reduce the relevance score at each position. For 5 items, the penalty would beUsing this penalty, we can now calculate the discounted cumulative gain simply by taking the sum of the  . Mathematically, this is given by:To understand the behavior of the log-penalty, let’s plot ranking position in x-axis and the percentage of relevance score i.e. \(\frac{1}{log_{2}(i+1)} * 100\) in the y-axis. As seen, in position 1, we don’t apply any penalty and score remains unchanged. But, the percentage of score kept decays exponentially from 100% in position 1 to 63% in position 2, 50% in position 3, and so on.Let’s now calculate DCG for our example.Based on these penalized scores, we can now calculate DCG at various k values simply by taking their sum up to k.There is also an alternative formulation for DCG@K that gives more penalty if relevant items are ranked lower. This formulation is preferred more in industry.While DCG solves the issues with cumulative gain, it has a limitation. Suppose we a query Q1 with 3 results and query Q2 with 5 results. Then the query with 5 results Q2 will have a larger overall DCG score. But we can’t say that query 2 was better than query 1.To allow a comparison of DCG across queries, we can use NDCG that normalizes the DCG values using the ideal order of the relevant items.Let’s take our previous example where we had already calculated the DCG values at various K values.For our example, ideally, we would have wanted the items to be sorted in descending order of relevance scores.Let’s calculate the ideal DCG(IDCG) for this order.Now we can calculate the NDCG@k for various k by diving DCG@k by IDCG@k as shown below:Thus, we get NDCG scores with a range between 0 and 1. A perfect ranking would get a score of 1. We can also compare NDCG@k scores of different queries since it’s a normalized score.Thus, we learned about various evaluation metrics for both binary and graded ground-truth labels and how each metric improves upon the previous.Most software products we encounter today have some form of search functionality integrated into them. We search for content on Google, videos on YouTube, products on Amazon, messages on Slack, emails on Gmail, people on Facebook, and so on.As users, the workflow is pretty simple. We can search for items by writing our queries in a search box and the ranking model in their system gives us back the top-N most relevant results.How do we evaluate how good the top-N results are?In this post, I will answer the above question by explaining the common offline metrics used in learning to rank problems. These metrics are useful not only for evaluating search results but also for problems like keyword extraction and item recommendation.Let’s take a simple toy example to understand the details and trade-offs of various evaluation metrics.We have a ranking model that gives us back 5-most relevant results for a certain query. The first, third, and fifth results were  as per our ground-truth annotation.Let’s look at various metrics to evaluate this simple example.This metric quantifies how many items in the top-K results were relevant. Mathematically, this is given by:For our example, precision@1 = 1 as all items in the first 1 results is relevant.Similarly, precision@2 = 0.5 as only one of the top-2 results are relevant.Thus, we can calculate the precision score for all k values.A limitation of precision@k is that it doesn’t consider the position of the relevant items. Consider two models A and B that have the same number of relevant results i.e. 3 out of 5.For model A, the first three items were relevant, while for model B, the last three items were relevant. Precision@5 would be the same for both of these models even though model A is better.This metric gives how many actual relevant results were shown out of all actual relevant results for the query. Mathematically, this is given by:For our example, recall@1 = 0.33 as only one of the 3 actual relevant items are present.Similarly, recall@3 = 0.67 as only two of the 3 actual relevant items are present.Thus, we can calculate the recall score for different K values.This is a combined metric that incorporates both Precision@k and Recall@k by taking their harmonic mean. We can calculate it as:Using the previously calculated values of precision and recall, we can calculate F1-scores for different K values as shown below.While precision, recall, and F1 give us a single-value metric, they don’t consider the order in which the returned search results are sent. To solve that limitation, people have devised order-aware metrics given below:This metric is useful when we want our system to return the best relevant item and want that item to be at a higher position. Mathematically, this is given by:where:To calculate MRR, we first calculate the . It is simply the reciprocal of the rank of the first correct relevant result and the value ranges from 0 to 1.For our example, the reciprocal rank is \(\frac{1}{1}=1\) as the first correct item is at position 1.Let’s see another example where the only one relevant result is present at the end of the list i.e. position 5. It gets a lower reciprocal rank score of 0.2.Let’s consider another example where none of the returned results are relevant. In such a scenario, the reciprocal rank will be 0.For multiple different queries, we can calculate the MRR by taking the mean of the reciprocal rank for each query.We can see that MRR doesn’t care about the position of the remaining relevant results. So, if your use-case requires returning multiple relevant results in the best possible way, MRR is not a suitable metric.Average Precision is a metric that evaluates whether all of the ground-truth relevant items selected by the model are ranked higher or not. Unlike MRR, it considers all the relevant items.Mathematically, it is given by:where:For our example, we can calculate the AP based on our Precision@K values for different K.To illustrate the advantage of AP, let’s take our previous example but place the 3 relevant results at the beginning. We can see that this gets a perfect AP score than the above example.If we want to evaluate average precision across multiple queries, we can use the MAP. It is simply the mean of the average precision for all queries. Mathematically, this is given bywhereLet’s take another toy example where we annotated the items not just as relevant or not-relevant but instead used a grading scale between 0 to 5 where 0 denotes least relevant and 5 denotes the most relevant.
We have a ranking model that gives us back 5-most relevant results for a certain query. The first item had a relevance score of 3 as per our ground-truth annotation, the second item has a relevance score of 2 and so on.Let’s understand the various metrics to evaluate this type of setup.This metric uses a simple idea to just sum up the relevance scores for top-K items. The total score is called cumulative gain. Mathematically, this is given by:For our example, CG@2 will be 5 because we add the first two relevance scores 3 and 2.Similarly, we can calculate the cumulative gain for all the K-values as:While simple, CG doesn’t take into account the order of the relevant items. So, even if we swap a less-relevant item to the first position, the CG@2 will be the same.We saw how a simple cumulative gain doesn’t take into account the position. But, we would normally want items with a high relevance score to be present at a better rank.Consider an example below. With the cumulative gain, we are simply adding the scores without taking into account their position.An item with a relevance score of 3 at position 1 is better than the same item with relevance score 3 at position 2.So, we need some way to penalize the scores by their position. DCG introduces a log-based penalty function to reduce the relevance score at each position. For 5 items, the penalty would beUsing this penalty, we can now calculate the discounted cumulative gain simply by taking the sum of the  . Mathematically, this is given by:To understand the behavior of the log-penalty, let’s plot ranking position in x-axis and the percentage of relevance score i.e. \(\frac{1}{log_{2}(i+1)} * 100\) in the y-axis. As seen, in position 1, we don’t apply any penalty and score remains unchanged. But, the percentage of score kept decays exponentially from 100% in position 1 to 63% in position 2, 50% in position 3, and so on.Let’s now calculate DCG for our example.Based on these penalized scores, we can now calculate DCG at various k values simply by taking their sum up to k.There is also an alternative formulation for DCG@K that gives more penalty if relevant items are ranked lower. This formulation is preferred more in industry.While DCG solves the issues with cumulative gain, it has a limitation. Suppose we a query Q1 with 3 results and query Q2 with 5 results. Then the query with 5 results Q2 will have a larger overall DCG score. But we can’t say that query 2 was better than query 1.To allow a comparison of DCG across queries, we can use NDCG that normalizes the DCG values using the ideal order of the relevant items.Let’s take our previous example where we had already calculated the DCG values at various K values.For our example, ideally, we would have wanted the items to be sorted in descending order of relevance scores.Let’s calculate the ideal DCG(IDCG) for this order.Now we can calculate the NDCG@k for various k by diving DCG@k by IDCG@k as shown below:Thus, we get NDCG scores with a range between 0 and 1. A perfect ranking would get a score of 1. We can also compare NDCG@k scores of different queries since it’s a normalized score.Thus, we learned about various evaluation metrics for both binary and graded ground-truth labels and how each metric improves upon the previous.Most software products we encounter today have some form of search functionality integrated into them. We search for content on Google, videos on YouTube, products on Amazon, messages on Slack, emails on Gmail, people on Facebook, and so on.As users, the workflow is pretty simple. We can search for items by writing our queries in a search box and the ranking model in their system gives us back the top-N most relevant results.How do we evaluate how good the top-N results are?In this post, I will answer the above question by explaining the common offline metrics used in learning to rank problems. These metrics are useful not only for evaluating search results but also for problems like keyword extraction and item recommendation.Let’s take a simple toy example to understand the details and trade-offs of various evaluation metrics.We have a ranking model that gives us back 5-most relevant results for a certain query. The first, third, and fifth results were  as per our ground-truth annotation.Let’s look at various metrics to evaluate this simple example.This metric quantifies how many items in the top-K results were relevant. Mathematically, this is given by:For our example, precision@1 = 1 as all items in the first 1 results is relevant.Similarly, precision@2 = 0.5 as only one of the top-2 results are relevant.Thus, we can calculate the precision score for all k values.A limitation of precision@k is that it doesn’t consider the position of the relevant items. Consider two models A and B that have the same number of relevant results i.e. 3 out of 5.For model A, the first three items were relevant, while for model B, the last three items were relevant. Precision@5 would be the same for both of these models even though model A is better.This metric gives how many actual relevant results were shown out of all actual relevant results for the query. Mathematically, this is given by:For our example, recall@1 = 0.33 as only one of the 3 actual relevant items are present.Similarly, recall@3 = 0.67 as only two of the 3 actual relevant items are present.Thus, we can calculate the recall score for different K values.This is a combined metric that incorporates both Precision@k and Recall@k by taking their harmonic mean. We can calculate it as:Using the previously calculated values of precision and recall, we can calculate F1-scores for different K values as shown below.While precision, recall, and F1 give us a single-value metric, they don’t consider the order in which the returned search results are sent. To solve that limitation, people have devised order-aware metrics given below:This metric is useful when we want our system to return the best relevant item and want that item to be at a higher position. Mathematically, this is given by:where:To calculate MRR, we first calculate the . It is simply the reciprocal of the rank of the first correct relevant result and the value ranges from 0 to 1.For our example, the reciprocal rank is \(\frac{1}{1}=1\) as the first correct item is at position 1.Let’s see another example where the only one relevant result is present at the end of the list i.e. position 5. It gets a lower reciprocal rank score of 0.2.Let’s consider another example where none of the returned results are relevant. In such a scenario, the reciprocal rank will be 0.For multiple different queries, we can calculate the MRR by taking the mean of the reciprocal rank for each query.We can see that MRR doesn’t care about the position of the remaining relevant results. So, if your use-case requires returning multiple relevant results in the best possible way, MRR is not a suitable metric.Average Precision is a metric that evaluates whether all of the ground-truth relevant items selected by the model are ranked higher or not. Unlike MRR, it considers all the relevant items.Mathematically, it is given by:where:For our example, we can calculate the AP based on our Precision@K values for different K.To illustrate the advantage of AP, let’s take our previous example but place the 3 relevant results at the beginning. We can see that this gets a perfect AP score than the above example.If we want to evaluate average precision across multiple queries, we can use the MAP. It is simply the mean of the average precision for all queries. Mathematically, this is given bywhereLet’s take another toy example where we annotated the items not just as relevant or not-relevant but instead used a grading scale between 0 to 5 where 0 denotes least relevant and 5 denotes the most relevant.
We have a ranking model that gives us back 5-most relevant results for a certain query. The first item had a relevance score of 3 as per our ground-truth annotation, the second item has a relevance score of 2 and so on.Let’s understand the various metrics to evaluate this type of setup.This metric uses a simple idea to just sum up the relevance scores for top-K items. The total score is called cumulative gain. Mathematically, this is given by:For our example, CG@2 will be 5 because we add the first two relevance scores 3 and 2.Similarly, we can calculate the cumulative gain for all the K-values as:While simple, CG doesn’t take into account the order of the relevant items. So, even if we swap a less-relevant item to the first position, the CG@2 will be the same.We saw how a simple cumulative gain doesn’t take into account the position. But, we would normally want items with a high relevance score to be present at a better rank.Consider an example below. With the cumulative gain, we are simply adding the scores without taking into account their position.An item with a relevance score of 3 at position 1 is better than the same item with relevance score 3 at position 2.So, we need some way to penalize the scores by their position. DCG introduces a log-based penalty function to reduce the relevance score at each position. For 5 items, the penalty would beUsing this penalty, we can now calculate the discounted cumulative gain simply by taking the sum of the  . Mathematically, this is given by:To understand the behavior of the log-penalty, let’s plot ranking position in x-axis and the percentage of relevance score i.e. \(\frac{1}{log_{2}(i+1)} * 100\) in the y-axis. As seen, in position 1, we don’t apply any penalty and score remains unchanged. But, the percentage of score kept decays exponentially from 100% in position 1 to 63% in position 2, 50% in position 3, and so on.Let’s now calculate DCG for our example.Based on these penalized scores, we can now calculate DCG at various k values simply by taking their sum up to k.There is also an alternative formulation for DCG@K that gives more penalty if relevant items are ranked lower. This formulation is preferred more in industry.While DCG solves the issues with cumulative gain, it has a limitation. Suppose we a query Q1 with 3 results and query Q2 with 5 results. Then the query with 5 results Q2 will have a larger overall DCG score. But we can’t say that query 2 was better than query 1.To allow a comparison of DCG across queries, we can use NDCG that normalizes the DCG values using the ideal order of the relevant items.Let’s take our previous example where we had already calculated the DCG values at various K values.For our example, ideally, we would have wanted the items to be sorted in descending order of relevance scores.Let’s calculate the ideal DCG(IDCG) for this order.Now we can calculate the NDCG@k for various k by diving DCG@k by IDCG@k as shown below:Thus, we get NDCG scores with a range between 0 and 1. A perfect ranking would get a score of 1. We can also compare NDCG@k scores of different queries since it’s a normalized score.Thus, we learned about various evaluation metrics for both binary and graded ground-truth labels and how each metric improves upon the previous.Most software products we encounter today have some form of search functionality integrated into them. We search for content on Google, videos on YouTube, products on Amazon, messages on Slack, emails on Gmail, people on Facebook, and so on.As users, the workflow is pretty simple. We can search for items by writing our queries in a search box and the ranking model in their system gives us back the top-N most relevant results.How do we evaluate how good the top-N results are?In this post, I will answer the above question by explaining the common offline metrics used in learning to rank problems. These metrics are useful not only for evaluating search results but also for problems like keyword extraction and item recommendation.Let’s take a simple toy example to understand the details and trade-offs of various evaluation metrics.We have a ranking model that gives us back 5-most relevant results for a certain query. The first, third, and fifth results were  as per our ground-truth annotation.Let’s look at various metrics to evaluate this simple example.This metric quantifies how many items in the top-K results were relevant. Mathematically, this is given by:For our example, precision@1 = 1 as all items in the first 1 results is relevant.Similarly, precision@2 = 0.5 as only one of the top-2 results are relevant.Thus, we can calculate the precision score for all k values.A limitation of precision@k is that it doesn’t consider the position of the relevant items. Consider two models A and B that have the same number of relevant results i.e. 3 out of 5.For model A, the first three items were relevant, while for model B, the last three items were relevant. Precision@5 would be the same for both of these models even though model A is better.This metric gives how many actual relevant results were shown out of all actual relevant results for the query. Mathematically, this is given by:For our example, recall@1 = 0.33 as only one of the 3 actual relevant items are present.Similarly, recall@3 = 0.67 as only two of the 3 actual relevant items are present.Thus, we can calculate the recall score for different K values.This is a combined metric that incorporates both Precision@k and Recall@k by taking their harmonic mean. We can calculate it as:Using the previously calculated values of precision and recall, we can calculate F1-scores for different K values as shown below.While precision, recall, and F1 give us a single-value metric, they don’t consider the order in which the returned search results are sent. To solve that limitation, people have devised order-aware metrics given below:This metric is useful when we want our system to return the best relevant item and want that item to be at a higher position. Mathematically, this is given by:where:To calculate MRR, we first calculate the . It is simply the reciprocal of the rank of the first correct relevant result and the value ranges from 0 to 1.For our example, the reciprocal rank is \(\frac{1}{1}=1\) as the first correct item is at position 1.Let’s see another example where the only one relevant result is present at the end of the list i.e. position 5. It gets a lower reciprocal rank score of 0.2.Let’s consider another example where none of the returned results are relevant. In such a scenario, the reciprocal rank will be 0.For multiple different queries, we can calculate the MRR by taking the mean of the reciprocal rank for each query.We can see that MRR doesn’t care about the position of the remaining relevant results. So, if your use-case requires returning multiple relevant results in the best possible way, MRR is not a suitable metric.Average Precision is a metric that evaluates whether all of the ground-truth relevant items selected by the model are ranked higher or not. Unlike MRR, it considers all the relevant items.Mathematically, it is given by:where:For our example, we can calculate the AP based on our Precision@K values for different K.To illustrate the advantage of AP, let’s take our previous example but place the 3 relevant results at the beginning. We can see that this gets a perfect AP score than the above example.If we want to evaluate average precision across multiple queries, we can use the MAP. It is simply the mean of the average precision for all queries. Mathematically, this is given bywhereLet’s take another toy example where we annotated the items not just as relevant or not-relevant but instead used a grading scale between 0 to 5 where 0 denotes least relevant and 5 denotes the most relevant.
We have a ranking model that gives us back 5-most relevant results for a certain query. The first item had a relevance score of 3 as per our ground-truth annotation, the second item has a relevance score of 2 and so on.Let’s understand the various metrics to evaluate this type of setup.This metric uses a simple idea to just sum up the relevance scores for top-K items. The total score is called cumulative gain. Mathematically, this is given by:For our example, CG@2 will be 5 because we add the first two relevance scores 3 and 2.Similarly, we can calculate the cumulative gain for all the K-values as:While simple, CG doesn’t take into account the order of the relevant items. So, even if we swap a less-relevant item to the first position, the CG@2 will be the same.We saw how a simple cumulative gain doesn’t take into account the position. But, we would normally want items with a high relevance score to be present at a better rank.Consider an example below. With the cumulative gain, we are simply adding the scores without taking into account their position.An item with a relevance score of 3 at position 1 is better than the same item with relevance score 3 at position 2.So, we need some way to penalize the scores by their position. DCG introduces a log-based penalty function to reduce the relevance score at each position. For 5 items, the penalty would beUsing this penalty, we can now calculate the discounted cumulative gain simply by taking the sum of the  . Mathematically, this is given by:To understand the behavior of the log-penalty, let’s plot ranking position in x-axis and the percentage of relevance score i.e. \(\frac{1}{log_{2}(i+1)} * 100\) in the y-axis. As seen, in position 1, we don’t apply any penalty and score remains unchanged. But, the percentage of score kept decays exponentially from 100% in position 1 to 63% in position 2, 50% in position 3, and so on.Let’s now calculate DCG for our example.Based on these penalized scores, we can now calculate DCG at various k values simply by taking their sum up to k.There is also an alternative formulation for DCG@K that gives more penalty if relevant items are ranked lower. This formulation is preferred more in industry.While DCG solves the issues with cumulative gain, it has a limitation. Suppose we a query Q1 with 3 results and query Q2 with 5 results. Then the query with 5 results Q2 will have a larger overall DCG score. But we can’t say that query 2 was better than query 1.To allow a comparison of DCG across queries, we can use NDCG that normalizes the DCG values using the ideal order of the relevant items.Let’s take our previous example where we had already calculated the DCG values at various K values.For our example, ideally, we would have wanted the items to be sorted in descending order of relevance scores.Let’s calculate the ideal DCG(IDCG) for this order.Now we can calculate the NDCG@k for various k by diving DCG@k by IDCG@k as shown below:Thus, we get NDCG scores with a range between 0 and 1. A perfect ranking would get a score of 1. We can also compare NDCG@k scores of different queries since it’s a normalized score.Thus, we learned about various evaluation metrics for both binary and graded ground-truth labels and how each metric improves upon the previous.Most software products we encounter today have some form of search functionality integrated into them. We search for content on Google, videos on YouTube, products on Amazon, messages on Slack, emails on Gmail, people on Facebook, and so on.As users, the workflow is pretty simple. We can search for items by writing our queries in a search box and the ranking model in their system gives us back the top-N most relevant results.How do we evaluate how good the top-N results are?In this post, I will answer the above question by explaining the common offline metrics used in learning to rank problems. These metrics are useful not only for evaluating search results but also for problems like keyword extraction and item recommendation.Let’s take a simple toy example to understand the details and trade-offs of various evaluation metrics.We have a ranking model that gives us back 5-most relevant results for a certain query. The first, third, and fifth results were  as per our ground-truth annotation.Let’s look at various metrics to evaluate this simple example.This metric quantifies how many items in the top-K results were relevant. Mathematically, this is given by:For our example, precision@1 = 1 as all items in the first 1 results is relevant.Similarly, precision@2 = 0.5 as only one of the top-2 results are relevant.Thus, we can calculate the precision score for all k values.A limitation of precision@k is that it doesn’t consider the position of the relevant items. Consider two models A and B that have the same number of relevant results i.e. 3 out of 5.For model A, the first three items were relevant, while for model B, the last three items were relevant. Precision@5 would be the same for both of these models even though model A is better.This metric gives how many actual relevant results were shown out of all actual relevant results for the query. Mathematically, this is given by:For our example, recall@1 = 0.33 as only one of the 3 actual relevant items are present.Similarly, recall@3 = 0.67 as only two of the 3 actual relevant items are present.Thus, we can calculate the recall score for different K values.This is a combined metric that incorporates both Precision@k and Recall@k by taking their harmonic mean. We can calculate it as:Using the previously calculated values of precision and recall, we can calculate F1-scores for different K values as shown below.While precision, recall, and F1 give us a single-value metric, they don’t consider the order in which the returned search results are sent. To solve that limitation, people have devised order-aware metrics given below:This metric is useful when we want our system to return the best relevant item and want that item to be at a higher position. Mathematically, this is given by:where:To calculate MRR, we first calculate the . It is simply the reciprocal of the rank of the first correct relevant result and the value ranges from 0 to 1.For our example, the reciprocal rank is \(\frac{1}{1}=1\) as the first correct item is at position 1.Let’s see another example where the only one relevant result is present at the end of the list i.e. position 5. It gets a lower reciprocal rank score of 0.2.Let’s consider another example where none of the returned results are relevant. In such a scenario, the reciprocal rank will be 0.For multiple different queries, we can calculate the MRR by taking the mean of the reciprocal rank for each query.We can see that MRR doesn’t care about the position of the remaining relevant results. So, if your use-case requires returning multiple relevant results in the best possible way, MRR is not a suitable metric.Average Precision is a metric that evaluates whether all of the ground-truth relevant items selected by the model are ranked higher or not. Unlike MRR, it considers all the relevant items.Mathematically, it is given by:where:For our example, we can calculate the AP based on our Precision@K values for different K.To illustrate the advantage of AP, let’s take our previous example but place the 3 relevant results at the beginning. We can see that this gets a perfect AP score than the above example.If we want to evaluate average precision across multiple queries, we can use the MAP. It is simply the mean of the average precision for all queries. Mathematically, this is given bywhereLet’s take another toy example where we annotated the items not just as relevant or not-relevant but instead used a grading scale between 0 to 5 where 0 denotes least relevant and 5 denotes the most relevant.
We have a ranking model that gives us back 5-most relevant results for a certain query. The first item had a relevance score of 3 as per our ground-truth annotation, the second item has a relevance score of 2 and so on.Let’s understand the various metrics to evaluate this type of setup.This metric uses a simple idea to just sum up the relevance scores for top-K items. The total score is called cumulative gain. Mathematically, this is given by:For our example, CG@2 will be 5 because we add the first two relevance scores 3 and 2.Similarly, we can calculate the cumulative gain for all the K-values as:While simple, CG doesn’t take into account the order of the relevant items. So, even if we swap a less-relevant item to the first position, the CG@2 will be the same.We saw how a simple cumulative gain doesn’t take into account the position. But, we would normally want items with a high relevance score to be present at a better rank.Consider an example below. With the cumulative gain, we are simply adding the scores without taking into account their position.An item with a relevance score of 3 at position 1 is better than the same item with relevance score 3 at position 2.So, we need some way to penalize the scores by their position. DCG introduces a log-based penalty function to reduce the relevance score at each position. For 5 items, the penalty would beUsing this penalty, we can now calculate the discounted cumulative gain simply by taking the sum of the  . Mathematically, this is given by:To understand the behavior of the log-penalty, let’s plot ranking position in x-axis and the percentage of relevance score i.e. \(\frac{1}{log_{2}(i+1)} * 100\) in the y-axis. As seen, in position 1, we don’t apply any penalty and score remains unchanged. But, the percentage of score kept decays exponentially from 100% in position 1 to 63% in position 2, 50% in position 3, and so on.Let’s now calculate DCG for our example.Based on these penalized scores, we can now calculate DCG at various k values simply by taking their sum up to k.There is also an alternative formulation for DCG@K that gives more penalty if relevant items are ranked lower. This formulation is preferred more in industry.While DCG solves the issues with cumulative gain, it has a limitation. Suppose we a query Q1 with 3 results and query Q2 with 5 results. Then the query with 5 results Q2 will have a larger overall DCG score. But we can’t say that query 2 was better than query 1.To allow a comparison of DCG across queries, we can use NDCG that normalizes the DCG values using the ideal order of the relevant items.Let’s take our previous example where we had already calculated the DCG values at various K values.For our example, ideally, we would have wanted the items to be sorted in descending order of relevance scores.Let’s calculate the ideal DCG(IDCG) for this order.Now we can calculate the NDCG@k for various k by diving DCG@k by IDCG@k as shown below:Thus, we get NDCG scores with a range between 0 and 1. A perfect ranking would get a score of 1. We can also compare NDCG@k scores of different queries since it’s a normalized score.Thus, we learned about various evaluation metrics for both binary and graded ground-truth labels and how each metric improves upon the previous.Most software products we encounter today have some form of search functionality integrated into them. We search for content on Google, videos on YouTube, products on Amazon, messages on Slack, emails on Gmail, people on Facebook, and so on.As users, the workflow is pretty simple. We can search for items by writing our queries in a search box and the ranking model in their system gives us back the top-N most relevant results.How do we evaluate how good the top-N results are?In this post, I will answer the above question by explaining the common offline metrics used in learning to rank problems. These metrics are useful not only for evaluating search results but also for problems like keyword extraction and item recommendation.Let’s take a simple toy example to understand the details and trade-offs of various evaluation metrics.We have a ranking model that gives us back 5-most relevant results for a certain query. The first, third, and fifth results were  as per our ground-truth annotation.Let’s look at various metrics to evaluate this simple example.This metric quantifies how many items in the top-K results were relevant. Mathematically, this is given by:For our example, precision@1 = 1 as all items in the first 1 results is relevant.Similarly, precision@2 = 0.5 as only one of the top-2 results are relevant.Thus, we can calculate the precision score for all k values.A limitation of precision@k is that it doesn’t consider the position of the relevant items. Consider two models A and B that have the same number of relevant results i.e. 3 out of 5.For model A, the first three items were relevant, while for model B, the last three items were relevant. Precision@5 would be the same for both of these models even though model A is better.This metric gives how many actual relevant results were shown out of all actual relevant results for the query. Mathematically, this is given by:For our example, recall@1 = 0.33 as only one of the 3 actual relevant items are present.Similarly, recall@3 = 0.67 as only two of the 3 actual relevant items are present.Thus, we can calculate the recall score for different K values.This is a combined metric that incorporates both Precision@k and Recall@k by taking their harmonic mean. We can calculate it as:Using the previously calculated values of precision and recall, we can calculate F1-scores for different K values as shown below.While precision, recall, and F1 give us a single-value metric, they don’t consider the order in which the returned search results are sent. To solve that limitation, people have devised order-aware metrics given below:This metric is useful when we want our system to return the best relevant item and want that item to be at a higher position. Mathematically, this is given by:where:To calculate MRR, we first calculate the . It is simply the reciprocal of the rank of the first correct relevant result and the value ranges from 0 to 1.For our example, the reciprocal rank is \(\frac{1}{1}=1\) as the first correct item is at position 1.Let’s see another example where the only one relevant result is present at the end of the list i.e. position 5. It gets a lower reciprocal rank score of 0.2.Let’s consider another example where none of the returned results are relevant. In such a scenario, the reciprocal rank will be 0.For multiple different queries, we can calculate the MRR by taking the mean of the reciprocal rank for each query.We can see that MRR doesn’t care about the position of the remaining relevant results. So, if your use-case requires returning multiple relevant results in the best possible way, MRR is not a suitable metric.Average Precision is a metric that evaluates whether all of the ground-truth relevant items selected by the model are ranked higher or not. Unlike MRR, it considers all the relevant items.Mathematically, it is given by:where:For our example, we can calculate the AP based on our Precision@K values for different K.To illustrate the advantage of AP, let’s take our previous example but place the 3 relevant results at the beginning. We can see that this gets a perfect AP score than the above example.If we want to evaluate average precision across multiple queries, we can use the MAP. It is simply the mean of the average precision for all queries. Mathematically, this is given bywhereLet’s take another toy example where we annotated the items not just as relevant or not-relevant but instead used a grading scale between 0 to 5 where 0 denotes least relevant and 5 denotes the most relevant.
We have a ranking model that gives us back 5-most relevant results for a certain query. The first item had a relevance score of 3 as per our ground-truth annotation, the second item has a relevance score of 2 and so on.Let’s understand the various metrics to evaluate this type of setup.This metric uses a simple idea to just sum up the relevance scores for top-K items. The total score is called cumulative gain. Mathematically, this is given by:For our example, CG@2 will be 5 because we add the first two relevance scores 3 and 2.Similarly, we can calculate the cumulative gain for all the K-values as:While simple, CG doesn’t take into account the order of the relevant items. So, even if we swap a less-relevant item to the first position, the CG@2 will be the same.We saw how a simple cumulative gain doesn’t take into account the position. But, we would normally want items with a high relevance score to be present at a better rank.Consider an example below. With the cumulative gain, we are simply adding the scores without taking into account their position.An item with a relevance score of 3 at position 1 is better than the same item with relevance score 3 at position 2.So, we need some way to penalize the scores by their position. DCG introduces a log-based penalty function to reduce the relevance score at each position. For 5 items, the penalty would beUsing this penalty, we can now calculate the discounted cumulative gain simply by taking the sum of the  . Mathematically, this is given by:To understand the behavior of the log-penalty, let’s plot ranking position in x-axis and the percentage of relevance score i.e. \(\frac{1}{log_{2}(i+1)} * 100\) in the y-axis. As seen, in position 1, we don’t apply any penalty and score remains unchanged. But, the percentage of score kept decays exponentially from 100% in position 1 to 63% in position 2, 50% in position 3, and so on.Let’s now calculate DCG for our example.Based on these penalized scores, we can now calculate DCG at various k values simply by taking their sum up to k.There is also an alternative formulation for DCG@K that gives more penalty if relevant items are ranked lower. This formulation is preferred more in industry.While DCG solves the issues with cumulative gain, it has a limitation. Suppose we a query Q1 with 3 results and query Q2 with 5 results. Then the query with 5 results Q2 will have a larger overall DCG score. But we can’t say that query 2 was better than query 1.To allow a comparison of DCG across queries, we can use NDCG that normalizes the DCG values using the ideal order of the relevant items.Let’s take our previous example where we had already calculated the DCG values at various K values.For our example, ideally, we would have wanted the items to be sorted in descending order of relevance scores.Let’s calculate the ideal DCG(IDCG) for this order.Now we can calculate the NDCG@k for various k by diving DCG@k by IDCG@k as shown below:Thus, we get NDCG scores with a range between 0 and 1. A perfect ranking would get a score of 1. We can also compare NDCG@k scores of different queries since it’s a normalized score.Thus, we learned about various evaluation metrics for both binary and graded ground-truth labels and how each metric improves upon the previous.Most software products we encounter today have some form of search functionality integrated into them. We search for content on Google, videos on YouTube, products on Amazon, messages on Slack, emails on Gmail, people on Facebook, and so on.As users, the workflow is pretty simple. We can search for items by writing our queries in a search box and the ranking model in their system gives us back the top-N most relevant results.How do we evaluate how good the top-N results are?In this post, I will answer the above question by explaining the common offline metrics used in learning to rank problems. These metrics are useful not only for evaluating search results but also for problems like keyword extraction and item recommendation.Let’s take a simple toy example to understand the details and trade-offs of various evaluation metrics.We have a ranking model that gives us back 5-most relevant results for a certain query. The first, third, and fifth results were  as per our ground-truth annotation.Let’s look at various metrics to evaluate this simple example.This metric quantifies how many items in the top-K results were relevant. Mathematically, this is given by:For our example, precision@1 = 1 as all items in the first 1 results is relevant.Similarly, precision@2 = 0.5 as only one of the top-2 results are relevant.Thus, we can calculate the precision score for all k values.A limitation of precision@k is that it doesn’t consider the position of the relevant items. Consider two models A and B that have the same number of relevant results i.e. 3 out of 5.For model A, the first three items were relevant, while for model B, the last three items were relevant. Precision@5 would be the same for both of these models even though model A is better.This metric gives how many actual relevant results were shown out of all actual relevant results for the query. Mathematically, this is given by:For our example, recall@1 = 0.33 as only one of the 3 actual relevant items are present.Similarly, recall@3 = 0.67 as only two of the 3 actual relevant items are present.Thus, we can calculate the recall score for different K values.This is a combined metric that incorporates both Precision@k and Recall@k by taking their harmonic mean. We can calculate it as:Using the previously calculated values of precision and recall, we can calculate F1-scores for different K values as shown below.While precision, recall, and F1 give us a single-value metric, they don’t consider the order in which the returned search results are sent. To solve that limitation, people have devised order-aware metrics given below:This metric is useful when we want our system to return the best relevant item and want that item to be at a higher position. Mathematically, this is given by:where:To calculate MRR, we first calculate the . It is simply the reciprocal of the rank of the first correct relevant result and the value ranges from 0 to 1.For our example, the reciprocal rank is \(\frac{1}{1}=1\) as the first correct item is at position 1.Let’s see another example where the only one relevant result is present at the end of the list i.e. position 5. It gets a lower reciprocal rank score of 0.2.Let’s consider another example where none of the returned results are relevant. In such a scenario, the reciprocal rank will be 0.For multiple different queries, we can calculate the MRR by taking the mean of the reciprocal rank for each query.We can see that MRR doesn’t care about the position of the remaining relevant results. So, if your use-case requires returning multiple relevant results in the best possible way, MRR is not a suitable metric.Average Precision is a metric that evaluates whether all of the ground-truth relevant items selected by the model are ranked higher or not. Unlike MRR, it considers all the relevant items.Mathematically, it is given by:where:For our example, we can calculate the AP based on our Precision@K values for different K.To illustrate the advantage of AP, let’s take our previous example but place the 3 relevant results at the beginning. We can see that this gets a perfect AP score than the above example.If we want to evaluate average precision across multiple queries, we can use the MAP. It is simply the mean of the average precision for all queries. Mathematically, this is given bywhereLet’s take another toy example where we annotated the items not just as relevant or not-relevant but instead used a grading scale between 0 to 5 where 0 denotes least relevant and 5 denotes the most relevant.
We have a ranking model that gives us back 5-most relevant results for a certain query. The first item had a relevance score of 3 as per our ground-truth annotation, the second item has a relevance score of 2 and so on.Let’s understand the various metrics to evaluate this type of setup.This metric uses a simple idea to just sum up the relevance scores for top-K items. The total score is called cumulative gain. Mathematically, this is given by:For our example, CG@2 will be 5 because we add the first two relevance scores 3 and 2.Similarly, we can calculate the cumulative gain for all the K-values as:While simple, CG doesn’t take into account the order of the relevant items. So, even if we swap a less-relevant item to the first position, the CG@2 will be the same.We saw how a simple cumulative gain doesn’t take into account the position. But, we would normally want items with a high relevance score to be present at a better rank.Consider an example below. With the cumulative gain, we are simply adding the scores without taking into account their position.An item with a relevance score of 3 at position 1 is better than the same item with relevance score 3 at position 2.So, we need some way to penalize the scores by their position. DCG introduces a log-based penalty function to reduce the relevance score at each position. For 5 items, the penalty would beUsing this penalty, we can now calculate the discounted cumulative gain simply by taking the sum of the  . Mathematically, this is given by:To understand the behavior of the log-penalty, let’s plot ranking position in x-axis and the percentage of relevance score i.e. \(\frac{1}{log_{2}(i+1)} * 100\) in the y-axis. As seen, in position 1, we don’t apply any penalty and score remains unchanged. But, the percentage of score kept decays exponentially from 100% in position 1 to 63% in position 2, 50% in position 3, and so on.Let’s now calculate DCG for our example.Based on these penalized scores, we can now calculate DCG at various k values simply by taking their sum up to k.There is also an alternative formulation for DCG@K that gives more penalty if relevant items are ranked lower. This formulation is preferred more in industry.While DCG solves the issues with cumulative gain, it has a limitation. Suppose we a query Q1 with 3 results and query Q2 with 5 results. Then the query with 5 results Q2 will have a larger overall DCG score. But we can’t say that query 2 was better than query 1.To allow a comparison of DCG across queries, we can use NDCG that normalizes the DCG values using the ideal order of the relevant items.Let’s take our previous example where we had already calculated the DCG values at various K values.For our example, ideally, we would have wanted the items to be sorted in descending order of relevance scores.Let’s calculate the ideal DCG(IDCG) for this order.Now we can calculate the NDCG@k for various k by diving DCG@k by IDCG@k as shown below:Thus, we get NDCG scores with a range between 0 and 1. A perfect ranking would get a score of 1. We can also compare NDCG@k scores of different queries since it’s a normalized score.Thus, we learned about various evaluation metrics for both binary and graded ground-truth labels and how each metric improves upon the previous.Most software products we encounter today have some form of search functionality integrated into them. We search for content on Google, videos on YouTube, products on Amazon, messages on Slack, emails on Gmail, people on Facebook, and so on.As users, the workflow is pretty simple. We can search for items by writing our queries in a search box and the ranking model in their system gives us back the top-N most relevant results.How do we evaluate how good the top-N results are?In this post, I will answer the above question by explaining the common offline metrics used in learning to rank problems. These metrics are useful not only for evaluating search results but also for problems like keyword extraction and item recommendation.Let’s take a simple toy example to understand the details and trade-offs of various evaluation metrics.We have a ranking model that gives us back 5-most relevant results for a certain query. The first, third, and fifth results were  as per our ground-truth annotation.Let’s look at various metrics to evaluate this simple example.This metric quantifies how many items in the top-K results were relevant. Mathematically, this is given by:For our example, precision@1 = 1 as all items in the first 1 results is relevant.Similarly, precision@2 = 0.5 as only one of the top-2 results are relevant.Thus, we can calculate the precision score for all k values.A limitation of precision@k is that it doesn’t consider the position of the relevant items. Consider two models A and B that have the same number of relevant results i.e. 3 out of 5.For model A, the first three items were relevant, while for model B, the last three items were relevant. Precision@5 would be the same for both of these models even though model A is better.This metric gives how many actual relevant results were shown out of all actual relevant results for the query. Mathematically, this is given by:For our example, recall@1 = 0.33 as only one of the 3 actual relevant items are present.Similarly, recall@3 = 0.67 as only two of the 3 actual relevant items are present.Thus, we can calculate the recall score for different K values.This is a combined metric that incorporates both Precision@k and Recall@k by taking their harmonic mean. We can calculate it as:Using the previously calculated values of precision and recall, we can calculate F1-scores for different K values as shown below.While precision, recall, and F1 give us a single-value metric, they don’t consider the order in which the returned search results are sent. To solve that limitation, people have devised order-aware metrics given below:This metric is useful when we want our system to return the best relevant item and want that item to be at a higher position. Mathematically, this is given by:where:To calculate MRR, we first calculate the . It is simply the reciprocal of the rank of the first correct relevant result and the value ranges from 0 to 1.For our example, the reciprocal rank is \(\frac{1}{1}=1\) as the first correct item is at position 1.Let’s see another example where the only one relevant result is present at the end of the list i.e. position 5. It gets a lower reciprocal rank score of 0.2.Let’s consider another example where none of the returned results are relevant. In such a scenario, the reciprocal rank will be 0.For multiple different queries, we can calculate the MRR by taking the mean of the reciprocal rank for each query.We can see that MRR doesn’t care about the position of the remaining relevant results. So, if your use-case requires returning multiple relevant results in the best possible way, MRR is not a suitable metric.Average Precision is a metric that evaluates whether all of the ground-truth relevant items selected by the model are ranked higher or not. Unlike MRR, it considers all the relevant items.Mathematically, it is given by:where:For our example, we can calculate the AP based on our Precision@K values for different K.To illustrate the advantage of AP, let’s take our previous example but place the 3 relevant results at the beginning. We can see that this gets a perfect AP score than the above example.If we want to evaluate average precision across multiple queries, we can use the MAP. It is simply the mean of the average precision for all queries. Mathematically, this is given bywhereLet’s take another toy example where we annotated the items not just as relevant or not-relevant but instead used a grading scale between 0 to 5 where 0 denotes least relevant and 5 denotes the most relevant.
We have a ranking model that gives us back 5-most relevant results for a certain query. The first item had a relevance score of 3 as per our ground-truth annotation, the second item has a relevance score of 2 and so on.Let’s understand the various metrics to evaluate this type of setup.This metric uses a simple idea to just sum up the relevance scores for top-K items. The total score is called cumulative gain. Mathematically, this is given by:For our example, CG@2 will be 5 because we add the first two relevance scores 3 and 2.Similarly, we can calculate the cumulative gain for all the K-values as:While simple, CG doesn’t take into account the order of the relevant items. So, even if we swap a less-relevant item to the first position, the CG@2 will be the same.We saw how a simple cumulative gain doesn’t take into account the position. But, we would normally want items with a high relevance score to be present at a better rank.Consider an example below. With the cumulative gain, we are simply adding the scores without taking into account their position.An item with a relevance score of 3 at position 1 is better than the same item with relevance score 3 at position 2.So, we need some way to penalize the scores by their position. DCG introduces a log-based penalty function to reduce the relevance score at each position. For 5 items, the penalty would beUsing this penalty, we can now calculate the discounted cumulative gain simply by taking the sum of the  . Mathematically, this is given by:To understand the behavior of the log-penalty, let’s plot ranking position in x-axis and the percentage of relevance score i.e. \(\frac{1}{log_{2}(i+1)} * 100\) in the y-axis. As seen, in position 1, we don’t apply any penalty and score remains unchanged. But, the percentage of score kept decays exponentially from 100% in position 1 to 63% in position 2, 50% in position 3, and so on.Let’s now calculate DCG for our example.Based on these penalized scores, we can now calculate DCG at various k values simply by taking their sum up to k.There is also an alternative formulation for DCG@K that gives more penalty if relevant items are ranked lower. This formulation is preferred more in industry.While DCG solves the issues with cumulative gain, it has a limitation. Suppose we a query Q1 with 3 results and query Q2 with 5 results. Then the query with 5 results Q2 will have a larger overall DCG score. But we can’t say that query 2 was better than query 1.To allow a comparison of DCG across queries, we can use NDCG that normalizes the DCG values using the ideal order of the relevant items.Let’s take our previous example where we had already calculated the DCG values at various K values.For our example, ideally, we would have wanted the items to be sorted in descending order of relevance scores.Let’s calculate the ideal DCG(IDCG) for this order.Now we can calculate the NDCG@k for various k by diving DCG@k by IDCG@k as shown below:Thus, we get NDCG scores with a range between 0 and 1. A perfect ranking would get a score of 1. We can also compare NDCG@k scores of different queries since it’s a normalized score.Thus, we learned about various evaluation metrics for both binary and graded ground-truth labels and how each metric improves upon the previous.Most software products we encounter today have some form of search functionality integrated into them. We search for content on Google, videos on YouTube, products on Amazon, messages on Slack, emails on Gmail, people on Facebook, and so on.As users, the workflow is pretty simple. We can search for items by writing our queries in a search box and the ranking model in their system gives us back the top-N most relevant results.How do we evaluate how good the top-N results are?In this post, I will answer the above question by explaining the common offline metrics used in learning to rank problems. These metrics are useful not only for evaluating search results but also for problems like keyword extraction and item recommendation.Let’s take a simple toy example to understand the details and trade-offs of various evaluation metrics.We have a ranking model that gives us back 5-most relevant results for a certain query. The first, third, and fifth results were  as per our ground-truth annotation.Let’s look at various metrics to evaluate this simple example.This metric quantifies how many items in the top-K results were relevant. Mathematically, this is given by:For our example, precision@1 = 1 as all items in the first 1 results is relevant.Similarly, precision@2 = 0.5 as only one of the top-2 results are relevant.Thus, we can calculate the precision score for all k values.A limitation of precision@k is that it doesn’t consider the position of the relevant items. Consider two models A and B that have the same number of relevant results i.e. 3 out of 5.For model A, the first three items were relevant, while for model B, the last three items were relevant. Precision@5 would be the same for both of these models even though model A is better.This metric gives how many actual relevant results were shown out of all actual relevant results for the query. Mathematically, this is given by:For our example, recall@1 = 0.33 as only one of the 3 actual relevant items are present.Similarly, recall@3 = 0.67 as only two of the 3 actual relevant items are present.Thus, we can calculate the recall score for different K values.This is a combined metric that incorporates both Precision@k and Recall@k by taking their harmonic mean. We can calculate it as:Using the previously calculated values of precision and recall, we can calculate F1-scores for different K values as shown below.While precision, recall, and F1 give us a single-value metric, they don’t consider the order in which the returned search results are sent. To solve that limitation, people have devised order-aware metrics given below:This metric is useful when we want our system to return the best relevant item and want that item to be at a higher position. Mathematically, this is given by:where:To calculate MRR, we first calculate the . It is simply the reciprocal of the rank of the first correct relevant result and the value ranges from 0 to 1.For our example, the reciprocal rank is \(\frac{1}{1}=1\) as the first correct item is at position 1.Let’s see another example where the only one relevant result is present at the end of the list i.e. position 5. It gets a lower reciprocal rank score of 0.2.Let’s consider another example where none of the returned results are relevant. In such a scenario, the reciprocal rank will be 0.For multiple different queries, we can calculate the MRR by taking the mean of the reciprocal rank for each query.We can see that MRR doesn’t care about the position of the remaining relevant results. So, if your use-case requires returning multiple relevant results in the best possible way, MRR is not a suitable metric.Average Precision is a metric that evaluates whether all of the ground-truth relevant items selected by the model are ranked higher or not. Unlike MRR, it considers all the relevant items.Mathematically, it is given by:where:For our example, we can calculate the AP based on our Precision@K values for different K.To illustrate the advantage of AP, let’s take our previous example but place the 3 relevant results at the beginning. We can see that this gets a perfect AP score than the above example.If we want to evaluate average precision across multiple queries, we can use the MAP. It is simply the mean of the average precision for all queries. Mathematically, this is given bywhereLet’s take another toy example where we annotated the items not just as relevant or not-relevant but instead used a grading scale between 0 to 5 where 0 denotes least relevant and 5 denotes the most relevant.
We have a ranking model that gives us back 5-most relevant results for a certain query. The first item had a relevance score of 3 as per our ground-truth annotation, the second item has a relevance score of 2 and so on.Let’s understand the various metrics to evaluate this type of setup.This metric uses a simple idea to just sum up the relevance scores for top-K items. The total score is called cumulative gain. Mathematically, this is given by:For our example, CG@2 will be 5 because we add the first two relevance scores 3 and 2.Similarly, we can calculate the cumulative gain for all the K-values as:While simple, CG doesn’t take into account the order of the relevant items. So, even if we swap a less-relevant item to the first position, the CG@2 will be the same.We saw how a simple cumulative gain doesn’t take into account the position. But, we would normally want items with a high relevance score to be present at a better rank.Consider an example below. With the cumulative gain, we are simply adding the scores without taking into account their position.An item with a relevance score of 3 at position 1 is better than the same item with relevance score 3 at position 2.So, we need some way to penalize the scores by their position. DCG introduces a log-based penalty function to reduce the relevance score at each position. For 5 items, the penalty would beUsing this penalty, we can now calculate the discounted cumulative gain simply by taking the sum of the  . Mathematically, this is given by:To understand the behavior of the log-penalty, let’s plot ranking position in x-axis and the percentage of relevance score i.e. \(\frac{1}{log_{2}(i+1)} * 100\) in the y-axis. As seen, in position 1, we don’t apply any penalty and score remains unchanged. But, the percentage of score kept decays exponentially from 100% in position 1 to 63% in position 2, 50% in position 3, and so on.Let’s now calculate DCG for our example.Based on these penalized scores, we can now calculate DCG at various k values simply by taking their sum up to k.There is also an alternative formulation for DCG@K that gives more penalty if relevant items are ranked lower. This formulation is preferred more in industry.While DCG solves the issues with cumulative gain, it has a limitation. Suppose we a query Q1 with 3 results and query Q2 with 5 results. Then the query with 5 results Q2 will have a larger overall DCG score. But we can’t say that query 2 was better than query 1.To allow a comparison of DCG across queries, we can use NDCG that normalizes the DCG values using the ideal order of the relevant items.Let’s take our previous example where we had already calculated the DCG values at various K values.For our example, ideally, we would have wanted the items to be sorted in descending order of relevance scores.Let’s calculate the ideal DCG(IDCG) for this order.Now we can calculate the NDCG@k for various k by diving DCG@k by IDCG@k as shown below:Thus, we get NDCG scores with a range between 0 and 1. A perfect ranking would get a score of 1. We can also compare NDCG@k scores of different queries since it’s a normalized score.Thus, we learned about various evaluation metrics for both binary and graded ground-truth labels and how each metric improves upon the previous.Most software products we encounter today have some form of search functionality integrated into them. We search for content on Google, videos on YouTube, products on Amazon, messages on Slack, emails on Gmail, people on Facebook, and so on.As users, the workflow is pretty simple. We can search for items by writing our queries in a search box and the ranking model in their system gives us back the top-N most relevant results.How do we evaluate how good the top-N results are?In this post, I will answer the above question by explaining the common offline metrics used in learning to rank problems. These metrics are useful not only for evaluating search results but also for problems like keyword extraction and item recommendation.Let’s take a simple toy example to understand the details and trade-offs of various evaluation metrics.We have a ranking model that gives us back 5-most relevant results for a certain query. The first, third, and fifth results were  as per our ground-truth annotation.Let’s look at various metrics to evaluate this simple example.This metric quantifies how many items in the top-K results were relevant. Mathematically, this is given by:For our example, precision@1 = 1 as all items in the first 1 results is relevant.Similarly, precision@2 = 0.5 as only one of the top-2 results are relevant.Thus, we can calculate the precision score for all k values.A limitation of precision@k is that it doesn’t consider the position of the relevant items. Consider two models A and B that have the same number of relevant results i.e. 3 out of 5.For model A, the first three items were relevant, while for model B, the last three items were relevant. Precision@5 would be the same for both of these models even though model A is better.This metric gives how many actual relevant results were shown out of all actual relevant results for the query. Mathematically, this is given by:For our example, recall@1 = 0.33 as only one of the 3 actual relevant items are present.Similarly, recall@3 = 0.67 as only two of the 3 actual relevant items are present.Thus, we can calculate the recall score for different K values.This is a combined metric that incorporates both Precision@k and Recall@k by taking their harmonic mean. We can calculate it as:Using the previously calculated values of precision and recall, we can calculate F1-scores for different K values as shown below.While precision, recall, and F1 give us a single-value metric, they don’t consider the order in which the returned search results are sent. To solve that limitation, people have devised order-aware metrics given below:This metric is useful when we want our system to return the best relevant item and want that item to be at a higher position. Mathematically, this is given by:where:To calculate MRR, we first calculate the . It is simply the reciprocal of the rank of the first correct relevant result and the value ranges from 0 to 1.For our example, the reciprocal rank is \(\frac{1}{1}=1\) as the first correct item is at position 1.Let’s see another example where the only one relevant result is present at the end of the list i.e. position 5. It gets a lower reciprocal rank score of 0.2.Let’s consider another example where none of the returned results are relevant. In such a scenario, the reciprocal rank will be 0.For multiple different queries, we can calculate the MRR by taking the mean of the reciprocal rank for each query.We can see that MRR doesn’t care about the position of the remaining relevant results. So, if your use-case requires returning multiple relevant results in the best possible way, MRR is not a suitable metric.Average Precision is a metric that evaluates whether all of the ground-truth relevant items selected by the model are ranked higher or not. Unlike MRR, it considers all the relevant items.Mathematically, it is given by:where:For our example, we can calculate the AP based on our Precision@K values for different K.To illustrate the advantage of AP, let’s take our previous example but place the 3 relevant results at the beginning. We can see that this gets a perfect AP score than the above example.If we want to evaluate average precision across multiple queries, we can use the MAP. It is simply the mean of the average precision for all queries. Mathematically, this is given bywhereLet’s take another toy example where we annotated the items not just as relevant or not-relevant but instead used a grading scale between 0 to 5 where 0 denotes least relevant and 5 denotes the most relevant.
We have a ranking model that gives us back 5-most relevant results for a certain query. The first item had a relevance score of 3 as per our ground-truth annotation, the second item has a relevance score of 2 and so on.Let’s understand the various metrics to evaluate this type of setup.This metric uses a simple idea to just sum up the relevance scores for top-K items. The total score is called cumulative gain. Mathematically, this is given by:For our example, CG@2 will be 5 because we add the first two relevance scores 3 and 2.Similarly, we can calculate the cumulative gain for all the K-values as:While simple, CG doesn’t take into account the order of the relevant items. So, even if we swap a less-relevant item to the first position, the CG@2 will be the same.We saw how a simple cumulative gain doesn’t take into account the position. But, we would normally want items with a high relevance score to be present at a better rank.Consider an example below. With the cumulative gain, we are simply adding the scores without taking into account their position.An item with a relevance score of 3 at position 1 is better than the same item with relevance score 3 at position 2.So, we need some way to penalize the scores by their position. DCG introduces a log-based penalty function to reduce the relevance score at each position. For 5 items, the penalty would beUsing this penalty, we can now calculate the discounted cumulative gain simply by taking the sum of the  . Mathematically, this is given by:To understand the behavior of the log-penalty, let’s plot ranking position in x-axis and the percentage of relevance score i.e. \(\frac{1}{log_{2}(i+1)} * 100\) in the y-axis. As seen, in position 1, we don’t apply any penalty and score remains unchanged. But, the percentage of score kept decays exponentially from 100% in position 1 to 63% in position 2, 50% in position 3, and so on.Let’s now calculate DCG for our example.Based on these penalized scores, we can now calculate DCG at various k values simply by taking their sum up to k.There is also an alternative formulation for DCG@K that gives more penalty if relevant items are ranked lower. This formulation is preferred more in industry.While DCG solves the issues with cumulative gain, it has a limitation. Suppose we a query Q1 with 3 results and query Q2 with 5 results. Then the query with 5 results Q2 will have a larger overall DCG score. But we can’t say that query 2 was better than query 1.To allow a comparison of DCG across queries, we can use NDCG that normalizes the DCG values using the ideal order of the relevant items.Let’s take our previous example where we had already calculated the DCG values at various K values.For our example, ideally, we would have wanted the items to be sorted in descending order of relevance scores.Let’s calculate the ideal DCG(IDCG) for this order.Now we can calculate the NDCG@k for various k by diving DCG@k by IDCG@k as shown below:Thus, we get NDCG scores with a range between 0 and 1. A perfect ranking would get a score of 1. We can also compare NDCG@k scores of different queries since it’s a normalized score.Thus, we learned about various evaluation metrics for both binary and graded ground-truth labels and how each metric improves upon the previous.Most software products we encounter today have some form of search functionality integrated into them. We search for content on Google, videos on YouTube, products on Amazon, messages on Slack, emails on Gmail, people on Facebook, and so on.As users, the workflow is pretty simple. We can search for items by writing our queries in a search box and the ranking model in their system gives us back the top-N most relevant results.How do we evaluate how good the top-N results are?In this post, I will answer the above question by explaining the common offline metrics used in learning to rank problems. These metrics are useful not only for evaluating search results but also for problems like keyword extraction and item recommendation.Let’s take a simple toy example to understand the details and trade-offs of various evaluation metrics.We have a ranking model that gives us back 5-most relevant results for a certain query. The first, third, and fifth results were  as per our ground-truth annotation.Let’s look at various metrics to evaluate this simple example.This metric quantifies how many items in the top-K results were relevant. Mathematically, this is given by:For our example, precision@1 = 1 as all items in the first 1 results is relevant.Similarly, precision@2 = 0.5 as only one of the top-2 results are relevant.Thus, we can calculate the precision score for all k values.A limitation of precision@k is that it doesn’t consider the position of the relevant items. Consider two models A and B that have the same number of relevant results i.e. 3 out of 5.For model A, the first three items were relevant, while for model B, the last three items were relevant. Precision@5 would be the same for both of these models even though model A is better.This metric gives how many actual relevant results were shown out of all actual relevant results for the query. Mathematically, this is given by:For our example, recall@1 = 0.33 as only one of the 3 actual relevant items are present.Similarly, recall@3 = 0.67 as only two of the 3 actual relevant items are present.Thus, we can calculate the recall score for different K values.This is a combined metric that incorporates both Precision@k and Recall@k by taking their harmonic mean. We can calculate it as:Using the previously calculated values of precision and recall, we can calculate F1-scores for different K values as shown below.While precision, recall, and F1 give us a single-value metric, they don’t consider the order in which the returned search results are sent. To solve that limitation, people have devised order-aware metrics given below:This metric is useful when we want our system to return the best relevant item and want that item to be at a higher position. Mathematically, this is given by:where:To calculate MRR, we first calculate the . It is simply the reciprocal of the rank of the first correct relevant result and the value ranges from 0 to 1.For our example, the reciprocal rank is \(\frac{1}{1}=1\) as the first correct item is at position 1.Let’s see another example where the only one relevant result is present at the end of the list i.e. position 5. It gets a lower reciprocal rank score of 0.2.Let’s consider another example where none of the returned results are relevant. In such a scenario, the reciprocal rank will be 0.For multiple different queries, we can calculate the MRR by taking the mean of the reciprocal rank for each query.We can see that MRR doesn’t care about the position of the remaining relevant results. So, if your use-case requires returning multiple relevant results in the best possible way, MRR is not a suitable metric.Average Precision is a metric that evaluates whether all of the ground-truth relevant items selected by the model are ranked higher or not. Unlike MRR, it considers all the relevant items.Mathematically, it is given by:where:For our example, we can calculate the AP based on our Precision@K values for different K.To illustrate the advantage of AP, let’s take our previous example but place the 3 relevant results at the beginning. We can see that this gets a perfect AP score than the above example.If we want to evaluate average precision across multiple queries, we can use the MAP. It is simply the mean of the average precision for all queries. Mathematically, this is given bywhereLet’s take another toy example where we annotated the items not just as relevant or not-relevant but instead used a grading scale between 0 to 5 where 0 denotes least relevant and 5 denotes the most relevant.
We have a ranking model that gives us back 5-most relevant results for a certain query. The first item had a relevance score of 3 as per our ground-truth annotation, the second item has a relevance score of 2 and so on.Let’s understand the various metrics to evaluate this type of setup.This metric uses a simple idea to just sum up the relevance scores for top-K items. The total score is called cumulative gain. Mathematically, this is given by:For our example, CG@2 will be 5 because we add the first two relevance scores 3 and 2.Similarly, we can calculate the cumulative gain for all the K-values as:While simple, CG doesn’t take into account the order of the relevant items. So, even if we swap a less-relevant item to the first position, the CG@2 will be the same.We saw how a simple cumulative gain doesn’t take into account the position. But, we would normally want items with a high relevance score to be present at a better rank.Consider an example below. With the cumulative gain, we are simply adding the scores without taking into account their position.An item with a relevance score of 3 at position 1 is better than the same item with relevance score 3 at position 2.So, we need some way to penalize the scores by their position. DCG introduces a log-based penalty function to reduce the relevance score at each position. For 5 items, the penalty would beUsing this penalty, we can now calculate the discounted cumulative gain simply by taking the sum of the  . Mathematically, this is given by:To understand the behavior of the log-penalty, let’s plot ranking position in x-axis and the percentage of relevance score i.e. \(\frac{1}{log_{2}(i+1)} * 100\) in the y-axis. As seen, in position 1, we don’t apply any penalty and score remains unchanged. But, the percentage of score kept decays exponentially from 100% in position 1 to 63% in position 2, 50% in position 3, and so on.Let’s now calculate DCG for our example.Based on these penalized scores, we can now calculate DCG at various k values simply by taking their sum up to k.There is also an alternative formulation for DCG@K that gives more penalty if relevant items are ranked lower. This formulation is preferred more in industry.While DCG solves the issues with cumulative gain, it has a limitation. Suppose we a query Q1 with 3 results and query Q2 with 5 results. Then the query with 5 results Q2 will have a larger overall DCG score. But we can’t say that query 2 was better than query 1.To allow a comparison of DCG across queries, we can use NDCG that normalizes the DCG values using the ideal order of the relevant items.Let’s take our previous example where we had already calculated the DCG values at various K values.For our example, ideally, we would have wanted the items to be sorted in descending order of relevance scores.Let’s calculate the ideal DCG(IDCG) for this order.Now we can calculate the NDCG@k for various k by diving DCG@k by IDCG@k as shown below:Thus, we get NDCG scores with a range between 0 and 1. A perfect ranking would get a score of 1. We can also compare NDCG@k scores of different queries since it’s a normalized score.Thus, we learned about various evaluation metrics for both binary and graded ground-truth labels and how each metric improves upon the previous.Most software products we encounter today have some form of search functionality integrated into them. We search for content on Google, videos on YouTube, products on Amazon, messages on Slack, emails on Gmail, people on Facebook, and so on.As users, the workflow is pretty simple. We can search for items by writing our queries in a search box and the ranking model in their system gives us back the top-N most relevant results.How do we evaluate how good the top-N results are?In this post, I will answer the above question by explaining the common offline metrics used in learning to rank problems. These metrics are useful not only for evaluating search results but also for problems like keyword extraction and item recommendation.Let’s take a simple toy example to understand the details and trade-offs of various evaluation metrics.We have a ranking model that gives us back 5-most relevant results for a certain query. The first, third, and fifth results were  as per our ground-truth annotation.Let’s look at various metrics to evaluate this simple example.This metric quantifies how many items in the top-K results were relevant. Mathematically, this is given by:For our example, precision@1 = 1 as all items in the first 1 results is relevant.Similarly, precision@2 = 0.5 as only one of the top-2 results are relevant.Thus, we can calculate the precision score for all k values.A limitation of precision@k is that it doesn’t consider the position of the relevant items. Consider two models A and B that have the same number of relevant results i.e. 3 out of 5.For model A, the first three items were relevant, while for model B, the last three items were relevant. Precision@5 would be the same for both of these models even though model A is better.This metric gives how many actual relevant results were shown out of all actual relevant results for the query. Mathematically, this is given by:For our example, recall@1 = 0.33 as only one of the 3 actual relevant items are present.Similarly, recall@3 = 0.67 as only two of the 3 actual relevant items are present.Thus, we can calculate the recall score for different K values.This is a combined metric that incorporates both Precision@k and Recall@k by taking their harmonic mean. We can calculate it as:Using the previously calculated values of precision and recall, we can calculate F1-scores for different K values as shown below.While precision, recall, and F1 give us a single-value metric, they don’t consider the order in which the returned search results are sent. To solve that limitation, people have devised order-aware metrics given below:This metric is useful when we want our system to return the best relevant item and want that item to be at a higher position. Mathematically, this is given by:where:To calculate MRR, we first calculate the . It is simply the reciprocal of the rank of the first correct relevant result and the value ranges from 0 to 1.For our example, the reciprocal rank is \(\frac{1}{1}=1\) as the first correct item is at position 1.Let’s see another example where the only one relevant result is present at the end of the list i.e. position 5. It gets a lower reciprocal rank score of 0.2.Let’s consider another example where none of the returned results are relevant. In such a scenario, the reciprocal rank will be 0.For multiple different queries, we can calculate the MRR by taking the mean of the reciprocal rank for each query.We can see that MRR doesn’t care about the position of the remaining relevant results. So, if your use-case requires returning multiple relevant results in the best possible way, MRR is not a suitable metric.Average Precision is a metric that evaluates whether all of the ground-truth relevant items selected by the model are ranked higher or not. Unlike MRR, it considers all the relevant items.Mathematically, it is given by:where:For our example, we can calculate the AP based on our Precision@K values for different K.To illustrate the advantage of AP, let’s take our previous example but place the 3 relevant results at the beginning. We can see that this gets a perfect AP score than the above example.If we want to evaluate average precision across multiple queries, we can use the MAP. It is simply the mean of the average precision for all queries. Mathematically, this is given bywhereLet’s take another toy example where we annotated the items not just as relevant or not-relevant but instead used a grading scale between 0 to 5 where 0 denotes least relevant and 5 denotes the most relevant.
We have a ranking model that gives us back 5-most relevant results for a certain query. The first item had a relevance score of 3 as per our ground-truth annotation, the second item has a relevance score of 2 and so on.Let’s understand the various metrics to evaluate this type of setup.This metric uses a simple idea to just sum up the relevance scores for top-K items. The total score is called cumulative gain. Mathematically, this is given by:For our example, CG@2 will be 5 because we add the first two relevance scores 3 and 2.Similarly, we can calculate the cumulative gain for all the K-values as:While simple, CG doesn’t take into account the order of the relevant items. So, even if we swap a less-relevant item to the first position, the CG@2 will be the same.We saw how a simple cumulative gain doesn’t take into account the position. But, we would normally want items with a high relevance score to be present at a better rank.Consider an example below. With the cumulative gain, we are simply adding the scores without taking into account their position.An item with a relevance score of 3 at position 1 is better than the same item with relevance score 3 at position 2.So, we need some way to penalize the scores by their position. DCG introduces a log-based penalty function to reduce the relevance score at each position. For 5 items, the penalty would beUsing this penalty, we can now calculate the discounted cumulative gain simply by taking the sum of the  . Mathematically, this is given by:To understand the behavior of the log-penalty, let’s plot ranking position in x-axis and the percentage of relevance score i.e. \(\frac{1}{log_{2}(i+1)} * 100\) in the y-axis. As seen, in position 1, we don’t apply any penalty and score remains unchanged. But, the percentage of score kept decays exponentially from 100% in position 1 to 63% in position 2, 50% in position 3, and so on.Let’s now calculate DCG for our example.Based on these penalized scores, we can now calculate DCG at various k values simply by taking their sum up to k.There is also an alternative formulation for DCG@K that gives more penalty if relevant items are ranked lower. This formulation is preferred more in industry.While DCG solves the issues with cumulative gain, it has a limitation. Suppose we a query Q1 with 3 results and query Q2 with 5 results. Then the query with 5 results Q2 will have a larger overall DCG score. But we can’t say that query 2 was better than query 1.To allow a comparison of DCG across queries, we can use NDCG that normalizes the DCG values using the ideal order of the relevant items.Let’s take our previous example where we had already calculated the DCG values at various K values.For our example, ideally, we would have wanted the items to be sorted in descending order of relevance scores.Let’s calculate the ideal DCG(IDCG) for this order.Now we can calculate the NDCG@k for various k by diving DCG@k by IDCG@k as shown below:Thus, we get NDCG scores with a range between 0 and 1. A perfect ranking would get a score of 1. We can also compare NDCG@k scores of different queries since it’s a normalized score.Thus, we learned about various evaluation metrics for both binary and graded ground-truth labels and how each metric improves upon the previous.Most software products we encounter today have some form of search functionality integrated into them. We search for content on Google, videos on YouTube, products on Amazon, messages on Slack, emails on Gmail, people on Facebook, and so on.As users, the workflow is pretty simple. We can search for items by writing our queries in a search box and the ranking model in their system gives us back the top-N most relevant results.How do we evaluate how good the top-N results are?In this post, I will answer the above question by explaining the common offline metrics used in learning to rank problems. These metrics are useful not only for evaluating search results but also for problems like keyword extraction and item recommendation.Let’s take a simple toy example to understand the details and trade-offs of various evaluation metrics.We have a ranking model that gives us back 5-most relevant results for a certain query. The first, third, and fifth results were  as per our ground-truth annotation.Let’s look at various metrics to evaluate this simple example.This metric quantifies how many items in the top-K results were relevant. Mathematically, this is given by:For our example, precision@1 = 1 as all items in the first 1 results is relevant.Similarly, precision@2 = 0.5 as only one of the top-2 results are relevant.Thus, we can calculate the precision score for all k values.A limitation of precision@k is that it doesn’t consider the position of the relevant items. Consider two models A and B that have the same number of relevant results i.e. 3 out of 5.For model A, the first three items were relevant, while for model B, the last three items were relevant. Precision@5 would be the same for both of these models even though model A is better.This metric gives how many actual relevant results were shown out of all actual relevant results for the query. Mathematically, this is given by:For our example, recall@1 = 0.33 as only one of the 3 actual relevant items are present.Similarly, recall@3 = 0.67 as only two of the 3 actual relevant items are present.Thus, we can calculate the recall score for different K values.This is a combined metric that incorporates both Precision@k and Recall@k by taking their harmonic mean. We can calculate it as:Using the previously calculated values of precision and recall, we can calculate F1-scores for different K values as shown below.While precision, recall, and F1 give us a single-value metric, they don’t consider the order in which the returned search results are sent. To solve that limitation, people have devised order-aware metrics given below:This metric is useful when we want our system to return the best relevant item and want that item to be at a higher position. Mathematically, this is given by:where:To calculate MRR, we first calculate the . It is simply the reciprocal of the rank of the first correct relevant result and the value ranges from 0 to 1.For our example, the reciprocal rank is \(\frac{1}{1}=1\) as the first correct item is at position 1.Let’s see another example where the only one relevant result is present at the end of the list i.e. position 5. It gets a lower reciprocal rank score of 0.2.Let’s consider another example where none of the returned results are relevant. In such a scenario, the reciprocal rank will be 0.For multiple different queries, we can calculate the MRR by taking the mean of the reciprocal rank for each query.We can see that MRR doesn’t care about the position of the remaining relevant results. So, if your use-case requires returning multiple relevant results in the best possible way, MRR is not a suitable metric.Average Precision is a metric that evaluates whether all of the ground-truth relevant items selected by the model are ranked higher or not. Unlike MRR, it considers all the relevant items.Mathematically, it is given by:where:For our example, we can calculate the AP based on our Precision@K values for different K.To illustrate the advantage of AP, let’s take our previous example but place the 3 relevant results at the beginning. We can see that this gets a perfect AP score than the above example.If we want to evaluate average precision across multiple queries, we can use the MAP. It is simply the mean of the average precision for all queries. Mathematically, this is given bywhereLet’s take another toy example where we annotated the items not just as relevant or not-relevant but instead used a grading scale between 0 to 5 where 0 denotes least relevant and 5 denotes the most relevant.
We have a ranking model that gives us back 5-most relevant results for a certain query. The first item had a relevance score of 3 as per our ground-truth annotation, the second item has a relevance score of 2 and so on.Let’s understand the various metrics to evaluate this type of setup.This metric uses a simple idea to just sum up the relevance scores for top-K items. The total score is called cumulative gain. Mathematically, this is given by:For our example, CG@2 will be 5 because we add the first two relevance scores 3 and 2.Similarly, we can calculate the cumulative gain for all the K-values as:While simple, CG doesn’t take into account the order of the relevant items. So, even if we swap a less-relevant item to the first position, the CG@2 will be the same.We saw how a simple cumulative gain doesn’t take into account the position. But, we would normally want items with a high relevance score to be present at a better rank.Consider an example below. With the cumulative gain, we are simply adding the scores without taking into account their position.An item with a relevance score of 3 at position 1 is better than the same item with relevance score 3 at position 2.So, we need some way to penalize the scores by their position. DCG introduces a log-based penalty function to reduce the relevance score at each position. For 5 items, the penalty would beUsing this penalty, we can now calculate the discounted cumulative gain simply by taking the sum of the  . Mathematically, this is given by:To understand the behavior of the log-penalty, let’s plot ranking position in x-axis and the percentage of relevance score i.e. \(\frac{1}{log_{2}(i+1)} * 100\) in the y-axis. As seen, in position 1, we don’t apply any penalty and score remains unchanged. But, the percentage of score kept decays exponentially from 100% in position 1 to 63% in position 2, 50% in position 3, and so on.Let’s now calculate DCG for our example.Based on these penalized scores, we can now calculate DCG at various k values simply by taking their sum up to k.There is also an alternative formulation for DCG@K that gives more penalty if relevant items are ranked lower. This formulation is preferred more in industry.While DCG solves the issues with cumulative gain, it has a limitation. Suppose we a query Q1 with 3 results and query Q2 with 5 results. Then the query with 5 results Q2 will have a larger overall DCG score. But we can’t say that query 2 was better than query 1.To allow a comparison of DCG across queries, we can use NDCG that normalizes the DCG values using the ideal order of the relevant items.Let’s take our previous example where we had already calculated the DCG values at various K values.For our example, ideally, we would have wanted the items to be sorted in descending order of relevance scores.Let’s calculate the ideal DCG(IDCG) for this order.Now we can calculate the NDCG@k for various k by diving DCG@k by IDCG@k as shown below:Thus, we get NDCG scores with a range between 0 and 1. A perfect ranking would get a score of 1. We can also compare NDCG@k scores of different queries since it’s a normalized score.Thus, we learned about various evaluation metrics for both binary and graded ground-truth labels and how each metric improves upon the previous.