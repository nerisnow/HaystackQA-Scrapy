I recently came across an interesting thread on Twitter discussing a hypothetical scenario where research papers are published on GitHub and subsequent papers are diffs over the original paper. Information overload has been a real problem in ML with so many new papers coming every month.If you could represent a paper as a code diff, many papers could be compressed down to <50 lines :) The diff would also be more intuitive to read and eval standardized.Some ideas are so different that this wouldn’t apply, but I think it would work well for the majority. 
This post is a fun experiment showcasing how the commit history could look like for the BERT paper and some of its subsequent variants.
commit Author: Devlin et al.Date: Thu Oct 11 00:50:01 2018 +0000-Transformer Decoder+Masked Language Modeling+Next Sentence Prediction+WordPiece 30Kcommit Author: Lample et al.Date: Tue Jan 22 13:22:34 2019 +0000+Translation Language Modeling(TLM)+Causal Language Modeling(CLM)commit Author: Lee et al.Date: Fri Jan 25 05:57:24 2019 +0000+PubMed Abstracts data+PubMed Central Full Texts datacommit Author: Liu et al.Date: Thu Jan 31 18:07:25 2019 +0000+Multi-task Learningcommit Author: Beltagy et al.Date: Tue Mar 26 05:11:46 2019 +0000-BERT WordPiece Vocabulary-English Wikipedia-BookCorpus+1.14M Semantic Scholar Papers(Biomedial + Computer Science)+ScispaCy segmentation+SciVOCAB WordPiece Vocabularycommit Author: Yang et al.Date: Wed Jun 19 17:35:48 2019 +0000-Masked Language Modeling-BERT Transformer+Permutation Language Modeling+Transformer-XL+Two-stream self-attention+SentencePiece Tokenizercommit Author: Joshi et al.Date: Wed Jul 24 15:43:40 2019 +0000-Random Token Masking-Next Sentence Prediction-Bi-sequence Training+Continuous Span Masking+Span-Boundary Objective(SBO)+Single-Sequence Trainingcommit Author: Liu et al.Date: Fri Jul 26 17:48:29 2019 +0000-Next Sentence Prediction-Static Masking of Tokens+Dynamic Masking of Tokens+Byte Pair Encoding(BPE) 50K+Large batch size+CC-NEWS(76G) dataset+OpenWebText(38G) dataset+Stories(31G) datasetcommit Author: Reimers et al.Date: Tue Aug 27 08:50:17 2019 +0000+Siamese Network Structure+Finetuning on SNLI and MNLIcommit Author: Lan et al.Date: Thu Sep 26 07:06:13 2019 +0000-Next Sentence Prediction+Sentence Order Prediction+Cross-layer Parameter Sharing+Factorized Embeddings+SentencePiece Tokenizercommit Author: Sanh et al.Date: Wed Oct 2 17:56:28 2019 +0000-Next Sentence Prediction-Token-Type Embeddings-[CLS] pooling+Knowledge Distillation+Cosine Embedding Loss+Dynamic Maskingcommit Author: Martin et al.Date: Sun Nov 10 10:46:37 2019 +0000-BERT-English+ROBERTA+French OSCAR dataset(138GB)+Whole-word Masking(WWM)+SentencePiece Tokenizercommit Author: Le et al.Date: Wed Dec 11 14:59:32 2019 +0000-BERT-English+ROBERTA+fastBPE+Stochastic Depth+French dataset(71GB)+FLUE(French Language Understanding Evaluation) benchmarkI recently came across an interesting thread on Twitter discussing a hypothetical scenario where research papers are published on GitHub and subsequent papers are diffs over the original paper. Information overload has been a real problem in ML with so many new papers coming every month.If you could represent a paper as a code diff, many papers could be compressed down to <50 lines :) The diff would also be more intuitive to read and eval standardized.Some ideas are so different that this wouldn’t apply, but I think it would work well for the majority. 
This post is a fun experiment showcasing how the commit history could look like for the BERT paper and some of its subsequent variants.
commit Author: Devlin et al.Date: Thu Oct 11 00:50:01 2018 +0000-Transformer Decoder+Masked Language Modeling+Next Sentence Prediction+WordPiece 30Kcommit Author: Lample et al.Date: Tue Jan 22 13:22:34 2019 +0000+Translation Language Modeling(TLM)+Causal Language Modeling(CLM)commit Author: Lee et al.Date: Fri Jan 25 05:57:24 2019 +0000+PubMed Abstracts data+PubMed Central Full Texts datacommit Author: Liu et al.Date: Thu Jan 31 18:07:25 2019 +0000+Multi-task Learningcommit Author: Beltagy et al.Date: Tue Mar 26 05:11:46 2019 +0000-BERT WordPiece Vocabulary-English Wikipedia-BookCorpus+1.14M Semantic Scholar Papers(Biomedial + Computer Science)+ScispaCy segmentation+SciVOCAB WordPiece Vocabularycommit Author: Yang et al.Date: Wed Jun 19 17:35:48 2019 +0000-Masked Language Modeling-BERT Transformer+Permutation Language Modeling+Transformer-XL+Two-stream self-attention+SentencePiece Tokenizercommit Author: Joshi et al.Date: Wed Jul 24 15:43:40 2019 +0000-Random Token Masking-Next Sentence Prediction-Bi-sequence Training+Continuous Span Masking+Span-Boundary Objective(SBO)+Single-Sequence Trainingcommit Author: Liu et al.Date: Fri Jul 26 17:48:29 2019 +0000-Next Sentence Prediction-Static Masking of Tokens+Dynamic Masking of Tokens+Byte Pair Encoding(BPE) 50K+Large batch size+CC-NEWS(76G) dataset+OpenWebText(38G) dataset+Stories(31G) datasetcommit Author: Reimers et al.Date: Tue Aug 27 08:50:17 2019 +0000+Siamese Network Structure+Finetuning on SNLI and MNLIcommit Author: Lan et al.Date: Thu Sep 26 07:06:13 2019 +0000-Next Sentence Prediction+Sentence Order Prediction+Cross-layer Parameter Sharing+Factorized Embeddings+SentencePiece Tokenizercommit Author: Sanh et al.Date: Wed Oct 2 17:56:28 2019 +0000-Next Sentence Prediction-Token-Type Embeddings-[CLS] pooling+Knowledge Distillation+Cosine Embedding Loss+Dynamic Maskingcommit Author: Martin et al.Date: Sun Nov 10 10:46:37 2019 +0000-BERT-English+ROBERTA+French OSCAR dataset(138GB)+Whole-word Masking(WWM)+SentencePiece Tokenizercommit Author: Le et al.Date: Wed Dec 11 14:59:32 2019 +0000-BERT-English+ROBERTA+fastBPE+Stochastic Depth+French dataset(71GB)+FLUE(French Language Understanding Evaluation) benchmarkI recently came across an interesting thread on Twitter discussing a hypothetical scenario where research papers are published on GitHub and subsequent papers are diffs over the original paper. Information overload has been a real problem in ML with so many new papers coming every month.If you could represent a paper as a code diff, many papers could be compressed down to <50 lines :) The diff would also be more intuitive to read and eval standardized.Some ideas are so different that this wouldn’t apply, but I think it would work well for the majority. 
This post is a fun experiment showcasing how the commit history could look like for the BERT paper and some of its subsequent variants.
commit Author: Devlin et al.Date: Thu Oct 11 00:50:01 2018 +0000-Transformer Decoder+Masked Language Modeling+Next Sentence Prediction+WordPiece 30Kcommit Author: Lample et al.Date: Tue Jan 22 13:22:34 2019 +0000+Translation Language Modeling(TLM)+Causal Language Modeling(CLM)commit Author: Lee et al.Date: Fri Jan 25 05:57:24 2019 +0000+PubMed Abstracts data+PubMed Central Full Texts datacommit Author: Liu et al.Date: Thu Jan 31 18:07:25 2019 +0000+Multi-task Learningcommit Author: Beltagy et al.Date: Tue Mar 26 05:11:46 2019 +0000-BERT WordPiece Vocabulary-English Wikipedia-BookCorpus+1.14M Semantic Scholar Papers(Biomedial + Computer Science)+ScispaCy segmentation+SciVOCAB WordPiece Vocabularycommit Author: Yang et al.Date: Wed Jun 19 17:35:48 2019 +0000-Masked Language Modeling-BERT Transformer+Permutation Language Modeling+Transformer-XL+Two-stream self-attention+SentencePiece Tokenizercommit Author: Joshi et al.Date: Wed Jul 24 15:43:40 2019 +0000-Random Token Masking-Next Sentence Prediction-Bi-sequence Training+Continuous Span Masking+Span-Boundary Objective(SBO)+Single-Sequence Trainingcommit Author: Liu et al.Date: Fri Jul 26 17:48:29 2019 +0000-Next Sentence Prediction-Static Masking of Tokens+Dynamic Masking of Tokens+Byte Pair Encoding(BPE) 50K+Large batch size+CC-NEWS(76G) dataset+OpenWebText(38G) dataset+Stories(31G) datasetcommit Author: Reimers et al.Date: Tue Aug 27 08:50:17 2019 +0000+Siamese Network Structure+Finetuning on SNLI and MNLIcommit Author: Lan et al.Date: Thu Sep 26 07:06:13 2019 +0000-Next Sentence Prediction+Sentence Order Prediction+Cross-layer Parameter Sharing+Factorized Embeddings+SentencePiece Tokenizercommit Author: Sanh et al.Date: Wed Oct 2 17:56:28 2019 +0000-Next Sentence Prediction-Token-Type Embeddings-[CLS] pooling+Knowledge Distillation+Cosine Embedding Loss+Dynamic Maskingcommit Author: Martin et al.Date: Sun Nov 10 10:46:37 2019 +0000-BERT-English+ROBERTA+French OSCAR dataset(138GB)+Whole-word Masking(WWM)+SentencePiece Tokenizercommit Author: Le et al.Date: Wed Dec 11 14:59:32 2019 +0000-BERT-English+ROBERTA+fastBPE+Stochastic Depth+French dataset(71GB)+FLUE(French Language Understanding Evaluation) benchmarkI recently came across an interesting thread on Twitter discussing a hypothetical scenario where research papers are published on GitHub and subsequent papers are diffs over the original paper. Information overload has been a real problem in ML with so many new papers coming every month.If you could represent a paper as a code diff, many papers could be compressed down to <50 lines :) The diff would also be more intuitive to read and eval standardized.Some ideas are so different that this wouldn’t apply, but I think it would work well for the majority. 
This post is a fun experiment showcasing how the commit history could look like for the BERT paper and some of its subsequent variants.
commit Author: Devlin et al.Date: Thu Oct 11 00:50:01 2018 +0000-Transformer Decoder+Masked Language Modeling+Next Sentence Prediction+WordPiece 30Kcommit Author: Lample et al.Date: Tue Jan 22 13:22:34 2019 +0000+Translation Language Modeling(TLM)+Causal Language Modeling(CLM)commit Author: Lee et al.Date: Fri Jan 25 05:57:24 2019 +0000+PubMed Abstracts data+PubMed Central Full Texts datacommit Author: Liu et al.Date: Thu Jan 31 18:07:25 2019 +0000+Multi-task Learningcommit Author: Beltagy et al.Date: Tue Mar 26 05:11:46 2019 +0000-BERT WordPiece Vocabulary-English Wikipedia-BookCorpus+1.14M Semantic Scholar Papers(Biomedial + Computer Science)+ScispaCy segmentation+SciVOCAB WordPiece Vocabularycommit Author: Yang et al.Date: Wed Jun 19 17:35:48 2019 +0000-Masked Language Modeling-BERT Transformer+Permutation Language Modeling+Transformer-XL+Two-stream self-attention+SentencePiece Tokenizercommit Author: Joshi et al.Date: Wed Jul 24 15:43:40 2019 +0000-Random Token Masking-Next Sentence Prediction-Bi-sequence Training+Continuous Span Masking+Span-Boundary Objective(SBO)+Single-Sequence Trainingcommit Author: Liu et al.Date: Fri Jul 26 17:48:29 2019 +0000-Next Sentence Prediction-Static Masking of Tokens+Dynamic Masking of Tokens+Byte Pair Encoding(BPE) 50K+Large batch size+CC-NEWS(76G) dataset+OpenWebText(38G) dataset+Stories(31G) datasetcommit Author: Reimers et al.Date: Tue Aug 27 08:50:17 2019 +0000+Siamese Network Structure+Finetuning on SNLI and MNLIcommit Author: Lan et al.Date: Thu Sep 26 07:06:13 2019 +0000-Next Sentence Prediction+Sentence Order Prediction+Cross-layer Parameter Sharing+Factorized Embeddings+SentencePiece Tokenizercommit Author: Sanh et al.Date: Wed Oct 2 17:56:28 2019 +0000-Next Sentence Prediction-Token-Type Embeddings-[CLS] pooling+Knowledge Distillation+Cosine Embedding Loss+Dynamic Maskingcommit Author: Martin et al.Date: Sun Nov 10 10:46:37 2019 +0000-BERT-English+ROBERTA+French OSCAR dataset(138GB)+Whole-word Masking(WWM)+SentencePiece Tokenizercommit Author: Le et al.Date: Wed Dec 11 14:59:32 2019 +0000-BERT-English+ROBERTA+fastBPE+Stochastic Depth+French dataset(71GB)+FLUE(French Language Understanding Evaluation) benchmarkI recently came across an interesting thread on Twitter discussing a hypothetical scenario where research papers are published on GitHub and subsequent papers are diffs over the original paper. Information overload has been a real problem in ML with so many new papers coming every month.If you could represent a paper as a code diff, many papers could be compressed down to <50 lines :) The diff would also be more intuitive to read and eval standardized.Some ideas are so different that this wouldn’t apply, but I think it would work well for the majority. 
This post is a fun experiment showcasing how the commit history could look like for the BERT paper and some of its subsequent variants.
commit Author: Devlin et al.Date: Thu Oct 11 00:50:01 2018 +0000-Transformer Decoder+Masked Language Modeling+Next Sentence Prediction+WordPiece 30Kcommit Author: Lample et al.Date: Tue Jan 22 13:22:34 2019 +0000+Translation Language Modeling(TLM)+Causal Language Modeling(CLM)commit Author: Lee et al.Date: Fri Jan 25 05:57:24 2019 +0000+PubMed Abstracts data+PubMed Central Full Texts datacommit Author: Liu et al.Date: Thu Jan 31 18:07:25 2019 +0000+Multi-task Learningcommit Author: Beltagy et al.Date: Tue Mar 26 05:11:46 2019 +0000-BERT WordPiece Vocabulary-English Wikipedia-BookCorpus+1.14M Semantic Scholar Papers(Biomedial + Computer Science)+ScispaCy segmentation+SciVOCAB WordPiece Vocabularycommit Author: Yang et al.Date: Wed Jun 19 17:35:48 2019 +0000-Masked Language Modeling-BERT Transformer+Permutation Language Modeling+Transformer-XL+Two-stream self-attention+SentencePiece Tokenizercommit Author: Joshi et al.Date: Wed Jul 24 15:43:40 2019 +0000-Random Token Masking-Next Sentence Prediction-Bi-sequence Training+Continuous Span Masking+Span-Boundary Objective(SBO)+Single-Sequence Trainingcommit Author: Liu et al.Date: Fri Jul 26 17:48:29 2019 +0000-Next Sentence Prediction-Static Masking of Tokens+Dynamic Masking of Tokens+Byte Pair Encoding(BPE) 50K+Large batch size+CC-NEWS(76G) dataset+OpenWebText(38G) dataset+Stories(31G) datasetcommit Author: Reimers et al.Date: Tue Aug 27 08:50:17 2019 +0000+Siamese Network Structure+Finetuning on SNLI and MNLIcommit Author: Lan et al.Date: Thu Sep 26 07:06:13 2019 +0000-Next Sentence Prediction+Sentence Order Prediction+Cross-layer Parameter Sharing+Factorized Embeddings+SentencePiece Tokenizercommit Author: Sanh et al.Date: Wed Oct 2 17:56:28 2019 +0000-Next Sentence Prediction-Token-Type Embeddings-[CLS] pooling+Knowledge Distillation+Cosine Embedding Loss+Dynamic Maskingcommit Author: Martin et al.Date: Sun Nov 10 10:46:37 2019 +0000-BERT-English+ROBERTA+French OSCAR dataset(138GB)+Whole-word Masking(WWM)+SentencePiece Tokenizercommit Author: Le et al.Date: Wed Dec 11 14:59:32 2019 +0000-BERT-English+ROBERTA+fastBPE+Stochastic Depth+French dataset(71GB)+FLUE(French Language Understanding Evaluation) benchmarkI recently came across an interesting thread on Twitter discussing a hypothetical scenario where research papers are published on GitHub and subsequent papers are diffs over the original paper. Information overload has been a real problem in ML with so many new papers coming every month.If you could represent a paper as a code diff, many papers could be compressed down to <50 lines :) The diff would also be more intuitive to read and eval standardized.Some ideas are so different that this wouldn’t apply, but I think it would work well for the majority. 
This post is a fun experiment showcasing how the commit history could look like for the BERT paper and some of its subsequent variants.
commit Author: Devlin et al.Date: Thu Oct 11 00:50:01 2018 +0000-Transformer Decoder+Masked Language Modeling+Next Sentence Prediction+WordPiece 30Kcommit Author: Lample et al.Date: Tue Jan 22 13:22:34 2019 +0000+Translation Language Modeling(TLM)+Causal Language Modeling(CLM)commit Author: Lee et al.Date: Fri Jan 25 05:57:24 2019 +0000+PubMed Abstracts data+PubMed Central Full Texts datacommit Author: Liu et al.Date: Thu Jan 31 18:07:25 2019 +0000+Multi-task Learningcommit Author: Beltagy et al.Date: Tue Mar 26 05:11:46 2019 +0000-BERT WordPiece Vocabulary-English Wikipedia-BookCorpus+1.14M Semantic Scholar Papers(Biomedial + Computer Science)+ScispaCy segmentation+SciVOCAB WordPiece Vocabularycommit Author: Yang et al.Date: Wed Jun 19 17:35:48 2019 +0000-Masked Language Modeling-BERT Transformer+Permutation Language Modeling+Transformer-XL+Two-stream self-attention+SentencePiece Tokenizercommit Author: Joshi et al.Date: Wed Jul 24 15:43:40 2019 +0000-Random Token Masking-Next Sentence Prediction-Bi-sequence Training+Continuous Span Masking+Span-Boundary Objective(SBO)+Single-Sequence Trainingcommit Author: Liu et al.Date: Fri Jul 26 17:48:29 2019 +0000-Next Sentence Prediction-Static Masking of Tokens+Dynamic Masking of Tokens+Byte Pair Encoding(BPE) 50K+Large batch size+CC-NEWS(76G) dataset+OpenWebText(38G) dataset+Stories(31G) datasetcommit Author: Reimers et al.Date: Tue Aug 27 08:50:17 2019 +0000+Siamese Network Structure+Finetuning on SNLI and MNLIcommit Author: Lan et al.Date: Thu Sep 26 07:06:13 2019 +0000-Next Sentence Prediction+Sentence Order Prediction+Cross-layer Parameter Sharing+Factorized Embeddings+SentencePiece Tokenizercommit Author: Sanh et al.Date: Wed Oct 2 17:56:28 2019 +0000-Next Sentence Prediction-Token-Type Embeddings-[CLS] pooling+Knowledge Distillation+Cosine Embedding Loss+Dynamic Maskingcommit Author: Martin et al.Date: Sun Nov 10 10:46:37 2019 +0000-BERT-English+ROBERTA+French OSCAR dataset(138GB)+Whole-word Masking(WWM)+SentencePiece Tokenizercommit Author: Le et al.Date: Wed Dec 11 14:59:32 2019 +0000-BERT-English+ROBERTA+fastBPE+Stochastic Depth+French dataset(71GB)+FLUE(French Language Understanding Evaluation) benchmarkI recently came across an interesting thread on Twitter discussing a hypothetical scenario where research papers are published on GitHub and subsequent papers are diffs over the original paper. Information overload has been a real problem in ML with so many new papers coming every month.If you could represent a paper as a code diff, many papers could be compressed down to <50 lines :) The diff would also be more intuitive to read and eval standardized.Some ideas are so different that this wouldn’t apply, but I think it would work well for the majority. 
This post is a fun experiment showcasing how the commit history could look like for the BERT paper and some of its subsequent variants.
commit Author: Devlin et al.Date: Thu Oct 11 00:50:01 2018 +0000-Transformer Decoder+Masked Language Modeling+Next Sentence Prediction+WordPiece 30Kcommit Author: Lample et al.Date: Tue Jan 22 13:22:34 2019 +0000+Translation Language Modeling(TLM)+Causal Language Modeling(CLM)commit Author: Lee et al.Date: Fri Jan 25 05:57:24 2019 +0000+PubMed Abstracts data+PubMed Central Full Texts datacommit Author: Liu et al.Date: Thu Jan 31 18:07:25 2019 +0000+Multi-task Learningcommit Author: Beltagy et al.Date: Tue Mar 26 05:11:46 2019 +0000-BERT WordPiece Vocabulary-English Wikipedia-BookCorpus+1.14M Semantic Scholar Papers(Biomedial + Computer Science)+ScispaCy segmentation+SciVOCAB WordPiece Vocabularycommit Author: Yang et al.Date: Wed Jun 19 17:35:48 2019 +0000-Masked Language Modeling-BERT Transformer+Permutation Language Modeling+Transformer-XL+Two-stream self-attention+SentencePiece Tokenizercommit Author: Joshi et al.Date: Wed Jul 24 15:43:40 2019 +0000-Random Token Masking-Next Sentence Prediction-Bi-sequence Training+Continuous Span Masking+Span-Boundary Objective(SBO)+Single-Sequence Trainingcommit Author: Liu et al.Date: Fri Jul 26 17:48:29 2019 +0000-Next Sentence Prediction-Static Masking of Tokens+Dynamic Masking of Tokens+Byte Pair Encoding(BPE) 50K+Large batch size+CC-NEWS(76G) dataset+OpenWebText(38G) dataset+Stories(31G) datasetcommit Author: Reimers et al.Date: Tue Aug 27 08:50:17 2019 +0000+Siamese Network Structure+Finetuning on SNLI and MNLIcommit Author: Lan et al.Date: Thu Sep 26 07:06:13 2019 +0000-Next Sentence Prediction+Sentence Order Prediction+Cross-layer Parameter Sharing+Factorized Embeddings+SentencePiece Tokenizercommit Author: Sanh et al.Date: Wed Oct 2 17:56:28 2019 +0000-Next Sentence Prediction-Token-Type Embeddings-[CLS] pooling+Knowledge Distillation+Cosine Embedding Loss+Dynamic Maskingcommit Author: Martin et al.Date: Sun Nov 10 10:46:37 2019 +0000-BERT-English+ROBERTA+French OSCAR dataset(138GB)+Whole-word Masking(WWM)+SentencePiece Tokenizercommit Author: Le et al.Date: Wed Dec 11 14:59:32 2019 +0000-BERT-English+ROBERTA+fastBPE+Stochastic Depth+French dataset(71GB)+FLUE(French Language Understanding Evaluation) benchmarkI recently came across an interesting thread on Twitter discussing a hypothetical scenario where research papers are published on GitHub and subsequent papers are diffs over the original paper. Information overload has been a real problem in ML with so many new papers coming every month.If you could represent a paper as a code diff, many papers could be compressed down to <50 lines :) The diff would also be more intuitive to read and eval standardized.Some ideas are so different that this wouldn’t apply, but I think it would work well for the majority. 
This post is a fun experiment showcasing how the commit history could look like for the BERT paper and some of its subsequent variants.
commit Author: Devlin et al.Date: Thu Oct 11 00:50:01 2018 +0000-Transformer Decoder+Masked Language Modeling+Next Sentence Prediction+WordPiece 30Kcommit Author: Lample et al.Date: Tue Jan 22 13:22:34 2019 +0000+Translation Language Modeling(TLM)+Causal Language Modeling(CLM)commit Author: Lee et al.Date: Fri Jan 25 05:57:24 2019 +0000+PubMed Abstracts data+PubMed Central Full Texts datacommit Author: Liu et al.Date: Thu Jan 31 18:07:25 2019 +0000+Multi-task Learningcommit Author: Beltagy et al.Date: Tue Mar 26 05:11:46 2019 +0000-BERT WordPiece Vocabulary-English Wikipedia-BookCorpus+1.14M Semantic Scholar Papers(Biomedial + Computer Science)+ScispaCy segmentation+SciVOCAB WordPiece Vocabularycommit Author: Yang et al.Date: Wed Jun 19 17:35:48 2019 +0000-Masked Language Modeling-BERT Transformer+Permutation Language Modeling+Transformer-XL+Two-stream self-attention+SentencePiece Tokenizercommit Author: Joshi et al.Date: Wed Jul 24 15:43:40 2019 +0000-Random Token Masking-Next Sentence Prediction-Bi-sequence Training+Continuous Span Masking+Span-Boundary Objective(SBO)+Single-Sequence Trainingcommit Author: Liu et al.Date: Fri Jul 26 17:48:29 2019 +0000-Next Sentence Prediction-Static Masking of Tokens+Dynamic Masking of Tokens+Byte Pair Encoding(BPE) 50K+Large batch size+CC-NEWS(76G) dataset+OpenWebText(38G) dataset+Stories(31G) datasetcommit Author: Reimers et al.Date: Tue Aug 27 08:50:17 2019 +0000+Siamese Network Structure+Finetuning on SNLI and MNLIcommit Author: Lan et al.Date: Thu Sep 26 07:06:13 2019 +0000-Next Sentence Prediction+Sentence Order Prediction+Cross-layer Parameter Sharing+Factorized Embeddings+SentencePiece Tokenizercommit Author: Sanh et al.Date: Wed Oct 2 17:56:28 2019 +0000-Next Sentence Prediction-Token-Type Embeddings-[CLS] pooling+Knowledge Distillation+Cosine Embedding Loss+Dynamic Maskingcommit Author: Martin et al.Date: Sun Nov 10 10:46:37 2019 +0000-BERT-English+ROBERTA+French OSCAR dataset(138GB)+Whole-word Masking(WWM)+SentencePiece Tokenizercommit Author: Le et al.Date: Wed Dec 11 14:59:32 2019 +0000-BERT-English+ROBERTA+fastBPE+Stochastic Depth+French dataset(71GB)+FLUE(French Language Understanding Evaluation) benchmark