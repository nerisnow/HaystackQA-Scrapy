Many self-supervised methods use  to generate surrogate labels and formulate an unsupervised learning problem as a supervised one. Some examples include rotation prediction, image colorization, jigsaw puzzles etc. However, such pretext tasks are domain-dependent and require expertise to design them. is a self-supervised method proposed by Caron et al. of Facebook AI Research that brings a different approach.
This method doesn’t require domain-specific knowledge and can be used to learn deep representations for scenarios where annotated data is scarce.DeepCluster combines two pieces: unsupervised clustering and deep neural networks. It proposes an end-to-end method to jointly learn parameters of a deep neural network and the cluster assignments of its representations. The features are generated and clustered iteratively to get both a trained model and labels as output artifacts.Let’s now understand how the deep cluster pipeline works with an interactive diagram.
As seen in the figure above, unlabeled images are taken and  are applied to them. Then, an  architecture such as  or  is used as the feature extractor. Initially, the  is initialized with randomly weights and we take the  from layer before the final classification head. Then,  is used to reduce the dimension of the  along with whitening and . Finally, the processed features are passed to  to get cluster assignment for each image.These cluster assignments are used as the  and the  is trained to predict these clusters. Cross-entropy loss is used to gauge the performance of the model. The model is trained for 100 epochs with the  step occurring once per epoch. Finally, we can take the  learned and use it for downstream tasks.Let’s see how DeepCluster is applied in practice with a step by step example of the whole pipeline from the input data to the output labels:
We take unlabeled images from the ImageNet dataset which consist of 1.3 million images uniformly distributed into 1000 classes. These images are prepared in mini-batches of 256.
The training set of N images can be denoted mathematically by:Transformations are applied to the images so that the features learned is invariant to augmentations. Two different augmentations are done, one when training model to learn representations and one when sending the image representations to the clustering algorithm:When model representations are to be sent for clustering, random augmentations are not used. The image is simply resized to 256*256 and the center crop is applied to get 224*224 image. Then normalization is applied.
In PyTorch, this can be implemented as:When the model is trained on image and labels, then we use random augmentations. The image is cropped to a random size and aspect ratio and then resized to 224*224. Then, the image is horizontally flipped with a 50% chance. Finally, we normalize the image with ImageNet mean and std.

In PyTorch, this can be implemented as:Once we get the normalized image, we convert it into grayscale. Then, we increase the local contrast of the image using the Sobel filters.
Below is a simplified snippet adapted from the author’s implementation . We can apply it on the augmented image  we got above.To perform clustering, we need to decide the number of clusters. This will be the number of classes the model will be trained on.
By default, ImageNet has 1000 classes, but the paper uses 10,000 clusters as this gives more fine-grained grouping of the unlabeled images. For example, if you previously had a grouping of cats and dogs and you increase clusters, then groupings of breeds of the cat and dog could be created.The paper primarily uses AlexNet architecture consisting of  and 3 fully connected layers. The Local Response Normalization layers are removed and Batch Normalization is applied instead. Dropout is also added. The filter size used is from 2012 competition: 96, 256, 384, 384, 256.Alternatively, the paper has also tried replacing AlexNet by VGG-16 with batch normalization to see impact on performance.To generating initial labels for the model to train on, we initialize AlexNet with random weights and the last fully connected layer FC3 removed. We perform a forward pass on the model on images and take the feature vector coming from the second fully connected layer FC2 of the model on an image. This feature vector has a dimension of 4096.This process is repeated for all images in the batch for the whole dataset. Thus, if we have N total images, we will have an image-feature matrix of [N, 4096].Before performing clustering, dimensionality reduction is applied to the image-feature matrix.For dimensionality reduction, Principal Component Analysis(PCA) is applied to the features to reduce them from 4096 dimensions to 256 dimensions. The values are also whitened. 
The paper uses the  library to perform this at scale. Faiss provides an efficient implementation of PCA which can be applied for some image-feature matrix  as:Then, L2 normalization is applied to the values we get after PCA.Thus, we finally get a matrix of  for total N images. Now, K-means clustering is applied to the pre-processed features to get images and their corresponding clusters. These clusters will act as the pseudo-labels on which the model will be trained.The paper use Johnson’s implementation of K-means from the paper . It is available in the faiss library. Since clustering has to be run on all the images, it takes one-third of the total training time.After clustering is done, new batches of images are created such that images from each cluster has an equal chance of being included. Random augmentations are applied to these images.Once we have the images and clusters, we train our ConvNet model like regular supervised learning. We use a batch size of 256 and use cross-entropy loss to compare model predictions to the ground truth cluster label. The model learns useful representations.The model is trained for 500 epochs. The clustering step is run once at the start of each epoch to generate pseudo-labels for the whole dataset. Then, the regular training of ConvNet using cross-entropy loss is continued for all the batches.
The paper uses SGD optimizer with momentum of 0.9, learning rate of 0.05 and weight decay of \(10^{-5}\). They trained it on Pascal P100 GPU.The official implementation of Deep Cluster in PyTorch by the paper authors is available on . They also provide  for AlexNet and Resnet-50 architectures.If you found this blog post useful, please consider citing it as:Many self-supervised methods use  to generate surrogate labels and formulate an unsupervised learning problem as a supervised one. Some examples include rotation prediction, image colorization, jigsaw puzzles etc. However, such pretext tasks are domain-dependent and require expertise to design them. is a self-supervised method proposed by Caron et al. of Facebook AI Research that brings a different approach.
This method doesn’t require domain-specific knowledge and can be used to learn deep representations for scenarios where annotated data is scarce.DeepCluster combines two pieces: unsupervised clustering and deep neural networks. It proposes an end-to-end method to jointly learn parameters of a deep neural network and the cluster assignments of its representations. The features are generated and clustered iteratively to get both a trained model and labels as output artifacts.Let’s now understand how the deep cluster pipeline works with an interactive diagram.
As seen in the figure above, unlabeled images are taken and  are applied to them. Then, an  architecture such as  or  is used as the feature extractor. Initially, the  is initialized with randomly weights and we take the  from layer before the final classification head. Then,  is used to reduce the dimension of the  along with whitening and . Finally, the processed features are passed to  to get cluster assignment for each image.These cluster assignments are used as the  and the  is trained to predict these clusters. Cross-entropy loss is used to gauge the performance of the model. The model is trained for 100 epochs with the  step occurring once per epoch. Finally, we can take the  learned and use it for downstream tasks.Let’s see how DeepCluster is applied in practice with a step by step example of the whole pipeline from the input data to the output labels:
We take unlabeled images from the ImageNet dataset which consist of 1.3 million images uniformly distributed into 1000 classes. These images are prepared in mini-batches of 256.
The training set of N images can be denoted mathematically by:Transformations are applied to the images so that the features learned is invariant to augmentations. Two different augmentations are done, one when training model to learn representations and one when sending the image representations to the clustering algorithm:When model representations are to be sent for clustering, random augmentations are not used. The image is simply resized to 256*256 and the center crop is applied to get 224*224 image. Then normalization is applied.
In PyTorch, this can be implemented as:When the model is trained on image and labels, then we use random augmentations. The image is cropped to a random size and aspect ratio and then resized to 224*224. Then, the image is horizontally flipped with a 50% chance. Finally, we normalize the image with ImageNet mean and std.

In PyTorch, this can be implemented as:Once we get the normalized image, we convert it into grayscale. Then, we increase the local contrast of the image using the Sobel filters.
Below is a simplified snippet adapted from the author’s implementation . We can apply it on the augmented image  we got above.To perform clustering, we need to decide the number of clusters. This will be the number of classes the model will be trained on.
By default, ImageNet has 1000 classes, but the paper uses 10,000 clusters as this gives more fine-grained grouping of the unlabeled images. For example, if you previously had a grouping of cats and dogs and you increase clusters, then groupings of breeds of the cat and dog could be created.The paper primarily uses AlexNet architecture consisting of  and 3 fully connected layers. The Local Response Normalization layers are removed and Batch Normalization is applied instead. Dropout is also added. The filter size used is from 2012 competition: 96, 256, 384, 384, 256.Alternatively, the paper has also tried replacing AlexNet by VGG-16 with batch normalization to see impact on performance.To generating initial labels for the model to train on, we initialize AlexNet with random weights and the last fully connected layer FC3 removed. We perform a forward pass on the model on images and take the feature vector coming from the second fully connected layer FC2 of the model on an image. This feature vector has a dimension of 4096.This process is repeated for all images in the batch for the whole dataset. Thus, if we have N total images, we will have an image-feature matrix of [N, 4096].Before performing clustering, dimensionality reduction is applied to the image-feature matrix.For dimensionality reduction, Principal Component Analysis(PCA) is applied to the features to reduce them from 4096 dimensions to 256 dimensions. The values are also whitened. 
The paper uses the  library to perform this at scale. Faiss provides an efficient implementation of PCA which can be applied for some image-feature matrix  as:Then, L2 normalization is applied to the values we get after PCA.Thus, we finally get a matrix of  for total N images. Now, K-means clustering is applied to the pre-processed features to get images and their corresponding clusters. These clusters will act as the pseudo-labels on which the model will be trained.The paper use Johnson’s implementation of K-means from the paper . It is available in the faiss library. Since clustering has to be run on all the images, it takes one-third of the total training time.After clustering is done, new batches of images are created such that images from each cluster has an equal chance of being included. Random augmentations are applied to these images.Once we have the images and clusters, we train our ConvNet model like regular supervised learning. We use a batch size of 256 and use cross-entropy loss to compare model predictions to the ground truth cluster label. The model learns useful representations.The model is trained for 500 epochs. The clustering step is run once at the start of each epoch to generate pseudo-labels for the whole dataset. Then, the regular training of ConvNet using cross-entropy loss is continued for all the batches.
The paper uses SGD optimizer with momentum of 0.9, learning rate of 0.05 and weight decay of \(10^{-5}\). They trained it on Pascal P100 GPU.The official implementation of Deep Cluster in PyTorch by the paper authors is available on . They also provide  for AlexNet and Resnet-50 architectures.If you found this blog post useful, please consider citing it as:Many self-supervised methods use  to generate surrogate labels and formulate an unsupervised learning problem as a supervised one. Some examples include rotation prediction, image colorization, jigsaw puzzles etc. However, such pretext tasks are domain-dependent and require expertise to design them. is a self-supervised method proposed by Caron et al. of Facebook AI Research that brings a different approach.
This method doesn’t require domain-specific knowledge and can be used to learn deep representations for scenarios where annotated data is scarce.DeepCluster combines two pieces: unsupervised clustering and deep neural networks. It proposes an end-to-end method to jointly learn parameters of a deep neural network and the cluster assignments of its representations. The features are generated and clustered iteratively to get both a trained model and labels as output artifacts.Let’s now understand how the deep cluster pipeline works with an interactive diagram.
As seen in the figure above, unlabeled images are taken and  are applied to them. Then, an  architecture such as  or  is used as the feature extractor. Initially, the  is initialized with randomly weights and we take the  from layer before the final classification head. Then,  is used to reduce the dimension of the  along with whitening and . Finally, the processed features are passed to  to get cluster assignment for each image.These cluster assignments are used as the  and the  is trained to predict these clusters. Cross-entropy loss is used to gauge the performance of the model. The model is trained for 100 epochs with the  step occurring once per epoch. Finally, we can take the  learned and use it for downstream tasks.Let’s see how DeepCluster is applied in practice with a step by step example of the whole pipeline from the input data to the output labels:
We take unlabeled images from the ImageNet dataset which consist of 1.3 million images uniformly distributed into 1000 classes. These images are prepared in mini-batches of 256.
The training set of N images can be denoted mathematically by:Transformations are applied to the images so that the features learned is invariant to augmentations. Two different augmentations are done, one when training model to learn representations and one when sending the image representations to the clustering algorithm:When model representations are to be sent for clustering, random augmentations are not used. The image is simply resized to 256*256 and the center crop is applied to get 224*224 image. Then normalization is applied.
In PyTorch, this can be implemented as:When the model is trained on image and labels, then we use random augmentations. The image is cropped to a random size and aspect ratio and then resized to 224*224. Then, the image is horizontally flipped with a 50% chance. Finally, we normalize the image with ImageNet mean and std.

In PyTorch, this can be implemented as:Once we get the normalized image, we convert it into grayscale. Then, we increase the local contrast of the image using the Sobel filters.
Below is a simplified snippet adapted from the author’s implementation . We can apply it on the augmented image  we got above.To perform clustering, we need to decide the number of clusters. This will be the number of classes the model will be trained on.
By default, ImageNet has 1000 classes, but the paper uses 10,000 clusters as this gives more fine-grained grouping of the unlabeled images. For example, if you previously had a grouping of cats and dogs and you increase clusters, then groupings of breeds of the cat and dog could be created.The paper primarily uses AlexNet architecture consisting of  and 3 fully connected layers. The Local Response Normalization layers are removed and Batch Normalization is applied instead. Dropout is also added. The filter size used is from 2012 competition: 96, 256, 384, 384, 256.Alternatively, the paper has also tried replacing AlexNet by VGG-16 with batch normalization to see impact on performance.To generating initial labels for the model to train on, we initialize AlexNet with random weights and the last fully connected layer FC3 removed. We perform a forward pass on the model on images and take the feature vector coming from the second fully connected layer FC2 of the model on an image. This feature vector has a dimension of 4096.This process is repeated for all images in the batch for the whole dataset. Thus, if we have N total images, we will have an image-feature matrix of [N, 4096].Before performing clustering, dimensionality reduction is applied to the image-feature matrix.For dimensionality reduction, Principal Component Analysis(PCA) is applied to the features to reduce them from 4096 dimensions to 256 dimensions. The values are also whitened. 
The paper uses the  library to perform this at scale. Faiss provides an efficient implementation of PCA which can be applied for some image-feature matrix  as:Then, L2 normalization is applied to the values we get after PCA.Thus, we finally get a matrix of  for total N images. Now, K-means clustering is applied to the pre-processed features to get images and their corresponding clusters. These clusters will act as the pseudo-labels on which the model will be trained.The paper use Johnson’s implementation of K-means from the paper . It is available in the faiss library. Since clustering has to be run on all the images, it takes one-third of the total training time.After clustering is done, new batches of images are created such that images from each cluster has an equal chance of being included. Random augmentations are applied to these images.Once we have the images and clusters, we train our ConvNet model like regular supervised learning. We use a batch size of 256 and use cross-entropy loss to compare model predictions to the ground truth cluster label. The model learns useful representations.The model is trained for 500 epochs. The clustering step is run once at the start of each epoch to generate pseudo-labels for the whole dataset. Then, the regular training of ConvNet using cross-entropy loss is continued for all the batches.
The paper uses SGD optimizer with momentum of 0.9, learning rate of 0.05 and weight decay of \(10^{-5}\). They trained it on Pascal P100 GPU.The official implementation of Deep Cluster in PyTorch by the paper authors is available on . They also provide  for AlexNet and Resnet-50 architectures.If you found this blog post useful, please consider citing it as:Many self-supervised methods use  to generate surrogate labels and formulate an unsupervised learning problem as a supervised one. Some examples include rotation prediction, image colorization, jigsaw puzzles etc. However, such pretext tasks are domain-dependent and require expertise to design them. is a self-supervised method proposed by Caron et al. of Facebook AI Research that brings a different approach.
This method doesn’t require domain-specific knowledge and can be used to learn deep representations for scenarios where annotated data is scarce.DeepCluster combines two pieces: unsupervised clustering and deep neural networks. It proposes an end-to-end method to jointly learn parameters of a deep neural network and the cluster assignments of its representations. The features are generated and clustered iteratively to get both a trained model and labels as output artifacts.Let’s now understand how the deep cluster pipeline works with an interactive diagram.
As seen in the figure above, unlabeled images are taken and  are applied to them. Then, an  architecture such as  or  is used as the feature extractor. Initially, the  is initialized with randomly weights and we take the  from layer before the final classification head. Then,  is used to reduce the dimension of the  along with whitening and . Finally, the processed features are passed to  to get cluster assignment for each image.These cluster assignments are used as the  and the  is trained to predict these clusters. Cross-entropy loss is used to gauge the performance of the model. The model is trained for 100 epochs with the  step occurring once per epoch. Finally, we can take the  learned and use it for downstream tasks.Let’s see how DeepCluster is applied in practice with a step by step example of the whole pipeline from the input data to the output labels:
We take unlabeled images from the ImageNet dataset which consist of 1.3 million images uniformly distributed into 1000 classes. These images are prepared in mini-batches of 256.
The training set of N images can be denoted mathematically by:Transformations are applied to the images so that the features learned is invariant to augmentations. Two different augmentations are done, one when training model to learn representations and one when sending the image representations to the clustering algorithm:When model representations are to be sent for clustering, random augmentations are not used. The image is simply resized to 256*256 and the center crop is applied to get 224*224 image. Then normalization is applied.
In PyTorch, this can be implemented as:When the model is trained on image and labels, then we use random augmentations. The image is cropped to a random size and aspect ratio and then resized to 224*224. Then, the image is horizontally flipped with a 50% chance. Finally, we normalize the image with ImageNet mean and std.

In PyTorch, this can be implemented as:Once we get the normalized image, we convert it into grayscale. Then, we increase the local contrast of the image using the Sobel filters.
Below is a simplified snippet adapted from the author’s implementation . We can apply it on the augmented image  we got above.To perform clustering, we need to decide the number of clusters. This will be the number of classes the model will be trained on.
By default, ImageNet has 1000 classes, but the paper uses 10,000 clusters as this gives more fine-grained grouping of the unlabeled images. For example, if you previously had a grouping of cats and dogs and you increase clusters, then groupings of breeds of the cat and dog could be created.The paper primarily uses AlexNet architecture consisting of  and 3 fully connected layers. The Local Response Normalization layers are removed and Batch Normalization is applied instead. Dropout is also added. The filter size used is from 2012 competition: 96, 256, 384, 384, 256.Alternatively, the paper has also tried replacing AlexNet by VGG-16 with batch normalization to see impact on performance.To generating initial labels for the model to train on, we initialize AlexNet with random weights and the last fully connected layer FC3 removed. We perform a forward pass on the model on images and take the feature vector coming from the second fully connected layer FC2 of the model on an image. This feature vector has a dimension of 4096.This process is repeated for all images in the batch for the whole dataset. Thus, if we have N total images, we will have an image-feature matrix of [N, 4096].Before performing clustering, dimensionality reduction is applied to the image-feature matrix.For dimensionality reduction, Principal Component Analysis(PCA) is applied to the features to reduce them from 4096 dimensions to 256 dimensions. The values are also whitened. 
The paper uses the  library to perform this at scale. Faiss provides an efficient implementation of PCA which can be applied for some image-feature matrix  as:Then, L2 normalization is applied to the values we get after PCA.Thus, we finally get a matrix of  for total N images. Now, K-means clustering is applied to the pre-processed features to get images and their corresponding clusters. These clusters will act as the pseudo-labels on which the model will be trained.The paper use Johnson’s implementation of K-means from the paper . It is available in the faiss library. Since clustering has to be run on all the images, it takes one-third of the total training time.After clustering is done, new batches of images are created such that images from each cluster has an equal chance of being included. Random augmentations are applied to these images.Once we have the images and clusters, we train our ConvNet model like regular supervised learning. We use a batch size of 256 and use cross-entropy loss to compare model predictions to the ground truth cluster label. The model learns useful representations.The model is trained for 500 epochs. The clustering step is run once at the start of each epoch to generate pseudo-labels for the whole dataset. Then, the regular training of ConvNet using cross-entropy loss is continued for all the batches.
The paper uses SGD optimizer with momentum of 0.9, learning rate of 0.05 and weight decay of \(10^{-5}\). They trained it on Pascal P100 GPU.The official implementation of Deep Cluster in PyTorch by the paper authors is available on . They also provide  for AlexNet and Resnet-50 architectures.If you found this blog post useful, please consider citing it as:Many self-supervised methods use  to generate surrogate labels and formulate an unsupervised learning problem as a supervised one. Some examples include rotation prediction, image colorization, jigsaw puzzles etc. However, such pretext tasks are domain-dependent and require expertise to design them. is a self-supervised method proposed by Caron et al. of Facebook AI Research that brings a different approach.
This method doesn’t require domain-specific knowledge and can be used to learn deep representations for scenarios where annotated data is scarce.DeepCluster combines two pieces: unsupervised clustering and deep neural networks. It proposes an end-to-end method to jointly learn parameters of a deep neural network and the cluster assignments of its representations. The features are generated and clustered iteratively to get both a trained model and labels as output artifacts.Let’s now understand how the deep cluster pipeline works with an interactive diagram.
As seen in the figure above, unlabeled images are taken and  are applied to them. Then, an  architecture such as  or  is used as the feature extractor. Initially, the  is initialized with randomly weights and we take the  from layer before the final classification head. Then,  is used to reduce the dimension of the  along with whitening and . Finally, the processed features are passed to  to get cluster assignment for each image.These cluster assignments are used as the  and the  is trained to predict these clusters. Cross-entropy loss is used to gauge the performance of the model. The model is trained for 100 epochs with the  step occurring once per epoch. Finally, we can take the  learned and use it for downstream tasks.Let’s see how DeepCluster is applied in practice with a step by step example of the whole pipeline from the input data to the output labels:
We take unlabeled images from the ImageNet dataset which consist of 1.3 million images uniformly distributed into 1000 classes. These images are prepared in mini-batches of 256.
The training set of N images can be denoted mathematically by:Transformations are applied to the images so that the features learned is invariant to augmentations. Two different augmentations are done, one when training model to learn representations and one when sending the image representations to the clustering algorithm:When model representations are to be sent for clustering, random augmentations are not used. The image is simply resized to 256*256 and the center crop is applied to get 224*224 image. Then normalization is applied.
In PyTorch, this can be implemented as:When the model is trained on image and labels, then we use random augmentations. The image is cropped to a random size and aspect ratio and then resized to 224*224. Then, the image is horizontally flipped with a 50% chance. Finally, we normalize the image with ImageNet mean and std.

In PyTorch, this can be implemented as:Once we get the normalized image, we convert it into grayscale. Then, we increase the local contrast of the image using the Sobel filters.
Below is a simplified snippet adapted from the author’s implementation . We can apply it on the augmented image  we got above.To perform clustering, we need to decide the number of clusters. This will be the number of classes the model will be trained on.
By default, ImageNet has 1000 classes, but the paper uses 10,000 clusters as this gives more fine-grained grouping of the unlabeled images. For example, if you previously had a grouping of cats and dogs and you increase clusters, then groupings of breeds of the cat and dog could be created.The paper primarily uses AlexNet architecture consisting of  and 3 fully connected layers. The Local Response Normalization layers are removed and Batch Normalization is applied instead. Dropout is also added. The filter size used is from 2012 competition: 96, 256, 384, 384, 256.Alternatively, the paper has also tried replacing AlexNet by VGG-16 with batch normalization to see impact on performance.To generating initial labels for the model to train on, we initialize AlexNet with random weights and the last fully connected layer FC3 removed. We perform a forward pass on the model on images and take the feature vector coming from the second fully connected layer FC2 of the model on an image. This feature vector has a dimension of 4096.This process is repeated for all images in the batch for the whole dataset. Thus, if we have N total images, we will have an image-feature matrix of [N, 4096].Before performing clustering, dimensionality reduction is applied to the image-feature matrix.For dimensionality reduction, Principal Component Analysis(PCA) is applied to the features to reduce them from 4096 dimensions to 256 dimensions. The values are also whitened. 
The paper uses the  library to perform this at scale. Faiss provides an efficient implementation of PCA which can be applied for some image-feature matrix  as:Then, L2 normalization is applied to the values we get after PCA.Thus, we finally get a matrix of  for total N images. Now, K-means clustering is applied to the pre-processed features to get images and their corresponding clusters. These clusters will act as the pseudo-labels on which the model will be trained.The paper use Johnson’s implementation of K-means from the paper . It is available in the faiss library. Since clustering has to be run on all the images, it takes one-third of the total training time.After clustering is done, new batches of images are created such that images from each cluster has an equal chance of being included. Random augmentations are applied to these images.Once we have the images and clusters, we train our ConvNet model like regular supervised learning. We use a batch size of 256 and use cross-entropy loss to compare model predictions to the ground truth cluster label. The model learns useful representations.The model is trained for 500 epochs. The clustering step is run once at the start of each epoch to generate pseudo-labels for the whole dataset. Then, the regular training of ConvNet using cross-entropy loss is continued for all the batches.
The paper uses SGD optimizer with momentum of 0.9, learning rate of 0.05 and weight decay of \(10^{-5}\). They trained it on Pascal P100 GPU.The official implementation of Deep Cluster in PyTorch by the paper authors is available on . They also provide  for AlexNet and Resnet-50 architectures.If you found this blog post useful, please consider citing it as:Many self-supervised methods use  to generate surrogate labels and formulate an unsupervised learning problem as a supervised one. Some examples include rotation prediction, image colorization, jigsaw puzzles etc. However, such pretext tasks are domain-dependent and require expertise to design them. is a self-supervised method proposed by Caron et al. of Facebook AI Research that brings a different approach.
This method doesn’t require domain-specific knowledge and can be used to learn deep representations for scenarios where annotated data is scarce.DeepCluster combines two pieces: unsupervised clustering and deep neural networks. It proposes an end-to-end method to jointly learn parameters of a deep neural network and the cluster assignments of its representations. The features are generated and clustered iteratively to get both a trained model and labels as output artifacts.Let’s now understand how the deep cluster pipeline works with an interactive diagram.
As seen in the figure above, unlabeled images are taken and  are applied to them. Then, an  architecture such as  or  is used as the feature extractor. Initially, the  is initialized with randomly weights and we take the  from layer before the final classification head. Then,  is used to reduce the dimension of the  along with whitening and . Finally, the processed features are passed to  to get cluster assignment for each image.These cluster assignments are used as the  and the  is trained to predict these clusters. Cross-entropy loss is used to gauge the performance of the model. The model is trained for 100 epochs with the  step occurring once per epoch. Finally, we can take the  learned and use it for downstream tasks.Let’s see how DeepCluster is applied in practice with a step by step example of the whole pipeline from the input data to the output labels:
We take unlabeled images from the ImageNet dataset which consist of 1.3 million images uniformly distributed into 1000 classes. These images are prepared in mini-batches of 256.
The training set of N images can be denoted mathematically by:Transformations are applied to the images so that the features learned is invariant to augmentations. Two different augmentations are done, one when training model to learn representations and one when sending the image representations to the clustering algorithm:When model representations are to be sent for clustering, random augmentations are not used. The image is simply resized to 256*256 and the center crop is applied to get 224*224 image. Then normalization is applied.
In PyTorch, this can be implemented as:When the model is trained on image and labels, then we use random augmentations. The image is cropped to a random size and aspect ratio and then resized to 224*224. Then, the image is horizontally flipped with a 50% chance. Finally, we normalize the image with ImageNet mean and std.

In PyTorch, this can be implemented as:Once we get the normalized image, we convert it into grayscale. Then, we increase the local contrast of the image using the Sobel filters.
Below is a simplified snippet adapted from the author’s implementation . We can apply it on the augmented image  we got above.To perform clustering, we need to decide the number of clusters. This will be the number of classes the model will be trained on.
By default, ImageNet has 1000 classes, but the paper uses 10,000 clusters as this gives more fine-grained grouping of the unlabeled images. For example, if you previously had a grouping of cats and dogs and you increase clusters, then groupings of breeds of the cat and dog could be created.The paper primarily uses AlexNet architecture consisting of  and 3 fully connected layers. The Local Response Normalization layers are removed and Batch Normalization is applied instead. Dropout is also added. The filter size used is from 2012 competition: 96, 256, 384, 384, 256.Alternatively, the paper has also tried replacing AlexNet by VGG-16 with batch normalization to see impact on performance.To generating initial labels for the model to train on, we initialize AlexNet with random weights and the last fully connected layer FC3 removed. We perform a forward pass on the model on images and take the feature vector coming from the second fully connected layer FC2 of the model on an image. This feature vector has a dimension of 4096.This process is repeated for all images in the batch for the whole dataset. Thus, if we have N total images, we will have an image-feature matrix of [N, 4096].Before performing clustering, dimensionality reduction is applied to the image-feature matrix.For dimensionality reduction, Principal Component Analysis(PCA) is applied to the features to reduce them from 4096 dimensions to 256 dimensions. The values are also whitened. 
The paper uses the  library to perform this at scale. Faiss provides an efficient implementation of PCA which can be applied for some image-feature matrix  as:Then, L2 normalization is applied to the values we get after PCA.Thus, we finally get a matrix of  for total N images. Now, K-means clustering is applied to the pre-processed features to get images and their corresponding clusters. These clusters will act as the pseudo-labels on which the model will be trained.The paper use Johnson’s implementation of K-means from the paper . It is available in the faiss library. Since clustering has to be run on all the images, it takes one-third of the total training time.After clustering is done, new batches of images are created such that images from each cluster has an equal chance of being included. Random augmentations are applied to these images.Once we have the images and clusters, we train our ConvNet model like regular supervised learning. We use a batch size of 256 and use cross-entropy loss to compare model predictions to the ground truth cluster label. The model learns useful representations.The model is trained for 500 epochs. The clustering step is run once at the start of each epoch to generate pseudo-labels for the whole dataset. Then, the regular training of ConvNet using cross-entropy loss is continued for all the batches.
The paper uses SGD optimizer with momentum of 0.9, learning rate of 0.05 and weight decay of \(10^{-5}\). They trained it on Pascal P100 GPU.The official implementation of Deep Cluster in PyTorch by the paper authors is available on . They also provide  for AlexNet and Resnet-50 architectures.If you found this blog post useful, please consider citing it as:Many self-supervised methods use  to generate surrogate labels and formulate an unsupervised learning problem as a supervised one. Some examples include rotation prediction, image colorization, jigsaw puzzles etc. However, such pretext tasks are domain-dependent and require expertise to design them. is a self-supervised method proposed by Caron et al. of Facebook AI Research that brings a different approach.
This method doesn’t require domain-specific knowledge and can be used to learn deep representations for scenarios where annotated data is scarce.DeepCluster combines two pieces: unsupervised clustering and deep neural networks. It proposes an end-to-end method to jointly learn parameters of a deep neural network and the cluster assignments of its representations. The features are generated and clustered iteratively to get both a trained model and labels as output artifacts.Let’s now understand how the deep cluster pipeline works with an interactive diagram.
As seen in the figure above, unlabeled images are taken and  are applied to them. Then, an  architecture such as  or  is used as the feature extractor. Initially, the  is initialized with randomly weights and we take the  from layer before the final classification head. Then,  is used to reduce the dimension of the  along with whitening and . Finally, the processed features are passed to  to get cluster assignment for each image.These cluster assignments are used as the  and the  is trained to predict these clusters. Cross-entropy loss is used to gauge the performance of the model. The model is trained for 100 epochs with the  step occurring once per epoch. Finally, we can take the  learned and use it for downstream tasks.Let’s see how DeepCluster is applied in practice with a step by step example of the whole pipeline from the input data to the output labels:
We take unlabeled images from the ImageNet dataset which consist of 1.3 million images uniformly distributed into 1000 classes. These images are prepared in mini-batches of 256.
The training set of N images can be denoted mathematically by:Transformations are applied to the images so that the features learned is invariant to augmentations. Two different augmentations are done, one when training model to learn representations and one when sending the image representations to the clustering algorithm:When model representations are to be sent for clustering, random augmentations are not used. The image is simply resized to 256*256 and the center crop is applied to get 224*224 image. Then normalization is applied.
In PyTorch, this can be implemented as:When the model is trained on image and labels, then we use random augmentations. The image is cropped to a random size and aspect ratio and then resized to 224*224. Then, the image is horizontally flipped with a 50% chance. Finally, we normalize the image with ImageNet mean and std.

In PyTorch, this can be implemented as:Once we get the normalized image, we convert it into grayscale. Then, we increase the local contrast of the image using the Sobel filters.
Below is a simplified snippet adapted from the author’s implementation . We can apply it on the augmented image  we got above.To perform clustering, we need to decide the number of clusters. This will be the number of classes the model will be trained on.
By default, ImageNet has 1000 classes, but the paper uses 10,000 clusters as this gives more fine-grained grouping of the unlabeled images. For example, if you previously had a grouping of cats and dogs and you increase clusters, then groupings of breeds of the cat and dog could be created.The paper primarily uses AlexNet architecture consisting of  and 3 fully connected layers. The Local Response Normalization layers are removed and Batch Normalization is applied instead. Dropout is also added. The filter size used is from 2012 competition: 96, 256, 384, 384, 256.Alternatively, the paper has also tried replacing AlexNet by VGG-16 with batch normalization to see impact on performance.To generating initial labels for the model to train on, we initialize AlexNet with random weights and the last fully connected layer FC3 removed. We perform a forward pass on the model on images and take the feature vector coming from the second fully connected layer FC2 of the model on an image. This feature vector has a dimension of 4096.This process is repeated for all images in the batch for the whole dataset. Thus, if we have N total images, we will have an image-feature matrix of [N, 4096].Before performing clustering, dimensionality reduction is applied to the image-feature matrix.For dimensionality reduction, Principal Component Analysis(PCA) is applied to the features to reduce them from 4096 dimensions to 256 dimensions. The values are also whitened. 
The paper uses the  library to perform this at scale. Faiss provides an efficient implementation of PCA which can be applied for some image-feature matrix  as:Then, L2 normalization is applied to the values we get after PCA.Thus, we finally get a matrix of  for total N images. Now, K-means clustering is applied to the pre-processed features to get images and their corresponding clusters. These clusters will act as the pseudo-labels on which the model will be trained.The paper use Johnson’s implementation of K-means from the paper . It is available in the faiss library. Since clustering has to be run on all the images, it takes one-third of the total training time.After clustering is done, new batches of images are created such that images from each cluster has an equal chance of being included. Random augmentations are applied to these images.Once we have the images and clusters, we train our ConvNet model like regular supervised learning. We use a batch size of 256 and use cross-entropy loss to compare model predictions to the ground truth cluster label. The model learns useful representations.The model is trained for 500 epochs. The clustering step is run once at the start of each epoch to generate pseudo-labels for the whole dataset. Then, the regular training of ConvNet using cross-entropy loss is continued for all the batches.
The paper uses SGD optimizer with momentum of 0.9, learning rate of 0.05 and weight decay of \(10^{-5}\). They trained it on Pascal P100 GPU.The official implementation of Deep Cluster in PyTorch by the paper authors is available on . They also provide  for AlexNet and Resnet-50 architectures.If you found this blog post useful, please consider citing it as:Many self-supervised methods use  to generate surrogate labels and formulate an unsupervised learning problem as a supervised one. Some examples include rotation prediction, image colorization, jigsaw puzzles etc. However, such pretext tasks are domain-dependent and require expertise to design them. is a self-supervised method proposed by Caron et al. of Facebook AI Research that brings a different approach.
This method doesn’t require domain-specific knowledge and can be used to learn deep representations for scenarios where annotated data is scarce.DeepCluster combines two pieces: unsupervised clustering and deep neural networks. It proposes an end-to-end method to jointly learn parameters of a deep neural network and the cluster assignments of its representations. The features are generated and clustered iteratively to get both a trained model and labels as output artifacts.Let’s now understand how the deep cluster pipeline works with an interactive diagram.
As seen in the figure above, unlabeled images are taken and  are applied to them. Then, an  architecture such as  or  is used as the feature extractor. Initially, the  is initialized with randomly weights and we take the  from layer before the final classification head. Then,  is used to reduce the dimension of the  along with whitening and . Finally, the processed features are passed to  to get cluster assignment for each image.These cluster assignments are used as the  and the  is trained to predict these clusters. Cross-entropy loss is used to gauge the performance of the model. The model is trained for 100 epochs with the  step occurring once per epoch. Finally, we can take the  learned and use it for downstream tasks.Let’s see how DeepCluster is applied in practice with a step by step example of the whole pipeline from the input data to the output labels:
We take unlabeled images from the ImageNet dataset which consist of 1.3 million images uniformly distributed into 1000 classes. These images are prepared in mini-batches of 256.
The training set of N images can be denoted mathematically by:Transformations are applied to the images so that the features learned is invariant to augmentations. Two different augmentations are done, one when training model to learn representations and one when sending the image representations to the clustering algorithm:When model representations are to be sent for clustering, random augmentations are not used. The image is simply resized to 256*256 and the center crop is applied to get 224*224 image. Then normalization is applied.
In PyTorch, this can be implemented as:When the model is trained on image and labels, then we use random augmentations. The image is cropped to a random size and aspect ratio and then resized to 224*224. Then, the image is horizontally flipped with a 50% chance. Finally, we normalize the image with ImageNet mean and std.

In PyTorch, this can be implemented as:Once we get the normalized image, we convert it into grayscale. Then, we increase the local contrast of the image using the Sobel filters.
Below is a simplified snippet adapted from the author’s implementation . We can apply it on the augmented image  we got above.To perform clustering, we need to decide the number of clusters. This will be the number of classes the model will be trained on.
By default, ImageNet has 1000 classes, but the paper uses 10,000 clusters as this gives more fine-grained grouping of the unlabeled images. For example, if you previously had a grouping of cats and dogs and you increase clusters, then groupings of breeds of the cat and dog could be created.The paper primarily uses AlexNet architecture consisting of  and 3 fully connected layers. The Local Response Normalization layers are removed and Batch Normalization is applied instead. Dropout is also added. The filter size used is from 2012 competition: 96, 256, 384, 384, 256.Alternatively, the paper has also tried replacing AlexNet by VGG-16 with batch normalization to see impact on performance.To generating initial labels for the model to train on, we initialize AlexNet with random weights and the last fully connected layer FC3 removed. We perform a forward pass on the model on images and take the feature vector coming from the second fully connected layer FC2 of the model on an image. This feature vector has a dimension of 4096.This process is repeated for all images in the batch for the whole dataset. Thus, if we have N total images, we will have an image-feature matrix of [N, 4096].Before performing clustering, dimensionality reduction is applied to the image-feature matrix.For dimensionality reduction, Principal Component Analysis(PCA) is applied to the features to reduce them from 4096 dimensions to 256 dimensions. The values are also whitened. 
The paper uses the  library to perform this at scale. Faiss provides an efficient implementation of PCA which can be applied for some image-feature matrix  as:Then, L2 normalization is applied to the values we get after PCA.Thus, we finally get a matrix of  for total N images. Now, K-means clustering is applied to the pre-processed features to get images and their corresponding clusters. These clusters will act as the pseudo-labels on which the model will be trained.The paper use Johnson’s implementation of K-means from the paper . It is available in the faiss library. Since clustering has to be run on all the images, it takes one-third of the total training time.After clustering is done, new batches of images are created such that images from each cluster has an equal chance of being included. Random augmentations are applied to these images.Once we have the images and clusters, we train our ConvNet model like regular supervised learning. We use a batch size of 256 and use cross-entropy loss to compare model predictions to the ground truth cluster label. The model learns useful representations.The model is trained for 500 epochs. The clustering step is run once at the start of each epoch to generate pseudo-labels for the whole dataset. Then, the regular training of ConvNet using cross-entropy loss is continued for all the batches.
The paper uses SGD optimizer with momentum of 0.9, learning rate of 0.05 and weight decay of \(10^{-5}\). They trained it on Pascal P100 GPU.The official implementation of Deep Cluster in PyTorch by the paper authors is available on . They also provide  for AlexNet and Resnet-50 architectures.If you found this blog post useful, please consider citing it as:Many self-supervised methods use  to generate surrogate labels and formulate an unsupervised learning problem as a supervised one. Some examples include rotation prediction, image colorization, jigsaw puzzles etc. However, such pretext tasks are domain-dependent and require expertise to design them. is a self-supervised method proposed by Caron et al. of Facebook AI Research that brings a different approach.
This method doesn’t require domain-specific knowledge and can be used to learn deep representations for scenarios where annotated data is scarce.DeepCluster combines two pieces: unsupervised clustering and deep neural networks. It proposes an end-to-end method to jointly learn parameters of a deep neural network and the cluster assignments of its representations. The features are generated and clustered iteratively to get both a trained model and labels as output artifacts.Let’s now understand how the deep cluster pipeline works with an interactive diagram.
As seen in the figure above, unlabeled images are taken and  are applied to them. Then, an  architecture such as  or  is used as the feature extractor. Initially, the  is initialized with randomly weights and we take the  from layer before the final classification head. Then,  is used to reduce the dimension of the  along with whitening and . Finally, the processed features are passed to  to get cluster assignment for each image.These cluster assignments are used as the  and the  is trained to predict these clusters. Cross-entropy loss is used to gauge the performance of the model. The model is trained for 100 epochs with the  step occurring once per epoch. Finally, we can take the  learned and use it for downstream tasks.Let’s see how DeepCluster is applied in practice with a step by step example of the whole pipeline from the input data to the output labels:
We take unlabeled images from the ImageNet dataset which consist of 1.3 million images uniformly distributed into 1000 classes. These images are prepared in mini-batches of 256.
The training set of N images can be denoted mathematically by:Transformations are applied to the images so that the features learned is invariant to augmentations. Two different augmentations are done, one when training model to learn representations and one when sending the image representations to the clustering algorithm:When model representations are to be sent for clustering, random augmentations are not used. The image is simply resized to 256*256 and the center crop is applied to get 224*224 image. Then normalization is applied.
In PyTorch, this can be implemented as:When the model is trained on image and labels, then we use random augmentations. The image is cropped to a random size and aspect ratio and then resized to 224*224. Then, the image is horizontally flipped with a 50% chance. Finally, we normalize the image with ImageNet mean and std.

In PyTorch, this can be implemented as:Once we get the normalized image, we convert it into grayscale. Then, we increase the local contrast of the image using the Sobel filters.
Below is a simplified snippet adapted from the author’s implementation . We can apply it on the augmented image  we got above.To perform clustering, we need to decide the number of clusters. This will be the number of classes the model will be trained on.
By default, ImageNet has 1000 classes, but the paper uses 10,000 clusters as this gives more fine-grained grouping of the unlabeled images. For example, if you previously had a grouping of cats and dogs and you increase clusters, then groupings of breeds of the cat and dog could be created.The paper primarily uses AlexNet architecture consisting of  and 3 fully connected layers. The Local Response Normalization layers are removed and Batch Normalization is applied instead. Dropout is also added. The filter size used is from 2012 competition: 96, 256, 384, 384, 256.Alternatively, the paper has also tried replacing AlexNet by VGG-16 with batch normalization to see impact on performance.To generating initial labels for the model to train on, we initialize AlexNet with random weights and the last fully connected layer FC3 removed. We perform a forward pass on the model on images and take the feature vector coming from the second fully connected layer FC2 of the model on an image. This feature vector has a dimension of 4096.This process is repeated for all images in the batch for the whole dataset. Thus, if we have N total images, we will have an image-feature matrix of [N, 4096].Before performing clustering, dimensionality reduction is applied to the image-feature matrix.For dimensionality reduction, Principal Component Analysis(PCA) is applied to the features to reduce them from 4096 dimensions to 256 dimensions. The values are also whitened. 
The paper uses the  library to perform this at scale. Faiss provides an efficient implementation of PCA which can be applied for some image-feature matrix  as:Then, L2 normalization is applied to the values we get after PCA.Thus, we finally get a matrix of  for total N images. Now, K-means clustering is applied to the pre-processed features to get images and their corresponding clusters. These clusters will act as the pseudo-labels on which the model will be trained.The paper use Johnson’s implementation of K-means from the paper . It is available in the faiss library. Since clustering has to be run on all the images, it takes one-third of the total training time.After clustering is done, new batches of images are created such that images from each cluster has an equal chance of being included. Random augmentations are applied to these images.Once we have the images and clusters, we train our ConvNet model like regular supervised learning. We use a batch size of 256 and use cross-entropy loss to compare model predictions to the ground truth cluster label. The model learns useful representations.The model is trained for 500 epochs. The clustering step is run once at the start of each epoch to generate pseudo-labels for the whole dataset. Then, the regular training of ConvNet using cross-entropy loss is continued for all the batches.
The paper uses SGD optimizer with momentum of 0.9, learning rate of 0.05 and weight decay of \(10^{-5}\). They trained it on Pascal P100 GPU.The official implementation of Deep Cluster in PyTorch by the paper authors is available on . They also provide  for AlexNet and Resnet-50 architectures.If you found this blog post useful, please consider citing it as:Many self-supervised methods use  to generate surrogate labels and formulate an unsupervised learning problem as a supervised one. Some examples include rotation prediction, image colorization, jigsaw puzzles etc. However, such pretext tasks are domain-dependent and require expertise to design them. is a self-supervised method proposed by Caron et al. of Facebook AI Research that brings a different approach.
This method doesn’t require domain-specific knowledge and can be used to learn deep representations for scenarios where annotated data is scarce.DeepCluster combines two pieces: unsupervised clustering and deep neural networks. It proposes an end-to-end method to jointly learn parameters of a deep neural network and the cluster assignments of its representations. The features are generated and clustered iteratively to get both a trained model and labels as output artifacts.Let’s now understand how the deep cluster pipeline works with an interactive diagram.
As seen in the figure above, unlabeled images are taken and  are applied to them. Then, an  architecture such as  or  is used as the feature extractor. Initially, the  is initialized with randomly weights and we take the  from layer before the final classification head. Then,  is used to reduce the dimension of the  along with whitening and . Finally, the processed features are passed to  to get cluster assignment for each image.These cluster assignments are used as the  and the  is trained to predict these clusters. Cross-entropy loss is used to gauge the performance of the model. The model is trained for 100 epochs with the  step occurring once per epoch. Finally, we can take the  learned and use it for downstream tasks.Let’s see how DeepCluster is applied in practice with a step by step example of the whole pipeline from the input data to the output labels:
We take unlabeled images from the ImageNet dataset which consist of 1.3 million images uniformly distributed into 1000 classes. These images are prepared in mini-batches of 256.
The training set of N images can be denoted mathematically by:Transformations are applied to the images so that the features learned is invariant to augmentations. Two different augmentations are done, one when training model to learn representations and one when sending the image representations to the clustering algorithm:When model representations are to be sent for clustering, random augmentations are not used. The image is simply resized to 256*256 and the center crop is applied to get 224*224 image. Then normalization is applied.
In PyTorch, this can be implemented as:When the model is trained on image and labels, then we use random augmentations. The image is cropped to a random size and aspect ratio and then resized to 224*224. Then, the image is horizontally flipped with a 50% chance. Finally, we normalize the image with ImageNet mean and std.

In PyTorch, this can be implemented as:Once we get the normalized image, we convert it into grayscale. Then, we increase the local contrast of the image using the Sobel filters.
Below is a simplified snippet adapted from the author’s implementation . We can apply it on the augmented image  we got above.To perform clustering, we need to decide the number of clusters. This will be the number of classes the model will be trained on.
By default, ImageNet has 1000 classes, but the paper uses 10,000 clusters as this gives more fine-grained grouping of the unlabeled images. For example, if you previously had a grouping of cats and dogs and you increase clusters, then groupings of breeds of the cat and dog could be created.The paper primarily uses AlexNet architecture consisting of  and 3 fully connected layers. The Local Response Normalization layers are removed and Batch Normalization is applied instead. Dropout is also added. The filter size used is from 2012 competition: 96, 256, 384, 384, 256.Alternatively, the paper has also tried replacing AlexNet by VGG-16 with batch normalization to see impact on performance.To generating initial labels for the model to train on, we initialize AlexNet with random weights and the last fully connected layer FC3 removed. We perform a forward pass on the model on images and take the feature vector coming from the second fully connected layer FC2 of the model on an image. This feature vector has a dimension of 4096.This process is repeated for all images in the batch for the whole dataset. Thus, if we have N total images, we will have an image-feature matrix of [N, 4096].Before performing clustering, dimensionality reduction is applied to the image-feature matrix.For dimensionality reduction, Principal Component Analysis(PCA) is applied to the features to reduce them from 4096 dimensions to 256 dimensions. The values are also whitened. 
The paper uses the  library to perform this at scale. Faiss provides an efficient implementation of PCA which can be applied for some image-feature matrix  as:Then, L2 normalization is applied to the values we get after PCA.Thus, we finally get a matrix of  for total N images. Now, K-means clustering is applied to the pre-processed features to get images and their corresponding clusters. These clusters will act as the pseudo-labels on which the model will be trained.The paper use Johnson’s implementation of K-means from the paper . It is available in the faiss library. Since clustering has to be run on all the images, it takes one-third of the total training time.After clustering is done, new batches of images are created such that images from each cluster has an equal chance of being included. Random augmentations are applied to these images.Once we have the images and clusters, we train our ConvNet model like regular supervised learning. We use a batch size of 256 and use cross-entropy loss to compare model predictions to the ground truth cluster label. The model learns useful representations.The model is trained for 500 epochs. The clustering step is run once at the start of each epoch to generate pseudo-labels for the whole dataset. Then, the regular training of ConvNet using cross-entropy loss is continued for all the batches.
The paper uses SGD optimizer with momentum of 0.9, learning rate of 0.05 and weight decay of \(10^{-5}\). They trained it on Pascal P100 GPU.The official implementation of Deep Cluster in PyTorch by the paper authors is available on . They also provide  for AlexNet and Resnet-50 architectures.If you found this blog post useful, please consider citing it as:Many self-supervised methods use  to generate surrogate labels and formulate an unsupervised learning problem as a supervised one. Some examples include rotation prediction, image colorization, jigsaw puzzles etc. However, such pretext tasks are domain-dependent and require expertise to design them. is a self-supervised method proposed by Caron et al. of Facebook AI Research that brings a different approach.
This method doesn’t require domain-specific knowledge and can be used to learn deep representations for scenarios where annotated data is scarce.DeepCluster combines two pieces: unsupervised clustering and deep neural networks. It proposes an end-to-end method to jointly learn parameters of a deep neural network and the cluster assignments of its representations. The features are generated and clustered iteratively to get both a trained model and labels as output artifacts.Let’s now understand how the deep cluster pipeline works with an interactive diagram.
As seen in the figure above, unlabeled images are taken and  are applied to them. Then, an  architecture such as  or  is used as the feature extractor. Initially, the  is initialized with randomly weights and we take the  from layer before the final classification head. Then,  is used to reduce the dimension of the  along with whitening and . Finally, the processed features are passed to  to get cluster assignment for each image.These cluster assignments are used as the  and the  is trained to predict these clusters. Cross-entropy loss is used to gauge the performance of the model. The model is trained for 100 epochs with the  step occurring once per epoch. Finally, we can take the  learned and use it for downstream tasks.Let’s see how DeepCluster is applied in practice with a step by step example of the whole pipeline from the input data to the output labels:
We take unlabeled images from the ImageNet dataset which consist of 1.3 million images uniformly distributed into 1000 classes. These images are prepared in mini-batches of 256.
The training set of N images can be denoted mathematically by:Transformations are applied to the images so that the features learned is invariant to augmentations. Two different augmentations are done, one when training model to learn representations and one when sending the image representations to the clustering algorithm:When model representations are to be sent for clustering, random augmentations are not used. The image is simply resized to 256*256 and the center crop is applied to get 224*224 image. Then normalization is applied.
In PyTorch, this can be implemented as:When the model is trained on image and labels, then we use random augmentations. The image is cropped to a random size and aspect ratio and then resized to 224*224. Then, the image is horizontally flipped with a 50% chance. Finally, we normalize the image with ImageNet mean and std.

In PyTorch, this can be implemented as:Once we get the normalized image, we convert it into grayscale. Then, we increase the local contrast of the image using the Sobel filters.
Below is a simplified snippet adapted from the author’s implementation . We can apply it on the augmented image  we got above.To perform clustering, we need to decide the number of clusters. This will be the number of classes the model will be trained on.
By default, ImageNet has 1000 classes, but the paper uses 10,000 clusters as this gives more fine-grained grouping of the unlabeled images. For example, if you previously had a grouping of cats and dogs and you increase clusters, then groupings of breeds of the cat and dog could be created.The paper primarily uses AlexNet architecture consisting of  and 3 fully connected layers. The Local Response Normalization layers are removed and Batch Normalization is applied instead. Dropout is also added. The filter size used is from 2012 competition: 96, 256, 384, 384, 256.Alternatively, the paper has also tried replacing AlexNet by VGG-16 with batch normalization to see impact on performance.To generating initial labels for the model to train on, we initialize AlexNet with random weights and the last fully connected layer FC3 removed. We perform a forward pass on the model on images and take the feature vector coming from the second fully connected layer FC2 of the model on an image. This feature vector has a dimension of 4096.This process is repeated for all images in the batch for the whole dataset. Thus, if we have N total images, we will have an image-feature matrix of [N, 4096].Before performing clustering, dimensionality reduction is applied to the image-feature matrix.For dimensionality reduction, Principal Component Analysis(PCA) is applied to the features to reduce them from 4096 dimensions to 256 dimensions. The values are also whitened. 
The paper uses the  library to perform this at scale. Faiss provides an efficient implementation of PCA which can be applied for some image-feature matrix  as:Then, L2 normalization is applied to the values we get after PCA.Thus, we finally get a matrix of  for total N images. Now, K-means clustering is applied to the pre-processed features to get images and their corresponding clusters. These clusters will act as the pseudo-labels on which the model will be trained.The paper use Johnson’s implementation of K-means from the paper . It is available in the faiss library. Since clustering has to be run on all the images, it takes one-third of the total training time.After clustering is done, new batches of images are created such that images from each cluster has an equal chance of being included. Random augmentations are applied to these images.Once we have the images and clusters, we train our ConvNet model like regular supervised learning. We use a batch size of 256 and use cross-entropy loss to compare model predictions to the ground truth cluster label. The model learns useful representations.The model is trained for 500 epochs. The clustering step is run once at the start of each epoch to generate pseudo-labels for the whole dataset. Then, the regular training of ConvNet using cross-entropy loss is continued for all the batches.
The paper uses SGD optimizer with momentum of 0.9, learning rate of 0.05 and weight decay of \(10^{-5}\). They trained it on Pascal P100 GPU.The official implementation of Deep Cluster in PyTorch by the paper authors is available on . They also provide  for AlexNet and Resnet-50 architectures.If you found this blog post useful, please consider citing it as: