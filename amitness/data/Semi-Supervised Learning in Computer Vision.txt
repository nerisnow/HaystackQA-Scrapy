Semi-supervised learning methods for Computer Vision have been advancing quickly in the past few years. Current state-of-the-art methods are simplifying prior work in terms of architecture and loss function or introducing hybrid methods by blending different formulations.In this post, I will illustrate the key ideas of these recent methods for semi-supervised learning through diagrams.In this semi-supervised formulation, a model is trained on labeled data and used to predict pseudo-labels for the unlabeled data. The model is then trained on both ground truth labels and pseudo-labels simultaneously. proposed a very simple and efficient formulation called “Pseudo-label” in 2013.The idea is to train a  simultaneously on a batch of both labeled and unlabeled images. The  is trained on labeled images in usual supervised manner with a cross-entropy loss. The same model is used to get predictions for a batch of unlabeled images and the  is used as the . Then, cross-entropy loss is calculated by comparing  predictions and the pseudo-label for the unlabeled images .The total loss is a weighted sum of the labeled and unlabeled loss terms.To make sure the model has learned enough from the labeled data, the \(\alpha_t\) term is set to 0 during the initial 100 training steps. It is then gradually increased up to 600 training steps and then kept constant.
 proposed a semi-supervised method inspired by Knowledge Distillation called “Noisy Student” in 2019.The key idea is to train two separate models called  and . The  is first trained on the labeled images and then it is used to infer the pseudo-labels for the unlabeled images. These pseudo-labels can either be soft-label or converted to hard-label by . Then, the labeled and unlabeled images are combined together and a  is trained on this combined data. The images are augmented using RandAugment as a form of input noise. Also, model noise such as Dropout and Stochastic Depth are incorporated in the student model architecture.Once a  is trained, it becomes the new  and this process is repeated for three iterations.This paradigm uses the idea that  predictions on an unlabeled image should remain the same even after adding noise. We could use input noise such as Image Augmentation and Gaussian noise. Noise can also be incorporated in the architecture itself using Dropout.This model was proposed by  in a conference paper at ICLR 2017.The key idea is to create two random augmentations of an image for both labeled and unlabeled data. Then, a  is used to predict the label of both these images. The  of these two  is used as a . For labeled images, we also calculate the . The total loss is a weighted sum of these two loss terms. A weight  is applied to decide how much the consistency loss contributes in the overall loss.This method was also proposed by  in the same paper as the pi-model. It modifies the π-model by leveraging the  of predictions.The key idea is to use the  of past predictions as one view. To get another view, we augment the image as usual and a  is used to predict the label. The  of  and  is used as a . For labeled images, we also calculate the . The final loss is a weighted sum of these two loss terms. A weight  is applied to decide how much the consistency loss contributes in the overall loss.This method was proposed by . The general approach is similar to Temporal Ensembling but it uses Exponential Moving Average(EMA) of the model parameters instead of predictions.The key idea is to have two models called  and . The  model is a regular model with dropout. And the  model has the same architecture as the  model but its weights are set using an  of the weights of  model. For a labeled or unlabeled image, we create two random augmented versions of the image. Then, the  model is used to predict  for first image. And, the  model is used to predict the  for the second augmented image. The  of these two  is used as a . For labeled images, we also calculate the . The final loss is a weighted sum of these two loss terms. A weight  is applied to decide how much the consistency loss contributes in the overall loss.This method was proposed by . It uses the concept of adversarial attack for consistency regularization.The key idea is to generate an adversarial transformation of an image that will change the model prediction. To do so, first, an image is taken and an adversarial variant of it is created such that the KL-divergence between the model output for the original image and the adversarial image is maximized.Then we proceed as previous methods. We take a labeled/unlabeled image as first view and take its adversarial example generated in previous step as the second view. Then, the same  is used to predict  for both images. The  of these two  is used as a . For labeled images, we also calculate the . The final loss is a weighted sum of these two loss terms. A weight  is applied to decide how much the consistency loss contributes in the overall loss.This method was proposed by  and works for both images and text. Here, we will understand the method in the context of images.The key idea is to create an augmented version of a unlabeled image using AutoAugment. Then, a same  is used to predict the label of both these images. The  of these two  is used as a . For labeled images, we only calculate the  and don’t calculate any . The final loss is a weighted sum of these two loss terms. A weight  is applied to decide how much the consistency loss contributes in the overall loss.This paradigm combines ideas from previous work such as self-training and consistency regularization along with additional components for performance improvement.This holistic method was proposed by .To understand this method, let’s take a walk through each of the steps.i. For the labeled image, we create an augmentation of it. For the unlabeled image, we create K augmentations and get the model  on all K-images. Then, the  are  and  is applied to get a final pseudo-label. This pseudo-label will be used for all the K-augmentations.ii. The batches of augmented labeled and unlabeled images are combined and the whole group is shuffled. Then, the first N images of this group are taken as \(W_L\), and the remaining M images are taken as \(W_U\).iii. Now, Mixup is applied between the augmented labeled batch and group \(W_L\). Similarly, mixup is applied between the M augmented unlabeled group and the \(W_U\) group. Thus, we get the final labeled and unlabeled group.iv. Now, for the labeled group, we take model predictions and compute  with the ground truth mixup labels. Similarly, for the unlabeled group, we compute model predictions and compute  with the mixup pseudo labels. A weighted sum is taken of these two terms with \(\lambda\) weighting the MSE loss.This method was proposed by  and combines pseudo-labeling and consistency regularization while vastly simplifying the overall method. It got state of the art results on a wide range of benchmarks.As seen, we train a supervised model on our labeled images with cross-entropy loss. For each unlabeled image,  and  are applied to get two images. The  is passed to our model and we get prediction over classes. The probability for the most confident class is compared to a . If it is above the , then we take that class as the ground label i.e. . Then, the  image is passed through our model to get a prediction over classes. This  is compared to ground truth  using cross-entropy loss. Both the losses are combined and the model is optimized.If you want to learn more about FixMatch, I have an  that goes over it in depth.Here is a high-level summary of the differences between all the above-mentioned methods.To evaluate the performance of these semi-supervised methods, the following datasets are commonly used. The authors simulate a low-data regime by using only a small portion(e.g. 40/250/4000/10000 examples) of the whole dataset as labeled and treating the remaining as the unlabeled set.Thus, we got an overview of how semi-supervised methods for Computer Vision have progressed over the years. This is a really important line of research that can have a direct impact on the industry.If you found this blog post useful, please consider citing it as:Semi-supervised learning methods for Computer Vision have been advancing quickly in the past few years. Current state-of-the-art methods are simplifying prior work in terms of architecture and loss function or introducing hybrid methods by blending different formulations.In this post, I will illustrate the key ideas of these recent methods for semi-supervised learning through diagrams.In this semi-supervised formulation, a model is trained on labeled data and used to predict pseudo-labels for the unlabeled data. The model is then trained on both ground truth labels and pseudo-labels simultaneously. proposed a very simple and efficient formulation called “Pseudo-label” in 2013.The idea is to train a  simultaneously on a batch of both labeled and unlabeled images. The  is trained on labeled images in usual supervised manner with a cross-entropy loss. The same model is used to get predictions for a batch of unlabeled images and the  is used as the . Then, cross-entropy loss is calculated by comparing  predictions and the pseudo-label for the unlabeled images .The total loss is a weighted sum of the labeled and unlabeled loss terms.To make sure the model has learned enough from the labeled data, the \(\alpha_t\) term is set to 0 during the initial 100 training steps. It is then gradually increased up to 600 training steps and then kept constant.
 proposed a semi-supervised method inspired by Knowledge Distillation called “Noisy Student” in 2019.The key idea is to train two separate models called  and . The  is first trained on the labeled images and then it is used to infer the pseudo-labels for the unlabeled images. These pseudo-labels can either be soft-label or converted to hard-label by . Then, the labeled and unlabeled images are combined together and a  is trained on this combined data. The images are augmented using RandAugment as a form of input noise. Also, model noise such as Dropout and Stochastic Depth are incorporated in the student model architecture.Once a  is trained, it becomes the new  and this process is repeated for three iterations.This paradigm uses the idea that  predictions on an unlabeled image should remain the same even after adding noise. We could use input noise such as Image Augmentation and Gaussian noise. Noise can also be incorporated in the architecture itself using Dropout.This model was proposed by  in a conference paper at ICLR 2017.The key idea is to create two random augmentations of an image for both labeled and unlabeled data. Then, a  is used to predict the label of both these images. The  of these two  is used as a . For labeled images, we also calculate the . The total loss is a weighted sum of these two loss terms. A weight  is applied to decide how much the consistency loss contributes in the overall loss.This method was also proposed by  in the same paper as the pi-model. It modifies the π-model by leveraging the  of predictions.The key idea is to use the  of past predictions as one view. To get another view, we augment the image as usual and a  is used to predict the label. The  of  and  is used as a . For labeled images, we also calculate the . The final loss is a weighted sum of these two loss terms. A weight  is applied to decide how much the consistency loss contributes in the overall loss.This method was proposed by . The general approach is similar to Temporal Ensembling but it uses Exponential Moving Average(EMA) of the model parameters instead of predictions.The key idea is to have two models called  and . The  model is a regular model with dropout. And the  model has the same architecture as the  model but its weights are set using an  of the weights of  model. For a labeled or unlabeled image, we create two random augmented versions of the image. Then, the  model is used to predict  for first image. And, the  model is used to predict the  for the second augmented image. The  of these two  is used as a . For labeled images, we also calculate the . The final loss is a weighted sum of these two loss terms. A weight  is applied to decide how much the consistency loss contributes in the overall loss.This method was proposed by . It uses the concept of adversarial attack for consistency regularization.The key idea is to generate an adversarial transformation of an image that will change the model prediction. To do so, first, an image is taken and an adversarial variant of it is created such that the KL-divergence between the model output for the original image and the adversarial image is maximized.Then we proceed as previous methods. We take a labeled/unlabeled image as first view and take its adversarial example generated in previous step as the second view. Then, the same  is used to predict  for both images. The  of these two  is used as a . For labeled images, we also calculate the . The final loss is a weighted sum of these two loss terms. A weight  is applied to decide how much the consistency loss contributes in the overall loss.This method was proposed by  and works for both images and text. Here, we will understand the method in the context of images.The key idea is to create an augmented version of a unlabeled image using AutoAugment. Then, a same  is used to predict the label of both these images. The  of these two  is used as a . For labeled images, we only calculate the  and don’t calculate any . The final loss is a weighted sum of these two loss terms. A weight  is applied to decide how much the consistency loss contributes in the overall loss.This paradigm combines ideas from previous work such as self-training and consistency regularization along with additional components for performance improvement.This holistic method was proposed by .To understand this method, let’s take a walk through each of the steps.i. For the labeled image, we create an augmentation of it. For the unlabeled image, we create K augmentations and get the model  on all K-images. Then, the  are  and  is applied to get a final pseudo-label. This pseudo-label will be used for all the K-augmentations.ii. The batches of augmented labeled and unlabeled images are combined and the whole group is shuffled. Then, the first N images of this group are taken as \(W_L\), and the remaining M images are taken as \(W_U\).iii. Now, Mixup is applied between the augmented labeled batch and group \(W_L\). Similarly, mixup is applied between the M augmented unlabeled group and the \(W_U\) group. Thus, we get the final labeled and unlabeled group.iv. Now, for the labeled group, we take model predictions and compute  with the ground truth mixup labels. Similarly, for the unlabeled group, we compute model predictions and compute  with the mixup pseudo labels. A weighted sum is taken of these two terms with \(\lambda\) weighting the MSE loss.This method was proposed by  and combines pseudo-labeling and consistency regularization while vastly simplifying the overall method. It got state of the art results on a wide range of benchmarks.As seen, we train a supervised model on our labeled images with cross-entropy loss. For each unlabeled image,  and  are applied to get two images. The  is passed to our model and we get prediction over classes. The probability for the most confident class is compared to a . If it is above the , then we take that class as the ground label i.e. . Then, the  image is passed through our model to get a prediction over classes. This  is compared to ground truth  using cross-entropy loss. Both the losses are combined and the model is optimized.If you want to learn more about FixMatch, I have an  that goes over it in depth.Here is a high-level summary of the differences between all the above-mentioned methods.To evaluate the performance of these semi-supervised methods, the following datasets are commonly used. The authors simulate a low-data regime by using only a small portion(e.g. 40/250/4000/10000 examples) of the whole dataset as labeled and treating the remaining as the unlabeled set.Thus, we got an overview of how semi-supervised methods for Computer Vision have progressed over the years. This is a really important line of research that can have a direct impact on the industry.If you found this blog post useful, please consider citing it as:Semi-supervised learning methods for Computer Vision have been advancing quickly in the past few years. Current state-of-the-art methods are simplifying prior work in terms of architecture and loss function or introducing hybrid methods by blending different formulations.In this post, I will illustrate the key ideas of these recent methods for semi-supervised learning through diagrams.In this semi-supervised formulation, a model is trained on labeled data and used to predict pseudo-labels for the unlabeled data. The model is then trained on both ground truth labels and pseudo-labels simultaneously. proposed a very simple and efficient formulation called “Pseudo-label” in 2013.The idea is to train a  simultaneously on a batch of both labeled and unlabeled images. The  is trained on labeled images in usual supervised manner with a cross-entropy loss. The same model is used to get predictions for a batch of unlabeled images and the  is used as the . Then, cross-entropy loss is calculated by comparing  predictions and the pseudo-label for the unlabeled images .The total loss is a weighted sum of the labeled and unlabeled loss terms.To make sure the model has learned enough from the labeled data, the \(\alpha_t\) term is set to 0 during the initial 100 training steps. It is then gradually increased up to 600 training steps and then kept constant.
 proposed a semi-supervised method inspired by Knowledge Distillation called “Noisy Student” in 2019.The key idea is to train two separate models called  and . The  is first trained on the labeled images and then it is used to infer the pseudo-labels for the unlabeled images. These pseudo-labels can either be soft-label or converted to hard-label by . Then, the labeled and unlabeled images are combined together and a  is trained on this combined data. The images are augmented using RandAugment as a form of input noise. Also, model noise such as Dropout and Stochastic Depth are incorporated in the student model architecture.Once a  is trained, it becomes the new  and this process is repeated for three iterations.This paradigm uses the idea that  predictions on an unlabeled image should remain the same even after adding noise. We could use input noise such as Image Augmentation and Gaussian noise. Noise can also be incorporated in the architecture itself using Dropout.This model was proposed by  in a conference paper at ICLR 2017.The key idea is to create two random augmentations of an image for both labeled and unlabeled data. Then, a  is used to predict the label of both these images. The  of these two  is used as a . For labeled images, we also calculate the . The total loss is a weighted sum of these two loss terms. A weight  is applied to decide how much the consistency loss contributes in the overall loss.This method was also proposed by  in the same paper as the pi-model. It modifies the π-model by leveraging the  of predictions.The key idea is to use the  of past predictions as one view. To get another view, we augment the image as usual and a  is used to predict the label. The  of  and  is used as a . For labeled images, we also calculate the . The final loss is a weighted sum of these two loss terms. A weight  is applied to decide how much the consistency loss contributes in the overall loss.This method was proposed by . The general approach is similar to Temporal Ensembling but it uses Exponential Moving Average(EMA) of the model parameters instead of predictions.The key idea is to have two models called  and . The  model is a regular model with dropout. And the  model has the same architecture as the  model but its weights are set using an  of the weights of  model. For a labeled or unlabeled image, we create two random augmented versions of the image. Then, the  model is used to predict  for first image. And, the  model is used to predict the  for the second augmented image. The  of these two  is used as a . For labeled images, we also calculate the . The final loss is a weighted sum of these two loss terms. A weight  is applied to decide how much the consistency loss contributes in the overall loss.This method was proposed by . It uses the concept of adversarial attack for consistency regularization.The key idea is to generate an adversarial transformation of an image that will change the model prediction. To do so, first, an image is taken and an adversarial variant of it is created such that the KL-divergence between the model output for the original image and the adversarial image is maximized.Then we proceed as previous methods. We take a labeled/unlabeled image as first view and take its adversarial example generated in previous step as the second view. Then, the same  is used to predict  for both images. The  of these two  is used as a . For labeled images, we also calculate the . The final loss is a weighted sum of these two loss terms. A weight  is applied to decide how much the consistency loss contributes in the overall loss.This method was proposed by  and works for both images and text. Here, we will understand the method in the context of images.The key idea is to create an augmented version of a unlabeled image using AutoAugment. Then, a same  is used to predict the label of both these images. The  of these two  is used as a . For labeled images, we only calculate the  and don’t calculate any . The final loss is a weighted sum of these two loss terms. A weight  is applied to decide how much the consistency loss contributes in the overall loss.This paradigm combines ideas from previous work such as self-training and consistency regularization along with additional components for performance improvement.This holistic method was proposed by .To understand this method, let’s take a walk through each of the steps.i. For the labeled image, we create an augmentation of it. For the unlabeled image, we create K augmentations and get the model  on all K-images. Then, the  are  and  is applied to get a final pseudo-label. This pseudo-label will be used for all the K-augmentations.ii. The batches of augmented labeled and unlabeled images are combined and the whole group is shuffled. Then, the first N images of this group are taken as \(W_L\), and the remaining M images are taken as \(W_U\).iii. Now, Mixup is applied between the augmented labeled batch and group \(W_L\). Similarly, mixup is applied between the M augmented unlabeled group and the \(W_U\) group. Thus, we get the final labeled and unlabeled group.iv. Now, for the labeled group, we take model predictions and compute  with the ground truth mixup labels. Similarly, for the unlabeled group, we compute model predictions and compute  with the mixup pseudo labels. A weighted sum is taken of these two terms with \(\lambda\) weighting the MSE loss.This method was proposed by  and combines pseudo-labeling and consistency regularization while vastly simplifying the overall method. It got state of the art results on a wide range of benchmarks.As seen, we train a supervised model on our labeled images with cross-entropy loss. For each unlabeled image,  and  are applied to get two images. The  is passed to our model and we get prediction over classes. The probability for the most confident class is compared to a . If it is above the , then we take that class as the ground label i.e. . Then, the  image is passed through our model to get a prediction over classes. This  is compared to ground truth  using cross-entropy loss. Both the losses are combined and the model is optimized.If you want to learn more about FixMatch, I have an  that goes over it in depth.Here is a high-level summary of the differences between all the above-mentioned methods.To evaluate the performance of these semi-supervised methods, the following datasets are commonly used. The authors simulate a low-data regime by using only a small portion(e.g. 40/250/4000/10000 examples) of the whole dataset as labeled and treating the remaining as the unlabeled set.Thus, we got an overview of how semi-supervised methods for Computer Vision have progressed over the years. This is a really important line of research that can have a direct impact on the industry.If you found this blog post useful, please consider citing it as:Semi-supervised learning methods for Computer Vision have been advancing quickly in the past few years. Current state-of-the-art methods are simplifying prior work in terms of architecture and loss function or introducing hybrid methods by blending different formulations.In this post, I will illustrate the key ideas of these recent methods for semi-supervised learning through diagrams.In this semi-supervised formulation, a model is trained on labeled data and used to predict pseudo-labels for the unlabeled data. The model is then trained on both ground truth labels and pseudo-labels simultaneously. proposed a very simple and efficient formulation called “Pseudo-label” in 2013.The idea is to train a  simultaneously on a batch of both labeled and unlabeled images. The  is trained on labeled images in usual supervised manner with a cross-entropy loss. The same model is used to get predictions for a batch of unlabeled images and the  is used as the . Then, cross-entropy loss is calculated by comparing  predictions and the pseudo-label for the unlabeled images .The total loss is a weighted sum of the labeled and unlabeled loss terms.To make sure the model has learned enough from the labeled data, the \(\alpha_t\) term is set to 0 during the initial 100 training steps. It is then gradually increased up to 600 training steps and then kept constant.
 proposed a semi-supervised method inspired by Knowledge Distillation called “Noisy Student” in 2019.The key idea is to train two separate models called  and . The  is first trained on the labeled images and then it is used to infer the pseudo-labels for the unlabeled images. These pseudo-labels can either be soft-label or converted to hard-label by . Then, the labeled and unlabeled images are combined together and a  is trained on this combined data. The images are augmented using RandAugment as a form of input noise. Also, model noise such as Dropout and Stochastic Depth are incorporated in the student model architecture.Once a  is trained, it becomes the new  and this process is repeated for three iterations.This paradigm uses the idea that  predictions on an unlabeled image should remain the same even after adding noise. We could use input noise such as Image Augmentation and Gaussian noise. Noise can also be incorporated in the architecture itself using Dropout.This model was proposed by  in a conference paper at ICLR 2017.The key idea is to create two random augmentations of an image for both labeled and unlabeled data. Then, a  is used to predict the label of both these images. The  of these two  is used as a . For labeled images, we also calculate the . The total loss is a weighted sum of these two loss terms. A weight  is applied to decide how much the consistency loss contributes in the overall loss.This method was also proposed by  in the same paper as the pi-model. It modifies the π-model by leveraging the  of predictions.The key idea is to use the  of past predictions as one view. To get another view, we augment the image as usual and a  is used to predict the label. The  of  and  is used as a . For labeled images, we also calculate the . The final loss is a weighted sum of these two loss terms. A weight  is applied to decide how much the consistency loss contributes in the overall loss.This method was proposed by . The general approach is similar to Temporal Ensembling but it uses Exponential Moving Average(EMA) of the model parameters instead of predictions.The key idea is to have two models called  and . The  model is a regular model with dropout. And the  model has the same architecture as the  model but its weights are set using an  of the weights of  model. For a labeled or unlabeled image, we create two random augmented versions of the image. Then, the  model is used to predict  for first image. And, the  model is used to predict the  for the second augmented image. The  of these two  is used as a . For labeled images, we also calculate the . The final loss is a weighted sum of these two loss terms. A weight  is applied to decide how much the consistency loss contributes in the overall loss.This method was proposed by . It uses the concept of adversarial attack for consistency regularization.The key idea is to generate an adversarial transformation of an image that will change the model prediction. To do so, first, an image is taken and an adversarial variant of it is created such that the KL-divergence between the model output for the original image and the adversarial image is maximized.Then we proceed as previous methods. We take a labeled/unlabeled image as first view and take its adversarial example generated in previous step as the second view. Then, the same  is used to predict  for both images. The  of these two  is used as a . For labeled images, we also calculate the . The final loss is a weighted sum of these two loss terms. A weight  is applied to decide how much the consistency loss contributes in the overall loss.This method was proposed by  and works for both images and text. Here, we will understand the method in the context of images.The key idea is to create an augmented version of a unlabeled image using AutoAugment. Then, a same  is used to predict the label of both these images. The  of these two  is used as a . For labeled images, we only calculate the  and don’t calculate any . The final loss is a weighted sum of these two loss terms. A weight  is applied to decide how much the consistency loss contributes in the overall loss.This paradigm combines ideas from previous work such as self-training and consistency regularization along with additional components for performance improvement.This holistic method was proposed by .To understand this method, let’s take a walk through each of the steps.i. For the labeled image, we create an augmentation of it. For the unlabeled image, we create K augmentations and get the model  on all K-images. Then, the  are  and  is applied to get a final pseudo-label. This pseudo-label will be used for all the K-augmentations.ii. The batches of augmented labeled and unlabeled images are combined and the whole group is shuffled. Then, the first N images of this group are taken as \(W_L\), and the remaining M images are taken as \(W_U\).iii. Now, Mixup is applied between the augmented labeled batch and group \(W_L\). Similarly, mixup is applied between the M augmented unlabeled group and the \(W_U\) group. Thus, we get the final labeled and unlabeled group.iv. Now, for the labeled group, we take model predictions and compute  with the ground truth mixup labels. Similarly, for the unlabeled group, we compute model predictions and compute  with the mixup pseudo labels. A weighted sum is taken of these two terms with \(\lambda\) weighting the MSE loss.This method was proposed by  and combines pseudo-labeling and consistency regularization while vastly simplifying the overall method. It got state of the art results on a wide range of benchmarks.As seen, we train a supervised model on our labeled images with cross-entropy loss. For each unlabeled image,  and  are applied to get two images. The  is passed to our model and we get prediction over classes. The probability for the most confident class is compared to a . If it is above the , then we take that class as the ground label i.e. . Then, the  image is passed through our model to get a prediction over classes. This  is compared to ground truth  using cross-entropy loss. Both the losses are combined and the model is optimized.If you want to learn more about FixMatch, I have an  that goes over it in depth.Here is a high-level summary of the differences between all the above-mentioned methods.To evaluate the performance of these semi-supervised methods, the following datasets are commonly used. The authors simulate a low-data regime by using only a small portion(e.g. 40/250/4000/10000 examples) of the whole dataset as labeled and treating the remaining as the unlabeled set.Thus, we got an overview of how semi-supervised methods for Computer Vision have progressed over the years. This is a really important line of research that can have a direct impact on the industry.If you found this blog post useful, please consider citing it as:Semi-supervised learning methods for Computer Vision have been advancing quickly in the past few years. Current state-of-the-art methods are simplifying prior work in terms of architecture and loss function or introducing hybrid methods by blending different formulations.In this post, I will illustrate the key ideas of these recent methods for semi-supervised learning through diagrams.In this semi-supervised formulation, a model is trained on labeled data and used to predict pseudo-labels for the unlabeled data. The model is then trained on both ground truth labels and pseudo-labels simultaneously. proposed a very simple and efficient formulation called “Pseudo-label” in 2013.The idea is to train a  simultaneously on a batch of both labeled and unlabeled images. The  is trained on labeled images in usual supervised manner with a cross-entropy loss. The same model is used to get predictions for a batch of unlabeled images and the  is used as the . Then, cross-entropy loss is calculated by comparing  predictions and the pseudo-label for the unlabeled images .The total loss is a weighted sum of the labeled and unlabeled loss terms.To make sure the model has learned enough from the labeled data, the \(\alpha_t\) term is set to 0 during the initial 100 training steps. It is then gradually increased up to 600 training steps and then kept constant.
 proposed a semi-supervised method inspired by Knowledge Distillation called “Noisy Student” in 2019.The key idea is to train two separate models called  and . The  is first trained on the labeled images and then it is used to infer the pseudo-labels for the unlabeled images. These pseudo-labels can either be soft-label or converted to hard-label by . Then, the labeled and unlabeled images are combined together and a  is trained on this combined data. The images are augmented using RandAugment as a form of input noise. Also, model noise such as Dropout and Stochastic Depth are incorporated in the student model architecture.Once a  is trained, it becomes the new  and this process is repeated for three iterations.This paradigm uses the idea that  predictions on an unlabeled image should remain the same even after adding noise. We could use input noise such as Image Augmentation and Gaussian noise. Noise can also be incorporated in the architecture itself using Dropout.This model was proposed by  in a conference paper at ICLR 2017.The key idea is to create two random augmentations of an image for both labeled and unlabeled data. Then, a  is used to predict the label of both these images. The  of these two  is used as a . For labeled images, we also calculate the . The total loss is a weighted sum of these two loss terms. A weight  is applied to decide how much the consistency loss contributes in the overall loss.This method was also proposed by  in the same paper as the pi-model. It modifies the π-model by leveraging the  of predictions.The key idea is to use the  of past predictions as one view. To get another view, we augment the image as usual and a  is used to predict the label. The  of  and  is used as a . For labeled images, we also calculate the . The final loss is a weighted sum of these two loss terms. A weight  is applied to decide how much the consistency loss contributes in the overall loss.This method was proposed by . The general approach is similar to Temporal Ensembling but it uses Exponential Moving Average(EMA) of the model parameters instead of predictions.The key idea is to have two models called  and . The  model is a regular model with dropout. And the  model has the same architecture as the  model but its weights are set using an  of the weights of  model. For a labeled or unlabeled image, we create two random augmented versions of the image. Then, the  model is used to predict  for first image. And, the  model is used to predict the  for the second augmented image. The  of these two  is used as a . For labeled images, we also calculate the . The final loss is a weighted sum of these two loss terms. A weight  is applied to decide how much the consistency loss contributes in the overall loss.This method was proposed by . It uses the concept of adversarial attack for consistency regularization.The key idea is to generate an adversarial transformation of an image that will change the model prediction. To do so, first, an image is taken and an adversarial variant of it is created such that the KL-divergence between the model output for the original image and the adversarial image is maximized.Then we proceed as previous methods. We take a labeled/unlabeled image as first view and take its adversarial example generated in previous step as the second view. Then, the same  is used to predict  for both images. The  of these two  is used as a . For labeled images, we also calculate the . The final loss is a weighted sum of these two loss terms. A weight  is applied to decide how much the consistency loss contributes in the overall loss.This method was proposed by  and works for both images and text. Here, we will understand the method in the context of images.The key idea is to create an augmented version of a unlabeled image using AutoAugment. Then, a same  is used to predict the label of both these images. The  of these two  is used as a . For labeled images, we only calculate the  and don’t calculate any . The final loss is a weighted sum of these two loss terms. A weight  is applied to decide how much the consistency loss contributes in the overall loss.This paradigm combines ideas from previous work such as self-training and consistency regularization along with additional components for performance improvement.This holistic method was proposed by .To understand this method, let’s take a walk through each of the steps.i. For the labeled image, we create an augmentation of it. For the unlabeled image, we create K augmentations and get the model  on all K-images. Then, the  are  and  is applied to get a final pseudo-label. This pseudo-label will be used for all the K-augmentations.ii. The batches of augmented labeled and unlabeled images are combined and the whole group is shuffled. Then, the first N images of this group are taken as \(W_L\), and the remaining M images are taken as \(W_U\).iii. Now, Mixup is applied between the augmented labeled batch and group \(W_L\). Similarly, mixup is applied between the M augmented unlabeled group and the \(W_U\) group. Thus, we get the final labeled and unlabeled group.iv. Now, for the labeled group, we take model predictions and compute  with the ground truth mixup labels. Similarly, for the unlabeled group, we compute model predictions and compute  with the mixup pseudo labels. A weighted sum is taken of these two terms with \(\lambda\) weighting the MSE loss.This method was proposed by  and combines pseudo-labeling and consistency regularization while vastly simplifying the overall method. It got state of the art results on a wide range of benchmarks.As seen, we train a supervised model on our labeled images with cross-entropy loss. For each unlabeled image,  and  are applied to get two images. The  is passed to our model and we get prediction over classes. The probability for the most confident class is compared to a . If it is above the , then we take that class as the ground label i.e. . Then, the  image is passed through our model to get a prediction over classes. This  is compared to ground truth  using cross-entropy loss. Both the losses are combined and the model is optimized.If you want to learn more about FixMatch, I have an  that goes over it in depth.Here is a high-level summary of the differences between all the above-mentioned methods.To evaluate the performance of these semi-supervised methods, the following datasets are commonly used. The authors simulate a low-data regime by using only a small portion(e.g. 40/250/4000/10000 examples) of the whole dataset as labeled and treating the remaining as the unlabeled set.Thus, we got an overview of how semi-supervised methods for Computer Vision have progressed over the years. This is a really important line of research that can have a direct impact on the industry.If you found this blog post useful, please consider citing it as:Semi-supervised learning methods for Computer Vision have been advancing quickly in the past few years. Current state-of-the-art methods are simplifying prior work in terms of architecture and loss function or introducing hybrid methods by blending different formulations.In this post, I will illustrate the key ideas of these recent methods for semi-supervised learning through diagrams.In this semi-supervised formulation, a model is trained on labeled data and used to predict pseudo-labels for the unlabeled data. The model is then trained on both ground truth labels and pseudo-labels simultaneously. proposed a very simple and efficient formulation called “Pseudo-label” in 2013.The idea is to train a  simultaneously on a batch of both labeled and unlabeled images. The  is trained on labeled images in usual supervised manner with a cross-entropy loss. The same model is used to get predictions for a batch of unlabeled images and the  is used as the . Then, cross-entropy loss is calculated by comparing  predictions and the pseudo-label for the unlabeled images .The total loss is a weighted sum of the labeled and unlabeled loss terms.To make sure the model has learned enough from the labeled data, the \(\alpha_t\) term is set to 0 during the initial 100 training steps. It is then gradually increased up to 600 training steps and then kept constant.
 proposed a semi-supervised method inspired by Knowledge Distillation called “Noisy Student” in 2019.The key idea is to train two separate models called  and . The  is first trained on the labeled images and then it is used to infer the pseudo-labels for the unlabeled images. These pseudo-labels can either be soft-label or converted to hard-label by . Then, the labeled and unlabeled images are combined together and a  is trained on this combined data. The images are augmented using RandAugment as a form of input noise. Also, model noise such as Dropout and Stochastic Depth are incorporated in the student model architecture.Once a  is trained, it becomes the new  and this process is repeated for three iterations.This paradigm uses the idea that  predictions on an unlabeled image should remain the same even after adding noise. We could use input noise such as Image Augmentation and Gaussian noise. Noise can also be incorporated in the architecture itself using Dropout.This model was proposed by  in a conference paper at ICLR 2017.The key idea is to create two random augmentations of an image for both labeled and unlabeled data. Then, a  is used to predict the label of both these images. The  of these two  is used as a . For labeled images, we also calculate the . The total loss is a weighted sum of these two loss terms. A weight  is applied to decide how much the consistency loss contributes in the overall loss.This method was also proposed by  in the same paper as the pi-model. It modifies the π-model by leveraging the  of predictions.The key idea is to use the  of past predictions as one view. To get another view, we augment the image as usual and a  is used to predict the label. The  of  and  is used as a . For labeled images, we also calculate the . The final loss is a weighted sum of these two loss terms. A weight  is applied to decide how much the consistency loss contributes in the overall loss.This method was proposed by . The general approach is similar to Temporal Ensembling but it uses Exponential Moving Average(EMA) of the model parameters instead of predictions.The key idea is to have two models called  and . The  model is a regular model with dropout. And the  model has the same architecture as the  model but its weights are set using an  of the weights of  model. For a labeled or unlabeled image, we create two random augmented versions of the image. Then, the  model is used to predict  for first image. And, the  model is used to predict the  for the second augmented image. The  of these two  is used as a . For labeled images, we also calculate the . The final loss is a weighted sum of these two loss terms. A weight  is applied to decide how much the consistency loss contributes in the overall loss.This method was proposed by . It uses the concept of adversarial attack for consistency regularization.The key idea is to generate an adversarial transformation of an image that will change the model prediction. To do so, first, an image is taken and an adversarial variant of it is created such that the KL-divergence between the model output for the original image and the adversarial image is maximized.Then we proceed as previous methods. We take a labeled/unlabeled image as first view and take its adversarial example generated in previous step as the second view. Then, the same  is used to predict  for both images. The  of these two  is used as a . For labeled images, we also calculate the . The final loss is a weighted sum of these two loss terms. A weight  is applied to decide how much the consistency loss contributes in the overall loss.This method was proposed by  and works for both images and text. Here, we will understand the method in the context of images.The key idea is to create an augmented version of a unlabeled image using AutoAugment. Then, a same  is used to predict the label of both these images. The  of these two  is used as a . For labeled images, we only calculate the  and don’t calculate any . The final loss is a weighted sum of these two loss terms. A weight  is applied to decide how much the consistency loss contributes in the overall loss.This paradigm combines ideas from previous work such as self-training and consistency regularization along with additional components for performance improvement.This holistic method was proposed by .To understand this method, let’s take a walk through each of the steps.i. For the labeled image, we create an augmentation of it. For the unlabeled image, we create K augmentations and get the model  on all K-images. Then, the  are  and  is applied to get a final pseudo-label. This pseudo-label will be used for all the K-augmentations.ii. The batches of augmented labeled and unlabeled images are combined and the whole group is shuffled. Then, the first N images of this group are taken as \(W_L\), and the remaining M images are taken as \(W_U\).iii. Now, Mixup is applied between the augmented labeled batch and group \(W_L\). Similarly, mixup is applied between the M augmented unlabeled group and the \(W_U\) group. Thus, we get the final labeled and unlabeled group.iv. Now, for the labeled group, we take model predictions and compute  with the ground truth mixup labels. Similarly, for the unlabeled group, we compute model predictions and compute  with the mixup pseudo labels. A weighted sum is taken of these two terms with \(\lambda\) weighting the MSE loss.This method was proposed by  and combines pseudo-labeling and consistency regularization while vastly simplifying the overall method. It got state of the art results on a wide range of benchmarks.As seen, we train a supervised model on our labeled images with cross-entropy loss. For each unlabeled image,  and  are applied to get two images. The  is passed to our model and we get prediction over classes. The probability for the most confident class is compared to a . If it is above the , then we take that class as the ground label i.e. . Then, the  image is passed through our model to get a prediction over classes. This  is compared to ground truth  using cross-entropy loss. Both the losses are combined and the model is optimized.If you want to learn more about FixMatch, I have an  that goes over it in depth.Here is a high-level summary of the differences between all the above-mentioned methods.To evaluate the performance of these semi-supervised methods, the following datasets are commonly used. The authors simulate a low-data regime by using only a small portion(e.g. 40/250/4000/10000 examples) of the whole dataset as labeled and treating the remaining as the unlabeled set.Thus, we got an overview of how semi-supervised methods for Computer Vision have progressed over the years. This is a really important line of research that can have a direct impact on the industry.If you found this blog post useful, please consider citing it as:Semi-supervised learning methods for Computer Vision have been advancing quickly in the past few years. Current state-of-the-art methods are simplifying prior work in terms of architecture and loss function or introducing hybrid methods by blending different formulations.In this post, I will illustrate the key ideas of these recent methods for semi-supervised learning through diagrams.In this semi-supervised formulation, a model is trained on labeled data and used to predict pseudo-labels for the unlabeled data. The model is then trained on both ground truth labels and pseudo-labels simultaneously. proposed a very simple and efficient formulation called “Pseudo-label” in 2013.The idea is to train a  simultaneously on a batch of both labeled and unlabeled images. The  is trained on labeled images in usual supervised manner with a cross-entropy loss. The same model is used to get predictions for a batch of unlabeled images and the  is used as the . Then, cross-entropy loss is calculated by comparing  predictions and the pseudo-label for the unlabeled images .The total loss is a weighted sum of the labeled and unlabeled loss terms.To make sure the model has learned enough from the labeled data, the \(\alpha_t\) term is set to 0 during the initial 100 training steps. It is then gradually increased up to 600 training steps and then kept constant.
 proposed a semi-supervised method inspired by Knowledge Distillation called “Noisy Student” in 2019.The key idea is to train two separate models called  and . The  is first trained on the labeled images and then it is used to infer the pseudo-labels for the unlabeled images. These pseudo-labels can either be soft-label or converted to hard-label by . Then, the labeled and unlabeled images are combined together and a  is trained on this combined data. The images are augmented using RandAugment as a form of input noise. Also, model noise such as Dropout and Stochastic Depth are incorporated in the student model architecture.Once a  is trained, it becomes the new  and this process is repeated for three iterations.This paradigm uses the idea that  predictions on an unlabeled image should remain the same even after adding noise. We could use input noise such as Image Augmentation and Gaussian noise. Noise can also be incorporated in the architecture itself using Dropout.This model was proposed by  in a conference paper at ICLR 2017.The key idea is to create two random augmentations of an image for both labeled and unlabeled data. Then, a  is used to predict the label of both these images. The  of these two  is used as a . For labeled images, we also calculate the . The total loss is a weighted sum of these two loss terms. A weight  is applied to decide how much the consistency loss contributes in the overall loss.This method was also proposed by  in the same paper as the pi-model. It modifies the π-model by leveraging the  of predictions.The key idea is to use the  of past predictions as one view. To get another view, we augment the image as usual and a  is used to predict the label. The  of  and  is used as a . For labeled images, we also calculate the . The final loss is a weighted sum of these two loss terms. A weight  is applied to decide how much the consistency loss contributes in the overall loss.This method was proposed by . The general approach is similar to Temporal Ensembling but it uses Exponential Moving Average(EMA) of the model parameters instead of predictions.The key idea is to have two models called  and . The  model is a regular model with dropout. And the  model has the same architecture as the  model but its weights are set using an  of the weights of  model. For a labeled or unlabeled image, we create two random augmented versions of the image. Then, the  model is used to predict  for first image. And, the  model is used to predict the  for the second augmented image. The  of these two  is used as a . For labeled images, we also calculate the . The final loss is a weighted sum of these two loss terms. A weight  is applied to decide how much the consistency loss contributes in the overall loss.This method was proposed by . It uses the concept of adversarial attack for consistency regularization.The key idea is to generate an adversarial transformation of an image that will change the model prediction. To do so, first, an image is taken and an adversarial variant of it is created such that the KL-divergence between the model output for the original image and the adversarial image is maximized.Then we proceed as previous methods. We take a labeled/unlabeled image as first view and take its adversarial example generated in previous step as the second view. Then, the same  is used to predict  for both images. The  of these two  is used as a . For labeled images, we also calculate the . The final loss is a weighted sum of these two loss terms. A weight  is applied to decide how much the consistency loss contributes in the overall loss.This method was proposed by  and works for both images and text. Here, we will understand the method in the context of images.The key idea is to create an augmented version of a unlabeled image using AutoAugment. Then, a same  is used to predict the label of both these images. The  of these two  is used as a . For labeled images, we only calculate the  and don’t calculate any . The final loss is a weighted sum of these two loss terms. A weight  is applied to decide how much the consistency loss contributes in the overall loss.This paradigm combines ideas from previous work such as self-training and consistency regularization along with additional components for performance improvement.This holistic method was proposed by .To understand this method, let’s take a walk through each of the steps.i. For the labeled image, we create an augmentation of it. For the unlabeled image, we create K augmentations and get the model  on all K-images. Then, the  are  and  is applied to get a final pseudo-label. This pseudo-label will be used for all the K-augmentations.ii. The batches of augmented labeled and unlabeled images are combined and the whole group is shuffled. Then, the first N images of this group are taken as \(W_L\), and the remaining M images are taken as \(W_U\).iii. Now, Mixup is applied between the augmented labeled batch and group \(W_L\). Similarly, mixup is applied between the M augmented unlabeled group and the \(W_U\) group. Thus, we get the final labeled and unlabeled group.iv. Now, for the labeled group, we take model predictions and compute  with the ground truth mixup labels. Similarly, for the unlabeled group, we compute model predictions and compute  with the mixup pseudo labels. A weighted sum is taken of these two terms with \(\lambda\) weighting the MSE loss.This method was proposed by  and combines pseudo-labeling and consistency regularization while vastly simplifying the overall method. It got state of the art results on a wide range of benchmarks.As seen, we train a supervised model on our labeled images with cross-entropy loss. For each unlabeled image,  and  are applied to get two images. The  is passed to our model and we get prediction over classes. The probability for the most confident class is compared to a . If it is above the , then we take that class as the ground label i.e. . Then, the  image is passed through our model to get a prediction over classes. This  is compared to ground truth  using cross-entropy loss. Both the losses are combined and the model is optimized.If you want to learn more about FixMatch, I have an  that goes over it in depth.Here is a high-level summary of the differences between all the above-mentioned methods.To evaluate the performance of these semi-supervised methods, the following datasets are commonly used. The authors simulate a low-data regime by using only a small portion(e.g. 40/250/4000/10000 examples) of the whole dataset as labeled and treating the remaining as the unlabeled set.Thus, we got an overview of how semi-supervised methods for Computer Vision have progressed over the years. This is a really important line of research that can have a direct impact on the industry.If you found this blog post useful, please consider citing it as:Semi-supervised learning methods for Computer Vision have been advancing quickly in the past few years. Current state-of-the-art methods are simplifying prior work in terms of architecture and loss function or introducing hybrid methods by blending different formulations.In this post, I will illustrate the key ideas of these recent methods for semi-supervised learning through diagrams.In this semi-supervised formulation, a model is trained on labeled data and used to predict pseudo-labels for the unlabeled data. The model is then trained on both ground truth labels and pseudo-labels simultaneously. proposed a very simple and efficient formulation called “Pseudo-label” in 2013.The idea is to train a  simultaneously on a batch of both labeled and unlabeled images. The  is trained on labeled images in usual supervised manner with a cross-entropy loss. The same model is used to get predictions for a batch of unlabeled images and the  is used as the . Then, cross-entropy loss is calculated by comparing  predictions and the pseudo-label for the unlabeled images .The total loss is a weighted sum of the labeled and unlabeled loss terms.To make sure the model has learned enough from the labeled data, the \(\alpha_t\) term is set to 0 during the initial 100 training steps. It is then gradually increased up to 600 training steps and then kept constant.
 proposed a semi-supervised method inspired by Knowledge Distillation called “Noisy Student” in 2019.The key idea is to train two separate models called  and . The  is first trained on the labeled images and then it is used to infer the pseudo-labels for the unlabeled images. These pseudo-labels can either be soft-label or converted to hard-label by . Then, the labeled and unlabeled images are combined together and a  is trained on this combined data. The images are augmented using RandAugment as a form of input noise. Also, model noise such as Dropout and Stochastic Depth are incorporated in the student model architecture.Once a  is trained, it becomes the new  and this process is repeated for three iterations.This paradigm uses the idea that  predictions on an unlabeled image should remain the same even after adding noise. We could use input noise such as Image Augmentation and Gaussian noise. Noise can also be incorporated in the architecture itself using Dropout.This model was proposed by  in a conference paper at ICLR 2017.The key idea is to create two random augmentations of an image for both labeled and unlabeled data. Then, a  is used to predict the label of both these images. The  of these two  is used as a . For labeled images, we also calculate the . The total loss is a weighted sum of these two loss terms. A weight  is applied to decide how much the consistency loss contributes in the overall loss.This method was also proposed by  in the same paper as the pi-model. It modifies the π-model by leveraging the  of predictions.The key idea is to use the  of past predictions as one view. To get another view, we augment the image as usual and a  is used to predict the label. The  of  and  is used as a . For labeled images, we also calculate the . The final loss is a weighted sum of these two loss terms. A weight  is applied to decide how much the consistency loss contributes in the overall loss.This method was proposed by . The general approach is similar to Temporal Ensembling but it uses Exponential Moving Average(EMA) of the model parameters instead of predictions.The key idea is to have two models called  and . The  model is a regular model with dropout. And the  model has the same architecture as the  model but its weights are set using an  of the weights of  model. For a labeled or unlabeled image, we create two random augmented versions of the image. Then, the  model is used to predict  for first image. And, the  model is used to predict the  for the second augmented image. The  of these two  is used as a . For labeled images, we also calculate the . The final loss is a weighted sum of these two loss terms. A weight  is applied to decide how much the consistency loss contributes in the overall loss.This method was proposed by . It uses the concept of adversarial attack for consistency regularization.The key idea is to generate an adversarial transformation of an image that will change the model prediction. To do so, first, an image is taken and an adversarial variant of it is created such that the KL-divergence between the model output for the original image and the adversarial image is maximized.Then we proceed as previous methods. We take a labeled/unlabeled image as first view and take its adversarial example generated in previous step as the second view. Then, the same  is used to predict  for both images. The  of these two  is used as a . For labeled images, we also calculate the . The final loss is a weighted sum of these two loss terms. A weight  is applied to decide how much the consistency loss contributes in the overall loss.This method was proposed by  and works for both images and text. Here, we will understand the method in the context of images.The key idea is to create an augmented version of a unlabeled image using AutoAugment. Then, a same  is used to predict the label of both these images. The  of these two  is used as a . For labeled images, we only calculate the  and don’t calculate any . The final loss is a weighted sum of these two loss terms. A weight  is applied to decide how much the consistency loss contributes in the overall loss.This paradigm combines ideas from previous work such as self-training and consistency regularization along with additional components for performance improvement.This holistic method was proposed by .To understand this method, let’s take a walk through each of the steps.i. For the labeled image, we create an augmentation of it. For the unlabeled image, we create K augmentations and get the model  on all K-images. Then, the  are  and  is applied to get a final pseudo-label. This pseudo-label will be used for all the K-augmentations.ii. The batches of augmented labeled and unlabeled images are combined and the whole group is shuffled. Then, the first N images of this group are taken as \(W_L\), and the remaining M images are taken as \(W_U\).iii. Now, Mixup is applied between the augmented labeled batch and group \(W_L\). Similarly, mixup is applied between the M augmented unlabeled group and the \(W_U\) group. Thus, we get the final labeled and unlabeled group.iv. Now, for the labeled group, we take model predictions and compute  with the ground truth mixup labels. Similarly, for the unlabeled group, we compute model predictions and compute  with the mixup pseudo labels. A weighted sum is taken of these two terms with \(\lambda\) weighting the MSE loss.This method was proposed by  and combines pseudo-labeling and consistency regularization while vastly simplifying the overall method. It got state of the art results on a wide range of benchmarks.As seen, we train a supervised model on our labeled images with cross-entropy loss. For each unlabeled image,  and  are applied to get two images. The  is passed to our model and we get prediction over classes. The probability for the most confident class is compared to a . If it is above the , then we take that class as the ground label i.e. . Then, the  image is passed through our model to get a prediction over classes. This  is compared to ground truth  using cross-entropy loss. Both the losses are combined and the model is optimized.If you want to learn more about FixMatch, I have an  that goes over it in depth.Here is a high-level summary of the differences between all the above-mentioned methods.To evaluate the performance of these semi-supervised methods, the following datasets are commonly used. The authors simulate a low-data regime by using only a small portion(e.g. 40/250/4000/10000 examples) of the whole dataset as labeled and treating the remaining as the unlabeled set.Thus, we got an overview of how semi-supervised methods for Computer Vision have progressed over the years. This is a really important line of research that can have a direct impact on the industry.If you found this blog post useful, please consider citing it as:Semi-supervised learning methods for Computer Vision have been advancing quickly in the past few years. Current state-of-the-art methods are simplifying prior work in terms of architecture and loss function or introducing hybrid methods by blending different formulations.In this post, I will illustrate the key ideas of these recent methods for semi-supervised learning through diagrams.In this semi-supervised formulation, a model is trained on labeled data and used to predict pseudo-labels for the unlabeled data. The model is then trained on both ground truth labels and pseudo-labels simultaneously. proposed a very simple and efficient formulation called “Pseudo-label” in 2013.The idea is to train a  simultaneously on a batch of both labeled and unlabeled images. The  is trained on labeled images in usual supervised manner with a cross-entropy loss. The same model is used to get predictions for a batch of unlabeled images and the  is used as the . Then, cross-entropy loss is calculated by comparing  predictions and the pseudo-label for the unlabeled images .The total loss is a weighted sum of the labeled and unlabeled loss terms.To make sure the model has learned enough from the labeled data, the \(\alpha_t\) term is set to 0 during the initial 100 training steps. It is then gradually increased up to 600 training steps and then kept constant.
 proposed a semi-supervised method inspired by Knowledge Distillation called “Noisy Student” in 2019.The key idea is to train two separate models called  and . The  is first trained on the labeled images and then it is used to infer the pseudo-labels for the unlabeled images. These pseudo-labels can either be soft-label or converted to hard-label by . Then, the labeled and unlabeled images are combined together and a  is trained on this combined data. The images are augmented using RandAugment as a form of input noise. Also, model noise such as Dropout and Stochastic Depth are incorporated in the student model architecture.Once a  is trained, it becomes the new  and this process is repeated for three iterations.This paradigm uses the idea that  predictions on an unlabeled image should remain the same even after adding noise. We could use input noise such as Image Augmentation and Gaussian noise. Noise can also be incorporated in the architecture itself using Dropout.This model was proposed by  in a conference paper at ICLR 2017.The key idea is to create two random augmentations of an image for both labeled and unlabeled data. Then, a  is used to predict the label of both these images. The  of these two  is used as a . For labeled images, we also calculate the . The total loss is a weighted sum of these two loss terms. A weight  is applied to decide how much the consistency loss contributes in the overall loss.This method was also proposed by  in the same paper as the pi-model. It modifies the π-model by leveraging the  of predictions.The key idea is to use the  of past predictions as one view. To get another view, we augment the image as usual and a  is used to predict the label. The  of  and  is used as a . For labeled images, we also calculate the . The final loss is a weighted sum of these two loss terms. A weight  is applied to decide how much the consistency loss contributes in the overall loss.This method was proposed by . The general approach is similar to Temporal Ensembling but it uses Exponential Moving Average(EMA) of the model parameters instead of predictions.The key idea is to have two models called  and . The  model is a regular model with dropout. And the  model has the same architecture as the  model but its weights are set using an  of the weights of  model. For a labeled or unlabeled image, we create two random augmented versions of the image. Then, the  model is used to predict  for first image. And, the  model is used to predict the  for the second augmented image. The  of these two  is used as a . For labeled images, we also calculate the . The final loss is a weighted sum of these two loss terms. A weight  is applied to decide how much the consistency loss contributes in the overall loss.This method was proposed by . It uses the concept of adversarial attack for consistency regularization.The key idea is to generate an adversarial transformation of an image that will change the model prediction. To do so, first, an image is taken and an adversarial variant of it is created such that the KL-divergence between the model output for the original image and the adversarial image is maximized.Then we proceed as previous methods. We take a labeled/unlabeled image as first view and take its adversarial example generated in previous step as the second view. Then, the same  is used to predict  for both images. The  of these two  is used as a . For labeled images, we also calculate the . The final loss is a weighted sum of these two loss terms. A weight  is applied to decide how much the consistency loss contributes in the overall loss.This method was proposed by  and works for both images and text. Here, we will understand the method in the context of images.The key idea is to create an augmented version of a unlabeled image using AutoAugment. Then, a same  is used to predict the label of both these images. The  of these two  is used as a . For labeled images, we only calculate the  and don’t calculate any . The final loss is a weighted sum of these two loss terms. A weight  is applied to decide how much the consistency loss contributes in the overall loss.This paradigm combines ideas from previous work such as self-training and consistency regularization along with additional components for performance improvement.This holistic method was proposed by .To understand this method, let’s take a walk through each of the steps.i. For the labeled image, we create an augmentation of it. For the unlabeled image, we create K augmentations and get the model  on all K-images. Then, the  are  and  is applied to get a final pseudo-label. This pseudo-label will be used for all the K-augmentations.ii. The batches of augmented labeled and unlabeled images are combined and the whole group is shuffled. Then, the first N images of this group are taken as \(W_L\), and the remaining M images are taken as \(W_U\).iii. Now, Mixup is applied between the augmented labeled batch and group \(W_L\). Similarly, mixup is applied between the M augmented unlabeled group and the \(W_U\) group. Thus, we get the final labeled and unlabeled group.iv. Now, for the labeled group, we take model predictions and compute  with the ground truth mixup labels. Similarly, for the unlabeled group, we compute model predictions and compute  with the mixup pseudo labels. A weighted sum is taken of these two terms with \(\lambda\) weighting the MSE loss.This method was proposed by  and combines pseudo-labeling and consistency regularization while vastly simplifying the overall method. It got state of the art results on a wide range of benchmarks.As seen, we train a supervised model on our labeled images with cross-entropy loss. For each unlabeled image,  and  are applied to get two images. The  is passed to our model and we get prediction over classes. The probability for the most confident class is compared to a . If it is above the , then we take that class as the ground label i.e. . Then, the  image is passed through our model to get a prediction over classes. This  is compared to ground truth  using cross-entropy loss. Both the losses are combined and the model is optimized.If you want to learn more about FixMatch, I have an  that goes over it in depth.Here is a high-level summary of the differences between all the above-mentioned methods.To evaluate the performance of these semi-supervised methods, the following datasets are commonly used. The authors simulate a low-data regime by using only a small portion(e.g. 40/250/4000/10000 examples) of the whole dataset as labeled and treating the remaining as the unlabeled set.Thus, we got an overview of how semi-supervised methods for Computer Vision have progressed over the years. This is a really important line of research that can have a direct impact on the industry.If you found this blog post useful, please consider citing it as:Semi-supervised learning methods for Computer Vision have been advancing quickly in the past few years. Current state-of-the-art methods are simplifying prior work in terms of architecture and loss function or introducing hybrid methods by blending different formulations.In this post, I will illustrate the key ideas of these recent methods for semi-supervised learning through diagrams.In this semi-supervised formulation, a model is trained on labeled data and used to predict pseudo-labels for the unlabeled data. The model is then trained on both ground truth labels and pseudo-labels simultaneously. proposed a very simple and efficient formulation called “Pseudo-label” in 2013.The idea is to train a  simultaneously on a batch of both labeled and unlabeled images. The  is trained on labeled images in usual supervised manner with a cross-entropy loss. The same model is used to get predictions for a batch of unlabeled images and the  is used as the . Then, cross-entropy loss is calculated by comparing  predictions and the pseudo-label for the unlabeled images .The total loss is a weighted sum of the labeled and unlabeled loss terms.To make sure the model has learned enough from the labeled data, the \(\alpha_t\) term is set to 0 during the initial 100 training steps. It is then gradually increased up to 600 training steps and then kept constant.
 proposed a semi-supervised method inspired by Knowledge Distillation called “Noisy Student” in 2019.The key idea is to train two separate models called  and . The  is first trained on the labeled images and then it is used to infer the pseudo-labels for the unlabeled images. These pseudo-labels can either be soft-label or converted to hard-label by . Then, the labeled and unlabeled images are combined together and a  is trained on this combined data. The images are augmented using RandAugment as a form of input noise. Also, model noise such as Dropout and Stochastic Depth are incorporated in the student model architecture.Once a  is trained, it becomes the new  and this process is repeated for three iterations.This paradigm uses the idea that  predictions on an unlabeled image should remain the same even after adding noise. We could use input noise such as Image Augmentation and Gaussian noise. Noise can also be incorporated in the architecture itself using Dropout.This model was proposed by  in a conference paper at ICLR 2017.The key idea is to create two random augmentations of an image for both labeled and unlabeled data. Then, a  is used to predict the label of both these images. The  of these two  is used as a . For labeled images, we also calculate the . The total loss is a weighted sum of these two loss terms. A weight  is applied to decide how much the consistency loss contributes in the overall loss.This method was also proposed by  in the same paper as the pi-model. It modifies the π-model by leveraging the  of predictions.The key idea is to use the  of past predictions as one view. To get another view, we augment the image as usual and a  is used to predict the label. The  of  and  is used as a . For labeled images, we also calculate the . The final loss is a weighted sum of these two loss terms. A weight  is applied to decide how much the consistency loss contributes in the overall loss.This method was proposed by . The general approach is similar to Temporal Ensembling but it uses Exponential Moving Average(EMA) of the model parameters instead of predictions.The key idea is to have two models called  and . The  model is a regular model with dropout. And the  model has the same architecture as the  model but its weights are set using an  of the weights of  model. For a labeled or unlabeled image, we create two random augmented versions of the image. Then, the  model is used to predict  for first image. And, the  model is used to predict the  for the second augmented image. The  of these two  is used as a . For labeled images, we also calculate the . The final loss is a weighted sum of these two loss terms. A weight  is applied to decide how much the consistency loss contributes in the overall loss.This method was proposed by . It uses the concept of adversarial attack for consistency regularization.The key idea is to generate an adversarial transformation of an image that will change the model prediction. To do so, first, an image is taken and an adversarial variant of it is created such that the KL-divergence between the model output for the original image and the adversarial image is maximized.Then we proceed as previous methods. We take a labeled/unlabeled image as first view and take its adversarial example generated in previous step as the second view. Then, the same  is used to predict  for both images. The  of these two  is used as a . For labeled images, we also calculate the . The final loss is a weighted sum of these two loss terms. A weight  is applied to decide how much the consistency loss contributes in the overall loss.This method was proposed by  and works for both images and text. Here, we will understand the method in the context of images.The key idea is to create an augmented version of a unlabeled image using AutoAugment. Then, a same  is used to predict the label of both these images. The  of these two  is used as a . For labeled images, we only calculate the  and don’t calculate any . The final loss is a weighted sum of these two loss terms. A weight  is applied to decide how much the consistency loss contributes in the overall loss.This paradigm combines ideas from previous work such as self-training and consistency regularization along with additional components for performance improvement.This holistic method was proposed by .To understand this method, let’s take a walk through each of the steps.i. For the labeled image, we create an augmentation of it. For the unlabeled image, we create K augmentations and get the model  on all K-images. Then, the  are  and  is applied to get a final pseudo-label. This pseudo-label will be used for all the K-augmentations.ii. The batches of augmented labeled and unlabeled images are combined and the whole group is shuffled. Then, the first N images of this group are taken as \(W_L\), and the remaining M images are taken as \(W_U\).iii. Now, Mixup is applied between the augmented labeled batch and group \(W_L\). Similarly, mixup is applied between the M augmented unlabeled group and the \(W_U\) group. Thus, we get the final labeled and unlabeled group.iv. Now, for the labeled group, we take model predictions and compute  with the ground truth mixup labels. Similarly, for the unlabeled group, we compute model predictions and compute  with the mixup pseudo labels. A weighted sum is taken of these two terms with \(\lambda\) weighting the MSE loss.This method was proposed by  and combines pseudo-labeling and consistency regularization while vastly simplifying the overall method. It got state of the art results on a wide range of benchmarks.As seen, we train a supervised model on our labeled images with cross-entropy loss. For each unlabeled image,  and  are applied to get two images. The  is passed to our model and we get prediction over classes. The probability for the most confident class is compared to a . If it is above the , then we take that class as the ground label i.e. . Then, the  image is passed through our model to get a prediction over classes. This  is compared to ground truth  using cross-entropy loss. Both the losses are combined and the model is optimized.If you want to learn more about FixMatch, I have an  that goes over it in depth.Here is a high-level summary of the differences between all the above-mentioned methods.To evaluate the performance of these semi-supervised methods, the following datasets are commonly used. The authors simulate a low-data regime by using only a small portion(e.g. 40/250/4000/10000 examples) of the whole dataset as labeled and treating the remaining as the unlabeled set.Thus, we got an overview of how semi-supervised methods for Computer Vision have progressed over the years. This is a really important line of research that can have a direct impact on the industry.If you found this blog post useful, please consider citing it as:Semi-supervised learning methods for Computer Vision have been advancing quickly in the past few years. Current state-of-the-art methods are simplifying prior work in terms of architecture and loss function or introducing hybrid methods by blending different formulations.In this post, I will illustrate the key ideas of these recent methods for semi-supervised learning through diagrams.In this semi-supervised formulation, a model is trained on labeled data and used to predict pseudo-labels for the unlabeled data. The model is then trained on both ground truth labels and pseudo-labels simultaneously. proposed a very simple and efficient formulation called “Pseudo-label” in 2013.The idea is to train a  simultaneously on a batch of both labeled and unlabeled images. The  is trained on labeled images in usual supervised manner with a cross-entropy loss. The same model is used to get predictions for a batch of unlabeled images and the  is used as the . Then, cross-entropy loss is calculated by comparing  predictions and the pseudo-label for the unlabeled images .The total loss is a weighted sum of the labeled and unlabeled loss terms.To make sure the model has learned enough from the labeled data, the \(\alpha_t\) term is set to 0 during the initial 100 training steps. It is then gradually increased up to 600 training steps and then kept constant.
 proposed a semi-supervised method inspired by Knowledge Distillation called “Noisy Student” in 2019.The key idea is to train two separate models called  and . The  is first trained on the labeled images and then it is used to infer the pseudo-labels for the unlabeled images. These pseudo-labels can either be soft-label or converted to hard-label by . Then, the labeled and unlabeled images are combined together and a  is trained on this combined data. The images are augmented using RandAugment as a form of input noise. Also, model noise such as Dropout and Stochastic Depth are incorporated in the student model architecture.Once a  is trained, it becomes the new  and this process is repeated for three iterations.This paradigm uses the idea that  predictions on an unlabeled image should remain the same even after adding noise. We could use input noise such as Image Augmentation and Gaussian noise. Noise can also be incorporated in the architecture itself using Dropout.This model was proposed by  in a conference paper at ICLR 2017.The key idea is to create two random augmentations of an image for both labeled and unlabeled data. Then, a  is used to predict the label of both these images. The  of these two  is used as a . For labeled images, we also calculate the . The total loss is a weighted sum of these two loss terms. A weight  is applied to decide how much the consistency loss contributes in the overall loss.This method was also proposed by  in the same paper as the pi-model. It modifies the π-model by leveraging the  of predictions.The key idea is to use the  of past predictions as one view. To get another view, we augment the image as usual and a  is used to predict the label. The  of  and  is used as a . For labeled images, we also calculate the . The final loss is a weighted sum of these two loss terms. A weight  is applied to decide how much the consistency loss contributes in the overall loss.This method was proposed by . The general approach is similar to Temporal Ensembling but it uses Exponential Moving Average(EMA) of the model parameters instead of predictions.The key idea is to have two models called  and . The  model is a regular model with dropout. And the  model has the same architecture as the  model but its weights are set using an  of the weights of  model. For a labeled or unlabeled image, we create two random augmented versions of the image. Then, the  model is used to predict  for first image. And, the  model is used to predict the  for the second augmented image. The  of these two  is used as a . For labeled images, we also calculate the . The final loss is a weighted sum of these two loss terms. A weight  is applied to decide how much the consistency loss contributes in the overall loss.This method was proposed by . It uses the concept of adversarial attack for consistency regularization.The key idea is to generate an adversarial transformation of an image that will change the model prediction. To do so, first, an image is taken and an adversarial variant of it is created such that the KL-divergence between the model output for the original image and the adversarial image is maximized.Then we proceed as previous methods. We take a labeled/unlabeled image as first view and take its adversarial example generated in previous step as the second view. Then, the same  is used to predict  for both images. The  of these two  is used as a . For labeled images, we also calculate the . The final loss is a weighted sum of these two loss terms. A weight  is applied to decide how much the consistency loss contributes in the overall loss.This method was proposed by  and works for both images and text. Here, we will understand the method in the context of images.The key idea is to create an augmented version of a unlabeled image using AutoAugment. Then, a same  is used to predict the label of both these images. The  of these two  is used as a . For labeled images, we only calculate the  and don’t calculate any . The final loss is a weighted sum of these two loss terms. A weight  is applied to decide how much the consistency loss contributes in the overall loss.This paradigm combines ideas from previous work such as self-training and consistency regularization along with additional components for performance improvement.This holistic method was proposed by .To understand this method, let’s take a walk through each of the steps.i. For the labeled image, we create an augmentation of it. For the unlabeled image, we create K augmentations and get the model  on all K-images. Then, the  are  and  is applied to get a final pseudo-label. This pseudo-label will be used for all the K-augmentations.ii. The batches of augmented labeled and unlabeled images are combined and the whole group is shuffled. Then, the first N images of this group are taken as \(W_L\), and the remaining M images are taken as \(W_U\).iii. Now, Mixup is applied between the augmented labeled batch and group \(W_L\). Similarly, mixup is applied between the M augmented unlabeled group and the \(W_U\) group. Thus, we get the final labeled and unlabeled group.iv. Now, for the labeled group, we take model predictions and compute  with the ground truth mixup labels. Similarly, for the unlabeled group, we compute model predictions and compute  with the mixup pseudo labels. A weighted sum is taken of these two terms with \(\lambda\) weighting the MSE loss.This method was proposed by  and combines pseudo-labeling and consistency regularization while vastly simplifying the overall method. It got state of the art results on a wide range of benchmarks.As seen, we train a supervised model on our labeled images with cross-entropy loss. For each unlabeled image,  and  are applied to get two images. The  is passed to our model and we get prediction over classes. The probability for the most confident class is compared to a . If it is above the , then we take that class as the ground label i.e. . Then, the  image is passed through our model to get a prediction over classes. This  is compared to ground truth  using cross-entropy loss. Both the losses are combined and the model is optimized.If you want to learn more about FixMatch, I have an  that goes over it in depth.Here is a high-level summary of the differences between all the above-mentioned methods.To evaluate the performance of these semi-supervised methods, the following datasets are commonly used. The authors simulate a low-data regime by using only a small portion(e.g. 40/250/4000/10000 examples) of the whole dataset as labeled and treating the remaining as the unlabeled set.Thus, we got an overview of how semi-supervised methods for Computer Vision have progressed over the years. This is a really important line of research that can have a direct impact on the industry.If you found this blog post useful, please consider citing it as:Semi-supervised learning methods for Computer Vision have been advancing quickly in the past few years. Current state-of-the-art methods are simplifying prior work in terms of architecture and loss function or introducing hybrid methods by blending different formulations.In this post, I will illustrate the key ideas of these recent methods for semi-supervised learning through diagrams.In this semi-supervised formulation, a model is trained on labeled data and used to predict pseudo-labels for the unlabeled data. The model is then trained on both ground truth labels and pseudo-labels simultaneously. proposed a very simple and efficient formulation called “Pseudo-label” in 2013.The idea is to train a  simultaneously on a batch of both labeled and unlabeled images. The  is trained on labeled images in usual supervised manner with a cross-entropy loss. The same model is used to get predictions for a batch of unlabeled images and the  is used as the . Then, cross-entropy loss is calculated by comparing  predictions and the pseudo-label for the unlabeled images .The total loss is a weighted sum of the labeled and unlabeled loss terms.To make sure the model has learned enough from the labeled data, the \(\alpha_t\) term is set to 0 during the initial 100 training steps. It is then gradually increased up to 600 training steps and then kept constant.
 proposed a semi-supervised method inspired by Knowledge Distillation called “Noisy Student” in 2019.The key idea is to train two separate models called  and . The  is first trained on the labeled images and then it is used to infer the pseudo-labels for the unlabeled images. These pseudo-labels can either be soft-label or converted to hard-label by . Then, the labeled and unlabeled images are combined together and a  is trained on this combined data. The images are augmented using RandAugment as a form of input noise. Also, model noise such as Dropout and Stochastic Depth are incorporated in the student model architecture.Once a  is trained, it becomes the new  and this process is repeated for three iterations.This paradigm uses the idea that  predictions on an unlabeled image should remain the same even after adding noise. We could use input noise such as Image Augmentation and Gaussian noise. Noise can also be incorporated in the architecture itself using Dropout.This model was proposed by  in a conference paper at ICLR 2017.The key idea is to create two random augmentations of an image for both labeled and unlabeled data. Then, a  is used to predict the label of both these images. The  of these two  is used as a . For labeled images, we also calculate the . The total loss is a weighted sum of these two loss terms. A weight  is applied to decide how much the consistency loss contributes in the overall loss.This method was also proposed by  in the same paper as the pi-model. It modifies the π-model by leveraging the  of predictions.The key idea is to use the  of past predictions as one view. To get another view, we augment the image as usual and a  is used to predict the label. The  of  and  is used as a . For labeled images, we also calculate the . The final loss is a weighted sum of these two loss terms. A weight  is applied to decide how much the consistency loss contributes in the overall loss.This method was proposed by . The general approach is similar to Temporal Ensembling but it uses Exponential Moving Average(EMA) of the model parameters instead of predictions.The key idea is to have two models called  and . The  model is a regular model with dropout. And the  model has the same architecture as the  model but its weights are set using an  of the weights of  model. For a labeled or unlabeled image, we create two random augmented versions of the image. Then, the  model is used to predict  for first image. And, the  model is used to predict the  for the second augmented image. The  of these two  is used as a . For labeled images, we also calculate the . The final loss is a weighted sum of these two loss terms. A weight  is applied to decide how much the consistency loss contributes in the overall loss.This method was proposed by . It uses the concept of adversarial attack for consistency regularization.The key idea is to generate an adversarial transformation of an image that will change the model prediction. To do so, first, an image is taken and an adversarial variant of it is created such that the KL-divergence between the model output for the original image and the adversarial image is maximized.Then we proceed as previous methods. We take a labeled/unlabeled image as first view and take its adversarial example generated in previous step as the second view. Then, the same  is used to predict  for both images. The  of these two  is used as a . For labeled images, we also calculate the . The final loss is a weighted sum of these two loss terms. A weight  is applied to decide how much the consistency loss contributes in the overall loss.This method was proposed by  and works for both images and text. Here, we will understand the method in the context of images.The key idea is to create an augmented version of a unlabeled image using AutoAugment. Then, a same  is used to predict the label of both these images. The  of these two  is used as a . For labeled images, we only calculate the  and don’t calculate any . The final loss is a weighted sum of these two loss terms. A weight  is applied to decide how much the consistency loss contributes in the overall loss.This paradigm combines ideas from previous work such as self-training and consistency regularization along with additional components for performance improvement.This holistic method was proposed by .To understand this method, let’s take a walk through each of the steps.i. For the labeled image, we create an augmentation of it. For the unlabeled image, we create K augmentations and get the model  on all K-images. Then, the  are  and  is applied to get a final pseudo-label. This pseudo-label will be used for all the K-augmentations.ii. The batches of augmented labeled and unlabeled images are combined and the whole group is shuffled. Then, the first N images of this group are taken as \(W_L\), and the remaining M images are taken as \(W_U\).iii. Now, Mixup is applied between the augmented labeled batch and group \(W_L\). Similarly, mixup is applied between the M augmented unlabeled group and the \(W_U\) group. Thus, we get the final labeled and unlabeled group.iv. Now, for the labeled group, we take model predictions and compute  with the ground truth mixup labels. Similarly, for the unlabeled group, we compute model predictions and compute  with the mixup pseudo labels. A weighted sum is taken of these two terms with \(\lambda\) weighting the MSE loss.This method was proposed by  and combines pseudo-labeling and consistency regularization while vastly simplifying the overall method. It got state of the art results on a wide range of benchmarks.As seen, we train a supervised model on our labeled images with cross-entropy loss. For each unlabeled image,  and  are applied to get two images. The  is passed to our model and we get prediction over classes. The probability for the most confident class is compared to a . If it is above the , then we take that class as the ground label i.e. . Then, the  image is passed through our model to get a prediction over classes. This  is compared to ground truth  using cross-entropy loss. Both the losses are combined and the model is optimized.If you want to learn more about FixMatch, I have an  that goes over it in depth.Here is a high-level summary of the differences between all the above-mentioned methods.To evaluate the performance of these semi-supervised methods, the following datasets are commonly used. The authors simulate a low-data regime by using only a small portion(e.g. 40/250/4000/10000 examples) of the whole dataset as labeled and treating the remaining as the unlabeled set.Thus, we got an overview of how semi-supervised methods for Computer Vision have progressed over the years. This is a really important line of research that can have a direct impact on the industry.If you found this blog post useful, please consider citing it as:Semi-supervised learning methods for Computer Vision have been advancing quickly in the past few years. Current state-of-the-art methods are simplifying prior work in terms of architecture and loss function or introducing hybrid methods by blending different formulations.In this post, I will illustrate the key ideas of these recent methods for semi-supervised learning through diagrams.In this semi-supervised formulation, a model is trained on labeled data and used to predict pseudo-labels for the unlabeled data. The model is then trained on both ground truth labels and pseudo-labels simultaneously. proposed a very simple and efficient formulation called “Pseudo-label” in 2013.The idea is to train a  simultaneously on a batch of both labeled and unlabeled images. The  is trained on labeled images in usual supervised manner with a cross-entropy loss. The same model is used to get predictions for a batch of unlabeled images and the  is used as the . Then, cross-entropy loss is calculated by comparing  predictions and the pseudo-label for the unlabeled images .The total loss is a weighted sum of the labeled and unlabeled loss terms.To make sure the model has learned enough from the labeled data, the \(\alpha_t\) term is set to 0 during the initial 100 training steps. It is then gradually increased up to 600 training steps and then kept constant.
 proposed a semi-supervised method inspired by Knowledge Distillation called “Noisy Student” in 2019.The key idea is to train two separate models called  and . The  is first trained on the labeled images and then it is used to infer the pseudo-labels for the unlabeled images. These pseudo-labels can either be soft-label or converted to hard-label by . Then, the labeled and unlabeled images are combined together and a  is trained on this combined data. The images are augmented using RandAugment as a form of input noise. Also, model noise such as Dropout and Stochastic Depth are incorporated in the student model architecture.Once a  is trained, it becomes the new  and this process is repeated for three iterations.This paradigm uses the idea that  predictions on an unlabeled image should remain the same even after adding noise. We could use input noise such as Image Augmentation and Gaussian noise. Noise can also be incorporated in the architecture itself using Dropout.This model was proposed by  in a conference paper at ICLR 2017.The key idea is to create two random augmentations of an image for both labeled and unlabeled data. Then, a  is used to predict the label of both these images. The  of these two  is used as a . For labeled images, we also calculate the . The total loss is a weighted sum of these two loss terms. A weight  is applied to decide how much the consistency loss contributes in the overall loss.This method was also proposed by  in the same paper as the pi-model. It modifies the π-model by leveraging the  of predictions.The key idea is to use the  of past predictions as one view. To get another view, we augment the image as usual and a  is used to predict the label. The  of  and  is used as a . For labeled images, we also calculate the . The final loss is a weighted sum of these two loss terms. A weight  is applied to decide how much the consistency loss contributes in the overall loss.This method was proposed by . The general approach is similar to Temporal Ensembling but it uses Exponential Moving Average(EMA) of the model parameters instead of predictions.The key idea is to have two models called  and . The  model is a regular model with dropout. And the  model has the same architecture as the  model but its weights are set using an  of the weights of  model. For a labeled or unlabeled image, we create two random augmented versions of the image. Then, the  model is used to predict  for first image. And, the  model is used to predict the  for the second augmented image. The  of these two  is used as a . For labeled images, we also calculate the . The final loss is a weighted sum of these two loss terms. A weight  is applied to decide how much the consistency loss contributes in the overall loss.This method was proposed by . It uses the concept of adversarial attack for consistency regularization.The key idea is to generate an adversarial transformation of an image that will change the model prediction. To do so, first, an image is taken and an adversarial variant of it is created such that the KL-divergence between the model output for the original image and the adversarial image is maximized.Then we proceed as previous methods. We take a labeled/unlabeled image as first view and take its adversarial example generated in previous step as the second view. Then, the same  is used to predict  for both images. The  of these two  is used as a . For labeled images, we also calculate the . The final loss is a weighted sum of these two loss terms. A weight  is applied to decide how much the consistency loss contributes in the overall loss.This method was proposed by  and works for both images and text. Here, we will understand the method in the context of images.The key idea is to create an augmented version of a unlabeled image using AutoAugment. Then, a same  is used to predict the label of both these images. The  of these two  is used as a . For labeled images, we only calculate the  and don’t calculate any . The final loss is a weighted sum of these two loss terms. A weight  is applied to decide how much the consistency loss contributes in the overall loss.This paradigm combines ideas from previous work such as self-training and consistency regularization along with additional components for performance improvement.This holistic method was proposed by .To understand this method, let’s take a walk through each of the steps.i. For the labeled image, we create an augmentation of it. For the unlabeled image, we create K augmentations and get the model  on all K-images. Then, the  are  and  is applied to get a final pseudo-label. This pseudo-label will be used for all the K-augmentations.ii. The batches of augmented labeled and unlabeled images are combined and the whole group is shuffled. Then, the first N images of this group are taken as \(W_L\), and the remaining M images are taken as \(W_U\).iii. Now, Mixup is applied between the augmented labeled batch and group \(W_L\). Similarly, mixup is applied between the M augmented unlabeled group and the \(W_U\) group. Thus, we get the final labeled and unlabeled group.iv. Now, for the labeled group, we take model predictions and compute  with the ground truth mixup labels. Similarly, for the unlabeled group, we compute model predictions and compute  with the mixup pseudo labels. A weighted sum is taken of these two terms with \(\lambda\) weighting the MSE loss.This method was proposed by  and combines pseudo-labeling and consistency regularization while vastly simplifying the overall method. It got state of the art results on a wide range of benchmarks.As seen, we train a supervised model on our labeled images with cross-entropy loss. For each unlabeled image,  and  are applied to get two images. The  is passed to our model and we get prediction over classes. The probability for the most confident class is compared to a . If it is above the , then we take that class as the ground label i.e. . Then, the  image is passed through our model to get a prediction over classes. This  is compared to ground truth  using cross-entropy loss. Both the losses are combined and the model is optimized.If you want to learn more about FixMatch, I have an  that goes over it in depth.Here is a high-level summary of the differences between all the above-mentioned methods.To evaluate the performance of these semi-supervised methods, the following datasets are commonly used. The authors simulate a low-data regime by using only a small portion(e.g. 40/250/4000/10000 examples) of the whole dataset as labeled and treating the remaining as the unlabeled set.Thus, we got an overview of how semi-supervised methods for Computer Vision have progressed over the years. This is a really important line of research that can have a direct impact on the industry.If you found this blog post useful, please consider citing it as:Semi-supervised learning methods for Computer Vision have been advancing quickly in the past few years. Current state-of-the-art methods are simplifying prior work in terms of architecture and loss function or introducing hybrid methods by blending different formulations.In this post, I will illustrate the key ideas of these recent methods for semi-supervised learning through diagrams.In this semi-supervised formulation, a model is trained on labeled data and used to predict pseudo-labels for the unlabeled data. The model is then trained on both ground truth labels and pseudo-labels simultaneously. proposed a very simple and efficient formulation called “Pseudo-label” in 2013.The idea is to train a  simultaneously on a batch of both labeled and unlabeled images. The  is trained on labeled images in usual supervised manner with a cross-entropy loss. The same model is used to get predictions for a batch of unlabeled images and the  is used as the . Then, cross-entropy loss is calculated by comparing  predictions and the pseudo-label for the unlabeled images .The total loss is a weighted sum of the labeled and unlabeled loss terms.To make sure the model has learned enough from the labeled data, the \(\alpha_t\) term is set to 0 during the initial 100 training steps. It is then gradually increased up to 600 training steps and then kept constant.
 proposed a semi-supervised method inspired by Knowledge Distillation called “Noisy Student” in 2019.The key idea is to train two separate models called  and . The  is first trained on the labeled images and then it is used to infer the pseudo-labels for the unlabeled images. These pseudo-labels can either be soft-label or converted to hard-label by . Then, the labeled and unlabeled images are combined together and a  is trained on this combined data. The images are augmented using RandAugment as a form of input noise. Also, model noise such as Dropout and Stochastic Depth are incorporated in the student model architecture.Once a  is trained, it becomes the new  and this process is repeated for three iterations.This paradigm uses the idea that  predictions on an unlabeled image should remain the same even after adding noise. We could use input noise such as Image Augmentation and Gaussian noise. Noise can also be incorporated in the architecture itself using Dropout.This model was proposed by  in a conference paper at ICLR 2017.The key idea is to create two random augmentations of an image for both labeled and unlabeled data. Then, a  is used to predict the label of both these images. The  of these two  is used as a . For labeled images, we also calculate the . The total loss is a weighted sum of these two loss terms. A weight  is applied to decide how much the consistency loss contributes in the overall loss.This method was also proposed by  in the same paper as the pi-model. It modifies the π-model by leveraging the  of predictions.The key idea is to use the  of past predictions as one view. To get another view, we augment the image as usual and a  is used to predict the label. The  of  and  is used as a . For labeled images, we also calculate the . The final loss is a weighted sum of these two loss terms. A weight  is applied to decide how much the consistency loss contributes in the overall loss.This method was proposed by . The general approach is similar to Temporal Ensembling but it uses Exponential Moving Average(EMA) of the model parameters instead of predictions.The key idea is to have two models called  and . The  model is a regular model with dropout. And the  model has the same architecture as the  model but its weights are set using an  of the weights of  model. For a labeled or unlabeled image, we create two random augmented versions of the image. Then, the  model is used to predict  for first image. And, the  model is used to predict the  for the second augmented image. The  of these two  is used as a . For labeled images, we also calculate the . The final loss is a weighted sum of these two loss terms. A weight  is applied to decide how much the consistency loss contributes in the overall loss.This method was proposed by . It uses the concept of adversarial attack for consistency regularization.The key idea is to generate an adversarial transformation of an image that will change the model prediction. To do so, first, an image is taken and an adversarial variant of it is created such that the KL-divergence between the model output for the original image and the adversarial image is maximized.Then we proceed as previous methods. We take a labeled/unlabeled image as first view and take its adversarial example generated in previous step as the second view. Then, the same  is used to predict  for both images. The  of these two  is used as a . For labeled images, we also calculate the . The final loss is a weighted sum of these two loss terms. A weight  is applied to decide how much the consistency loss contributes in the overall loss.This method was proposed by  and works for both images and text. Here, we will understand the method in the context of images.The key idea is to create an augmented version of a unlabeled image using AutoAugment. Then, a same  is used to predict the label of both these images. The  of these two  is used as a . For labeled images, we only calculate the  and don’t calculate any . The final loss is a weighted sum of these two loss terms. A weight  is applied to decide how much the consistency loss contributes in the overall loss.This paradigm combines ideas from previous work such as self-training and consistency regularization along with additional components for performance improvement.This holistic method was proposed by .To understand this method, let’s take a walk through each of the steps.i. For the labeled image, we create an augmentation of it. For the unlabeled image, we create K augmentations and get the model  on all K-images. Then, the  are  and  is applied to get a final pseudo-label. This pseudo-label will be used for all the K-augmentations.ii. The batches of augmented labeled and unlabeled images are combined and the whole group is shuffled. Then, the first N images of this group are taken as \(W_L\), and the remaining M images are taken as \(W_U\).iii. Now, Mixup is applied between the augmented labeled batch and group \(W_L\). Similarly, mixup is applied between the M augmented unlabeled group and the \(W_U\) group. Thus, we get the final labeled and unlabeled group.iv. Now, for the labeled group, we take model predictions and compute  with the ground truth mixup labels. Similarly, for the unlabeled group, we compute model predictions and compute  with the mixup pseudo labels. A weighted sum is taken of these two terms with \(\lambda\) weighting the MSE loss.This method was proposed by  and combines pseudo-labeling and consistency regularization while vastly simplifying the overall method. It got state of the art results on a wide range of benchmarks.As seen, we train a supervised model on our labeled images with cross-entropy loss. For each unlabeled image,  and  are applied to get two images. The  is passed to our model and we get prediction over classes. The probability for the most confident class is compared to a . If it is above the , then we take that class as the ground label i.e. . Then, the  image is passed through our model to get a prediction over classes. This  is compared to ground truth  using cross-entropy loss. Both the losses are combined and the model is optimized.If you want to learn more about FixMatch, I have an  that goes over it in depth.Here is a high-level summary of the differences between all the above-mentioned methods.To evaluate the performance of these semi-supervised methods, the following datasets are commonly used. The authors simulate a low-data regime by using only a small portion(e.g. 40/250/4000/10000 examples) of the whole dataset as labeled and treating the remaining as the unlabeled set.Thus, we got an overview of how semi-supervised methods for Computer Vision have progressed over the years. This is a really important line of research that can have a direct impact on the industry.If you found this blog post useful, please consider citing it as:Semi-supervised learning methods for Computer Vision have been advancing quickly in the past few years. Current state-of-the-art methods are simplifying prior work in terms of architecture and loss function or introducing hybrid methods by blending different formulations.In this post, I will illustrate the key ideas of these recent methods for semi-supervised learning through diagrams.In this semi-supervised formulation, a model is trained on labeled data and used to predict pseudo-labels for the unlabeled data. The model is then trained on both ground truth labels and pseudo-labels simultaneously. proposed a very simple and efficient formulation called “Pseudo-label” in 2013.The idea is to train a  simultaneously on a batch of both labeled and unlabeled images. The  is trained on labeled images in usual supervised manner with a cross-entropy loss. The same model is used to get predictions for a batch of unlabeled images and the  is used as the . Then, cross-entropy loss is calculated by comparing  predictions and the pseudo-label for the unlabeled images .The total loss is a weighted sum of the labeled and unlabeled loss terms.To make sure the model has learned enough from the labeled data, the \(\alpha_t\) term is set to 0 during the initial 100 training steps. It is then gradually increased up to 600 training steps and then kept constant.
 proposed a semi-supervised method inspired by Knowledge Distillation called “Noisy Student” in 2019.The key idea is to train two separate models called  and . The  is first trained on the labeled images and then it is used to infer the pseudo-labels for the unlabeled images. These pseudo-labels can either be soft-label or converted to hard-label by . Then, the labeled and unlabeled images are combined together and a  is trained on this combined data. The images are augmented using RandAugment as a form of input noise. Also, model noise such as Dropout and Stochastic Depth are incorporated in the student model architecture.Once a  is trained, it becomes the new  and this process is repeated for three iterations.This paradigm uses the idea that  predictions on an unlabeled image should remain the same even after adding noise. We could use input noise such as Image Augmentation and Gaussian noise. Noise can also be incorporated in the architecture itself using Dropout.This model was proposed by  in a conference paper at ICLR 2017.The key idea is to create two random augmentations of an image for both labeled and unlabeled data. Then, a  is used to predict the label of both these images. The  of these two  is used as a . For labeled images, we also calculate the . The total loss is a weighted sum of these two loss terms. A weight  is applied to decide how much the consistency loss contributes in the overall loss.This method was also proposed by  in the same paper as the pi-model. It modifies the π-model by leveraging the  of predictions.The key idea is to use the  of past predictions as one view. To get another view, we augment the image as usual and a  is used to predict the label. The  of  and  is used as a . For labeled images, we also calculate the . The final loss is a weighted sum of these two loss terms. A weight  is applied to decide how much the consistency loss contributes in the overall loss.This method was proposed by . The general approach is similar to Temporal Ensembling but it uses Exponential Moving Average(EMA) of the model parameters instead of predictions.The key idea is to have two models called  and . The  model is a regular model with dropout. And the  model has the same architecture as the  model but its weights are set using an  of the weights of  model. For a labeled or unlabeled image, we create two random augmented versions of the image. Then, the  model is used to predict  for first image. And, the  model is used to predict the  for the second augmented image. The  of these two  is used as a . For labeled images, we also calculate the . The final loss is a weighted sum of these two loss terms. A weight  is applied to decide how much the consistency loss contributes in the overall loss.This method was proposed by . It uses the concept of adversarial attack for consistency regularization.The key idea is to generate an adversarial transformation of an image that will change the model prediction. To do so, first, an image is taken and an adversarial variant of it is created such that the KL-divergence between the model output for the original image and the adversarial image is maximized.Then we proceed as previous methods. We take a labeled/unlabeled image as first view and take its adversarial example generated in previous step as the second view. Then, the same  is used to predict  for both images. The  of these two  is used as a . For labeled images, we also calculate the . The final loss is a weighted sum of these two loss terms. A weight  is applied to decide how much the consistency loss contributes in the overall loss.This method was proposed by  and works for both images and text. Here, we will understand the method in the context of images.The key idea is to create an augmented version of a unlabeled image using AutoAugment. Then, a same  is used to predict the label of both these images. The  of these two  is used as a . For labeled images, we only calculate the  and don’t calculate any . The final loss is a weighted sum of these two loss terms. A weight  is applied to decide how much the consistency loss contributes in the overall loss.This paradigm combines ideas from previous work such as self-training and consistency regularization along with additional components for performance improvement.This holistic method was proposed by .To understand this method, let’s take a walk through each of the steps.i. For the labeled image, we create an augmentation of it. For the unlabeled image, we create K augmentations and get the model  on all K-images. Then, the  are  and  is applied to get a final pseudo-label. This pseudo-label will be used for all the K-augmentations.ii. The batches of augmented labeled and unlabeled images are combined and the whole group is shuffled. Then, the first N images of this group are taken as \(W_L\), and the remaining M images are taken as \(W_U\).iii. Now, Mixup is applied between the augmented labeled batch and group \(W_L\). Similarly, mixup is applied between the M augmented unlabeled group and the \(W_U\) group. Thus, we get the final labeled and unlabeled group.iv. Now, for the labeled group, we take model predictions and compute  with the ground truth mixup labels. Similarly, for the unlabeled group, we compute model predictions and compute  with the mixup pseudo labels. A weighted sum is taken of these two terms with \(\lambda\) weighting the MSE loss.This method was proposed by  and combines pseudo-labeling and consistency regularization while vastly simplifying the overall method. It got state of the art results on a wide range of benchmarks.As seen, we train a supervised model on our labeled images with cross-entropy loss. For each unlabeled image,  and  are applied to get two images. The  is passed to our model and we get prediction over classes. The probability for the most confident class is compared to a . If it is above the , then we take that class as the ground label i.e. . Then, the  image is passed through our model to get a prediction over classes. This  is compared to ground truth  using cross-entropy loss. Both the losses are combined and the model is optimized.If you want to learn more about FixMatch, I have an  that goes over it in depth.Here is a high-level summary of the differences between all the above-mentioned methods.To evaluate the performance of these semi-supervised methods, the following datasets are commonly used. The authors simulate a low-data regime by using only a small portion(e.g. 40/250/4000/10000 examples) of the whole dataset as labeled and treating the remaining as the unlabeled set.Thus, we got an overview of how semi-supervised methods for Computer Vision have progressed over the years. This is a really important line of research that can have a direct impact on the industry.If you found this blog post useful, please consider citing it as: