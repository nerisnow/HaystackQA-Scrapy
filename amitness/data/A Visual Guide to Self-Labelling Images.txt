In the past year, several methods for self-supervised learning of image representations have been proposed. A recent trend in the methods is using Contrastive Learning (, , ) which have given very promising results.However, as we had seen in our  on self-supervised learning, there exist many other problem formulations for self-supervised learning. One promising approach is:Combine clustering and representation learning together to learn both features and labels simultaneously.A paper  presented at ICLR 2020 by Asano et al. of the Visual Geometry Group(VGG), University of Oxford has a new take on this approach and achieved the state of the art results in various benchmarks.The most interesting part is that we can  with this method and then use those labels independently with any model architecture and regular supervised learning methods. Self-Labelling is a very practical idea for industries and domains with scarce labeled data. Let’s understand how it works.At a very high level, the Self-Labelling method works as follows:But, how will you generate labels for images in the first place without a trained model? This sounds like the chicken-and-egg problem where if the chicken came first, what did it hatch from and if the egg came first, who laid the egg?The solution to the problem is to use a randomly initialized network to bootstrap the first set of image labels. This has been shown to work empirically in the  paper.The authors of DeepCluster used a randomly initialized  and evaluated it on ImageNet. Since the ImageNet dataset has 1000 classes, if we randomly guessed the classes, we would get an baseline accuracy of  = . But, a randomly initialized AlexNet was shown to achieve  accuracy on ImageNet. This means that a randomly-initialized network possesses some faint signal in its weights.Thus, we can use labels obtained from a randomly initialized network to kick start the process which can be refined later.Let’s now understand how the self-labelling pipeline works.

As seen in the figure above, we first generate labels for  unlabeled images using a randomly initialized model. Then, the  algorithm is applied to cluster the unlabeled images and get a new set of labels. The  is again trained on these new set of labels and optimized with cross-entropy loss.  algorithm is run once in a while during the course of training to optimize and get new set of labels. This process is repeated for a number of epochs and we get the final labels and a .Let’s see how this method is implemented in practice with a step by step example of the whole pipeline from the input data to the output labels:First of all, we get N unlabeled images \(I_1, ..., I_N\) and take batches of them from some dataset. In the paper, batches of 256 unlabeled images are prepared from the ImageNet dataset.
We apply augmentations to the unlabeled images so that the self-labelling function learned is transformation invariant. The paper first randomly crops the image into size . Then, the image is converted into grayscale with a probability of 20%. Color Jitter is applied to this image. Finally, the horizontal flip is applied 50% of the time. After the transformations are applied, the image is normalized with a mean of and a standard deviation of .This can be implemented in PyTorch for some image as:We then need to choose the number of clusters(K) we want to group our data in. By default, ImageNet has 1000 classes so we could use 1000 clusters. The number of clusters is dependent on the data and can be chosen either by using domain knowledge or by comparing the number of clusters against model performance. This is denoted by:The paper experimented with the number of clusters ranging from 1000(1k) to 10,000(10k) and found the ImageNet performance improves till 3000 but slightly degrades when using more clusters than that. So the papers use 3000 clusters and as a result 3000 classes for the model.
A ConvNet architecture such as  or  is used as the feature extractor. This  is denoted by \(\color{#885e9c}{\phi(} I \color{#885e9c}{)}\)
and maps an image I to  \(m \in R^D\) with dimension D.Then, a  is used which is simply a single  that converts the feature vectors into class scores. These scores are converted into probabilities using the softmax operator.
The above model is initialized with random weights and we do a forward pass through the model to get class predictions for each image in the batch. These predicted classes are assumed as the initial labels.
Using these initial labels, we want to find a better distribution of images into clusters. To do that, the paper uses a novel approach quite different than K-means clustering that was used in DeepCluster. The authors apply the concept of optimal transport from operations research to tackle this problem.Let’s first understand the optimal transport problem with a simple real-world example:Now, that we understand the problem, let’s see how it applies in our case of cluster allocation. The authors have formulated the problem of assigning the unlabeled images into clusters as an optimal transport problem in this way::
The unlabeled images should be divided equally into the K clusters. This is referred to as the equipartition condition in the paper.:
The cost of allocating each image to a cluster is given by the model performance when trained using these clusters as the labels. Intuitively, this means the mistake model is making when we assign an unlabeled image to some cluster. If it is high, then that means our current label assignment is not ideal and so we should change it in the optimization step.We find the optimal matrix Q using a fast-variant of the Sinkhorn-Knopp algorithm. This algorithm involves a single matrix-vector multiplication and scales linearly with the number of images N. In the paper, they were able to reach convergence on ImageNet dataset within 2 minutes when using GPU to accelerate the process. For the algorithm and derivation of Sinkhorn-Knopp, please refer to the  paper. There is also an excellent blogpost by Michiel Stock that explains Optimal Transport .
Since we have updated labels Q, we can now take predictions of the model on the images and compare it to their corresponding cluster labels with a classification cross-entropy loss. The model is trained for a fixed number of epochs and as the cross-entropy loss decrease, the internal representation learned improves.
The optimization of labels at step 6 is scheduled to occur at most once an epoch. The authors experimented with not using self-labelling algorithm at all to doing the Sinkhorn-Knopp optimization once per epoch. The best result was achieved at 80.
This shows that self-labeling is giving us a significant increase in performance compared to  (only random-initialization and augmentation).The labels obtained for images from self-labelling can be used to train another network from scratch using standard supervised training.In the paper, they took labels assigned by SeLa with AlexNet and retrained another AlexNet network from scratch with those labels using only 90-epochs to get the same accuracy.They did another interesting experiment where 3000 labels obtained by applying SeLa to ResNet-50 was used to train AlexNet model from scratch. They got  accuracy which was higher than  accuracy obtained by training AlexNet from scratch directly. This shows how labels can be transferred between architectures.The authors have published their generated labels for the ImageNet dataset. These can be used to train a supervised model from scratch.The author have also setup an interactive demo  to look at all the clusters found from ImageNet.
The paper got state of the art results on CIFAR-10, CIFAR-100 and SVHN datasets beating best previous method . An interesting result is very small improvement() on SVHN, which the authors say is because the difference between supervised baseline of 96.1 and AND’s 93.7 is already small (<3%).

The authors also evaluated it using weighted KNN and an embedding size of 128 and outperformed previous methods by 2%.
 
The paper has an assumption that images are equally distributed over classes. So, to test the impact on the algorithm when it’s trained on unbalanced datasets, the authors prepared three datasets out of CIFAR-10:The official implementation of Self-Labelling in PyTorch by the paper authors is available . They also provide  for AlexNet and Resnet-50.If you found this blog post useful, please consider citing it as:In the past year, several methods for self-supervised learning of image representations have been proposed. A recent trend in the methods is using Contrastive Learning (, , ) which have given very promising results.However, as we had seen in our  on self-supervised learning, there exist many other problem formulations for self-supervised learning. One promising approach is:Combine clustering and representation learning together to learn both features and labels simultaneously.A paper  presented at ICLR 2020 by Asano et al. of the Visual Geometry Group(VGG), University of Oxford has a new take on this approach and achieved the state of the art results in various benchmarks.The most interesting part is that we can  with this method and then use those labels independently with any model architecture and regular supervised learning methods. Self-Labelling is a very practical idea for industries and domains with scarce labeled data. Let’s understand how it works.At a very high level, the Self-Labelling method works as follows:But, how will you generate labels for images in the first place without a trained model? This sounds like the chicken-and-egg problem where if the chicken came first, what did it hatch from and if the egg came first, who laid the egg?The solution to the problem is to use a randomly initialized network to bootstrap the first set of image labels. This has been shown to work empirically in the  paper.The authors of DeepCluster used a randomly initialized  and evaluated it on ImageNet. Since the ImageNet dataset has 1000 classes, if we randomly guessed the classes, we would get an baseline accuracy of  = . But, a randomly initialized AlexNet was shown to achieve  accuracy on ImageNet. This means that a randomly-initialized network possesses some faint signal in its weights.Thus, we can use labels obtained from a randomly initialized network to kick start the process which can be refined later.Let’s now understand how the self-labelling pipeline works.

As seen in the figure above, we first generate labels for  unlabeled images using a randomly initialized model. Then, the  algorithm is applied to cluster the unlabeled images and get a new set of labels. The  is again trained on these new set of labels and optimized with cross-entropy loss.  algorithm is run once in a while during the course of training to optimize and get new set of labels. This process is repeated for a number of epochs and we get the final labels and a .Let’s see how this method is implemented in practice with a step by step example of the whole pipeline from the input data to the output labels:First of all, we get N unlabeled images \(I_1, ..., I_N\) and take batches of them from some dataset. In the paper, batches of 256 unlabeled images are prepared from the ImageNet dataset.
We apply augmentations to the unlabeled images so that the self-labelling function learned is transformation invariant. The paper first randomly crops the image into size . Then, the image is converted into grayscale with a probability of 20%. Color Jitter is applied to this image. Finally, the horizontal flip is applied 50% of the time. After the transformations are applied, the image is normalized with a mean of and a standard deviation of .This can be implemented in PyTorch for some image as:We then need to choose the number of clusters(K) we want to group our data in. By default, ImageNet has 1000 classes so we could use 1000 clusters. The number of clusters is dependent on the data and can be chosen either by using domain knowledge or by comparing the number of clusters against model performance. This is denoted by:The paper experimented with the number of clusters ranging from 1000(1k) to 10,000(10k) and found the ImageNet performance improves till 3000 but slightly degrades when using more clusters than that. So the papers use 3000 clusters and as a result 3000 classes for the model.
A ConvNet architecture such as  or  is used as the feature extractor. This  is denoted by \(\color{#885e9c}{\phi(} I \color{#885e9c}{)}\)
and maps an image I to  \(m \in R^D\) with dimension D.Then, a  is used which is simply a single  that converts the feature vectors into class scores. These scores are converted into probabilities using the softmax operator.
The above model is initialized with random weights and we do a forward pass through the model to get class predictions for each image in the batch. These predicted classes are assumed as the initial labels.
Using these initial labels, we want to find a better distribution of images into clusters. To do that, the paper uses a novel approach quite different than K-means clustering that was used in DeepCluster. The authors apply the concept of optimal transport from operations research to tackle this problem.Let’s first understand the optimal transport problem with a simple real-world example:Now, that we understand the problem, let’s see how it applies in our case of cluster allocation. The authors have formulated the problem of assigning the unlabeled images into clusters as an optimal transport problem in this way::
The unlabeled images should be divided equally into the K clusters. This is referred to as the equipartition condition in the paper.:
The cost of allocating each image to a cluster is given by the model performance when trained using these clusters as the labels. Intuitively, this means the mistake model is making when we assign an unlabeled image to some cluster. If it is high, then that means our current label assignment is not ideal and so we should change it in the optimization step.We find the optimal matrix Q using a fast-variant of the Sinkhorn-Knopp algorithm. This algorithm involves a single matrix-vector multiplication and scales linearly with the number of images N. In the paper, they were able to reach convergence on ImageNet dataset within 2 minutes when using GPU to accelerate the process. For the algorithm and derivation of Sinkhorn-Knopp, please refer to the  paper. There is also an excellent blogpost by Michiel Stock that explains Optimal Transport .
Since we have updated labels Q, we can now take predictions of the model on the images and compare it to their corresponding cluster labels with a classification cross-entropy loss. The model is trained for a fixed number of epochs and as the cross-entropy loss decrease, the internal representation learned improves.
The optimization of labels at step 6 is scheduled to occur at most once an epoch. The authors experimented with not using self-labelling algorithm at all to doing the Sinkhorn-Knopp optimization once per epoch. The best result was achieved at 80.
This shows that self-labeling is giving us a significant increase in performance compared to  (only random-initialization and augmentation).The labels obtained for images from self-labelling can be used to train another network from scratch using standard supervised training.In the paper, they took labels assigned by SeLa with AlexNet and retrained another AlexNet network from scratch with those labels using only 90-epochs to get the same accuracy.They did another interesting experiment where 3000 labels obtained by applying SeLa to ResNet-50 was used to train AlexNet model from scratch. They got  accuracy which was higher than  accuracy obtained by training AlexNet from scratch directly. This shows how labels can be transferred between architectures.The authors have published their generated labels for the ImageNet dataset. These can be used to train a supervised model from scratch.The author have also setup an interactive demo  to look at all the clusters found from ImageNet.
The paper got state of the art results on CIFAR-10, CIFAR-100 and SVHN datasets beating best previous method . An interesting result is very small improvement() on SVHN, which the authors say is because the difference between supervised baseline of 96.1 and AND’s 93.7 is already small (<3%).

The authors also evaluated it using weighted KNN and an embedding size of 128 and outperformed previous methods by 2%.
 
The paper has an assumption that images are equally distributed over classes. So, to test the impact on the algorithm when it’s trained on unbalanced datasets, the authors prepared three datasets out of CIFAR-10:The official implementation of Self-Labelling in PyTorch by the paper authors is available . They also provide  for AlexNet and Resnet-50.If you found this blog post useful, please consider citing it as:In the past year, several methods for self-supervised learning of image representations have been proposed. A recent trend in the methods is using Contrastive Learning (, , ) which have given very promising results.However, as we had seen in our  on self-supervised learning, there exist many other problem formulations for self-supervised learning. One promising approach is:Combine clustering and representation learning together to learn both features and labels simultaneously.A paper  presented at ICLR 2020 by Asano et al. of the Visual Geometry Group(VGG), University of Oxford has a new take on this approach and achieved the state of the art results in various benchmarks.The most interesting part is that we can  with this method and then use those labels independently with any model architecture and regular supervised learning methods. Self-Labelling is a very practical idea for industries and domains with scarce labeled data. Let’s understand how it works.At a very high level, the Self-Labelling method works as follows:But, how will you generate labels for images in the first place without a trained model? This sounds like the chicken-and-egg problem where if the chicken came first, what did it hatch from and if the egg came first, who laid the egg?The solution to the problem is to use a randomly initialized network to bootstrap the first set of image labels. This has been shown to work empirically in the  paper.The authors of DeepCluster used a randomly initialized  and evaluated it on ImageNet. Since the ImageNet dataset has 1000 classes, if we randomly guessed the classes, we would get an baseline accuracy of  = . But, a randomly initialized AlexNet was shown to achieve  accuracy on ImageNet. This means that a randomly-initialized network possesses some faint signal in its weights.Thus, we can use labels obtained from a randomly initialized network to kick start the process which can be refined later.Let’s now understand how the self-labelling pipeline works.

As seen in the figure above, we first generate labels for  unlabeled images using a randomly initialized model. Then, the  algorithm is applied to cluster the unlabeled images and get a new set of labels. The  is again trained on these new set of labels and optimized with cross-entropy loss.  algorithm is run once in a while during the course of training to optimize and get new set of labels. This process is repeated for a number of epochs and we get the final labels and a .Let’s see how this method is implemented in practice with a step by step example of the whole pipeline from the input data to the output labels:First of all, we get N unlabeled images \(I_1, ..., I_N\) and take batches of them from some dataset. In the paper, batches of 256 unlabeled images are prepared from the ImageNet dataset.
We apply augmentations to the unlabeled images so that the self-labelling function learned is transformation invariant. The paper first randomly crops the image into size . Then, the image is converted into grayscale with a probability of 20%. Color Jitter is applied to this image. Finally, the horizontal flip is applied 50% of the time. After the transformations are applied, the image is normalized with a mean of and a standard deviation of .This can be implemented in PyTorch for some image as:We then need to choose the number of clusters(K) we want to group our data in. By default, ImageNet has 1000 classes so we could use 1000 clusters. The number of clusters is dependent on the data and can be chosen either by using domain knowledge or by comparing the number of clusters against model performance. This is denoted by:The paper experimented with the number of clusters ranging from 1000(1k) to 10,000(10k) and found the ImageNet performance improves till 3000 but slightly degrades when using more clusters than that. So the papers use 3000 clusters and as a result 3000 classes for the model.
A ConvNet architecture such as  or  is used as the feature extractor. This  is denoted by \(\color{#885e9c}{\phi(} I \color{#885e9c}{)}\)
and maps an image I to  \(m \in R^D\) with dimension D.Then, a  is used which is simply a single  that converts the feature vectors into class scores. These scores are converted into probabilities using the softmax operator.
The above model is initialized with random weights and we do a forward pass through the model to get class predictions for each image in the batch. These predicted classes are assumed as the initial labels.
Using these initial labels, we want to find a better distribution of images into clusters. To do that, the paper uses a novel approach quite different than K-means clustering that was used in DeepCluster. The authors apply the concept of optimal transport from operations research to tackle this problem.Let’s first understand the optimal transport problem with a simple real-world example:Now, that we understand the problem, let’s see how it applies in our case of cluster allocation. The authors have formulated the problem of assigning the unlabeled images into clusters as an optimal transport problem in this way::
The unlabeled images should be divided equally into the K clusters. This is referred to as the equipartition condition in the paper.:
The cost of allocating each image to a cluster is given by the model performance when trained using these clusters as the labels. Intuitively, this means the mistake model is making when we assign an unlabeled image to some cluster. If it is high, then that means our current label assignment is not ideal and so we should change it in the optimization step.We find the optimal matrix Q using a fast-variant of the Sinkhorn-Knopp algorithm. This algorithm involves a single matrix-vector multiplication and scales linearly with the number of images N. In the paper, they were able to reach convergence on ImageNet dataset within 2 minutes when using GPU to accelerate the process. For the algorithm and derivation of Sinkhorn-Knopp, please refer to the  paper. There is also an excellent blogpost by Michiel Stock that explains Optimal Transport .
Since we have updated labels Q, we can now take predictions of the model on the images and compare it to their corresponding cluster labels with a classification cross-entropy loss. The model is trained for a fixed number of epochs and as the cross-entropy loss decrease, the internal representation learned improves.
The optimization of labels at step 6 is scheduled to occur at most once an epoch. The authors experimented with not using self-labelling algorithm at all to doing the Sinkhorn-Knopp optimization once per epoch. The best result was achieved at 80.
This shows that self-labeling is giving us a significant increase in performance compared to  (only random-initialization and augmentation).The labels obtained for images from self-labelling can be used to train another network from scratch using standard supervised training.In the paper, they took labels assigned by SeLa with AlexNet and retrained another AlexNet network from scratch with those labels using only 90-epochs to get the same accuracy.They did another interesting experiment where 3000 labels obtained by applying SeLa to ResNet-50 was used to train AlexNet model from scratch. They got  accuracy which was higher than  accuracy obtained by training AlexNet from scratch directly. This shows how labels can be transferred between architectures.The authors have published their generated labels for the ImageNet dataset. These can be used to train a supervised model from scratch.The author have also setup an interactive demo  to look at all the clusters found from ImageNet.
The paper got state of the art results on CIFAR-10, CIFAR-100 and SVHN datasets beating best previous method . An interesting result is very small improvement() on SVHN, which the authors say is because the difference between supervised baseline of 96.1 and AND’s 93.7 is already small (<3%).

The authors also evaluated it using weighted KNN and an embedding size of 128 and outperformed previous methods by 2%.
 
The paper has an assumption that images are equally distributed over classes. So, to test the impact on the algorithm when it’s trained on unbalanced datasets, the authors prepared three datasets out of CIFAR-10:The official implementation of Self-Labelling in PyTorch by the paper authors is available . They also provide  for AlexNet and Resnet-50.If you found this blog post useful, please consider citing it as: