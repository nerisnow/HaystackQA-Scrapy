Keras provides a powerful abstraction for recurrent layers such as RNN, GRU, and LSTM for Natural Language Processing. When I first started learning about them from the documentation, I couldn’t clearly understand how to prepare input data shape, how various attributes of the layers affect the outputs, and how to compose these layers with the provided abstraction.Having learned it through experimentation, I wanted to share my understanding of the API with visualizations so that it’s helpful for anyone else having troubles.Let’s take a simple example of encoding the meaning of a whole sentence using an RNN layer in Keras.
Credits: Marvel StudiosTo use this sentence in an RNN, we need to first convert it into numeric form. We could either use one-hot encoding, pretrained word vectors, or learn word embeddings from scratch. For simplicity, let’s assume we used some word embedding to convert each word into 2 numbers.Now, to pass these words into an RNN, we treat each word as a time-step and the embedding as features. Let’s build an RNN layer to pass these into
As seen above, here is what the various parameters means and why they were set as such:Thus for a whole sentence, we get a vector of size 4 as output from the RNN layer as shown in the figure. You can verify this by printing the shape of the output from the layer.As seen, we create a random batch of input data with 1 sentence having 3 words and each word having an embedding of size 2. After passing through the LSTM layer, we get back a representation of size 4 for that one sentence.This can be combined with a Dense layer to build an architecture for something like sentiment analysis or text classification.Keras provides a  parameter to control output from the RNN cell. If we set it to , what it means is that the output from each unfolded RNN cell is returned instead of only the last cell.As seen above, we get an  of size  for each word in the sentence.This can be verified by the below code where we send one sentence with 3 words and embedding of size 2 for each word. As seen, the layer gives us back 3 outputs with a vector of size 4 for each word.Suppose we want to recognize entities in a text. For example, in our text “I am ”, we want to identify  as a .
We have already seen how to get output for each word in the sentence in the previous section. Now, we need some way to apply classification on the output vector from the RNN cell on each word. For simple cases such as text classification, you know how we use the  layer with  activation as the last layer.Similar to that, we can apply  layer on  from the RNN layer through a wrapper layer called TimeDistributed(). It will apply the  layer on  and give us class probability scores for the entities.As seen, we take a 3 word sentence and classify output of RNN for each word into 4 classes using . These classes can be the entities like name, person, location etc.We can also stack multiple recurrent layers one after another in Keras.We can understand the behavior of the code with the following figure:
Since the second layer needs inputs from the first layer, we set return_sequence=True for the first SimpleRNN layer. For the second layer, we usually set it to False if we are going to just be doing text classification. If out task is NER prediction, we can set it to True in the final layer as well.
Keras provides a powerful abstraction for recurrent layers such as RNN, GRU, and LSTM for Natural Language Processing. When I first started learning about them from the documentation, I couldn’t clearly understand how to prepare input data shape, how various attributes of the layers affect the outputs, and how to compose these layers with the provided abstraction.Having learned it through experimentation, I wanted to share my understanding of the API with visualizations so that it’s helpful for anyone else having troubles.Let’s take a simple example of encoding the meaning of a whole sentence using an RNN layer in Keras.
Credits: Marvel StudiosTo use this sentence in an RNN, we need to first convert it into numeric form. We could either use one-hot encoding, pretrained word vectors, or learn word embeddings from scratch. For simplicity, let’s assume we used some word embedding to convert each word into 2 numbers.Now, to pass these words into an RNN, we treat each word as a time-step and the embedding as features. Let’s build an RNN layer to pass these into
As seen above, here is what the various parameters means and why they were set as such:Thus for a whole sentence, we get a vector of size 4 as output from the RNN layer as shown in the figure. You can verify this by printing the shape of the output from the layer.As seen, we create a random batch of input data with 1 sentence having 3 words and each word having an embedding of size 2. After passing through the LSTM layer, we get back a representation of size 4 for that one sentence.This can be combined with a Dense layer to build an architecture for something like sentiment analysis or text classification.Keras provides a  parameter to control output from the RNN cell. If we set it to , what it means is that the output from each unfolded RNN cell is returned instead of only the last cell.As seen above, we get an  of size  for each word in the sentence.This can be verified by the below code where we send one sentence with 3 words and embedding of size 2 for each word. As seen, the layer gives us back 3 outputs with a vector of size 4 for each word.Suppose we want to recognize entities in a text. For example, in our text “I am ”, we want to identify  as a .
We have already seen how to get output for each word in the sentence in the previous section. Now, we need some way to apply classification on the output vector from the RNN cell on each word. For simple cases such as text classification, you know how we use the  layer with  activation as the last layer.Similar to that, we can apply  layer on  from the RNN layer through a wrapper layer called TimeDistributed(). It will apply the  layer on  and give us class probability scores for the entities.As seen, we take a 3 word sentence and classify output of RNN for each word into 4 classes using . These classes can be the entities like name, person, location etc.We can also stack multiple recurrent layers one after another in Keras.We can understand the behavior of the code with the following figure:
Since the second layer needs inputs from the first layer, we set return_sequence=True for the first SimpleRNN layer. For the second layer, we usually set it to False if we are going to just be doing text classification. If out task is NER prediction, we can set it to True in the final layer as well.
Keras provides a powerful abstraction for recurrent layers such as RNN, GRU, and LSTM for Natural Language Processing. When I first started learning about them from the documentation, I couldn’t clearly understand how to prepare input data shape, how various attributes of the layers affect the outputs, and how to compose these layers with the provided abstraction.Having learned it through experimentation, I wanted to share my understanding of the API with visualizations so that it’s helpful for anyone else having troubles.Let’s take a simple example of encoding the meaning of a whole sentence using an RNN layer in Keras.
Credits: Marvel StudiosTo use this sentence in an RNN, we need to first convert it into numeric form. We could either use one-hot encoding, pretrained word vectors, or learn word embeddings from scratch. For simplicity, let’s assume we used some word embedding to convert each word into 2 numbers.Now, to pass these words into an RNN, we treat each word as a time-step and the embedding as features. Let’s build an RNN layer to pass these into
As seen above, here is what the various parameters means and why they were set as such:Thus for a whole sentence, we get a vector of size 4 as output from the RNN layer as shown in the figure. You can verify this by printing the shape of the output from the layer.As seen, we create a random batch of input data with 1 sentence having 3 words and each word having an embedding of size 2. After passing through the LSTM layer, we get back a representation of size 4 for that one sentence.This can be combined with a Dense layer to build an architecture for something like sentiment analysis or text classification.Keras provides a  parameter to control output from the RNN cell. If we set it to , what it means is that the output from each unfolded RNN cell is returned instead of only the last cell.As seen above, we get an  of size  for each word in the sentence.This can be verified by the below code where we send one sentence with 3 words and embedding of size 2 for each word. As seen, the layer gives us back 3 outputs with a vector of size 4 for each word.Suppose we want to recognize entities in a text. For example, in our text “I am ”, we want to identify  as a .
We have already seen how to get output for each word in the sentence in the previous section. Now, we need some way to apply classification on the output vector from the RNN cell on each word. For simple cases such as text classification, you know how we use the  layer with  activation as the last layer.Similar to that, we can apply  layer on  from the RNN layer through a wrapper layer called TimeDistributed(). It will apply the  layer on  and give us class probability scores for the entities.As seen, we take a 3 word sentence and classify output of RNN for each word into 4 classes using . These classes can be the entities like name, person, location etc.We can also stack multiple recurrent layers one after another in Keras.We can understand the behavior of the code with the following figure:
Since the second layer needs inputs from the first layer, we set return_sequence=True for the first SimpleRNN layer. For the second layer, we usually set it to False if we are going to just be doing text classification. If out task is NER prediction, we can set it to True in the final layer as well.
