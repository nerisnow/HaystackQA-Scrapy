PyTorch has emerged as one of the go-to deep learning frameworks in recent years. This popularity can be attributed to its easy to use API and it being more “pythonic”.PyTorch leverages numerous native features of Python to give us a consistent and clean API. In this article, I will explain those native features in detail. Learning these will help you better understand why you do things a certain way in PyTorch and make better use of what it has to offer.Layers such as  are some of the basic constructs in PyTorch that we use to build our models. You import the layer and apply them to tensors.Here we are able to call layer on some tensor , so it must be a function right? Is  returning a function? Let’s verify it by checking the type.Surprise!  is actually a class and layer an object of that class.“What! How could we call it then? Aren’t only functions supposed to be callable?”Nope, we can create callable objects as well. Python provides a native way to make objects created from classes callable by using magic functions.
Let’s see a simple example of a class that doubles a number.Here we add a magic method  in the class to double any number passed to it. Now, you can create an object out of this class and call it on some number.Alternatively, the above code can be combined in the single line itself.This works because everything in Python is an object. See an example of a function below that doubles a number.Even functions invoke the method behind the scenes.Let’s see an example of a model that applies a single fully connected layer to MNIST images to get 10 outputs.The following code should be familiar to you. We are computing output of this model on some tensor x.We know calling the model directly on some tensor executes the  function on it. How does that work?It’s the same reason in previous example. We’re inheriting the class . Internally,  has a  magic method that calls the . So, when we override  method later, it’s executed.Thus, we were able to call the model directly on tensors.In PyTorch, it is common to create a custom class inheriting from the  class to prepare our training and test datasets. Have you ever wondered why we define methods with obscure names like  and  in it?These methods are builtin magic methods of Python. You know how we can get the length of iterables like list and tuples using  function.Python allows defining a  on our custom class so that  works on it. For example,Similarly, you know how we can access elements of list and tuples using index notation.Python allows a  magic method to allow such functionality for custom classes. For example,With the above concept, now you can easily understand the builtin dataset like MNIST and what you can do with them.Let’s create a dataloader for a training dataset of MNIST digits.Now, let’s try accessing first batch from the data loader directly without looping. If we try to access it via index, we get an exception.You might have been used to doing it in this way.Have you ever wondered why do we wrap trainloader by  and then call ? Let’s demystify this.Consider a list  with 3 elements. In Python, we can create an iterator out of  using the  function.Iterators are used because they allow lazy loading such that only one element is loaded in memory at a time.We get each element and when we reach the end of the list, we get a  exception.This pattern matches our usual machine learning workflow where we take small batches of data at a time in memory and do the forward and backward pass. So,  also incorporates this pattern in PyTorch.To create iterators out of classes in Python, we need to define magic methods  and Here, the  function calls the  magic method of the class returning that same object. Then, the  function calls the  magic method of the class to return next element present in our data.In PyTorch, the implementation of DataLoader implements this pattern as follows:So, they decouple the iterator creation part and the actual data loading part.Thus, we get images and labels for a single batch.Thus, we saw how PyTorch borrows several advanced concepts from native Python itself in its API design. I hope the article was helpful to demystify how these concepts work behind the scenes and will help you become a better PyTorch user.PyTorch has emerged as one of the go-to deep learning frameworks in recent years. This popularity can be attributed to its easy to use API and it being more “pythonic”.PyTorch leverages numerous native features of Python to give us a consistent and clean API. In this article, I will explain those native features in detail. Learning these will help you better understand why you do things a certain way in PyTorch and make better use of what it has to offer.Layers such as  are some of the basic constructs in PyTorch that we use to build our models. You import the layer and apply them to tensors.Here we are able to call layer on some tensor , so it must be a function right? Is  returning a function? Let’s verify it by checking the type.Surprise!  is actually a class and layer an object of that class.“What! How could we call it then? Aren’t only functions supposed to be callable?”Nope, we can create callable objects as well. Python provides a native way to make objects created from classes callable by using magic functions.
Let’s see a simple example of a class that doubles a number.Here we add a magic method  in the class to double any number passed to it. Now, you can create an object out of this class and call it on some number.Alternatively, the above code can be combined in the single line itself.This works because everything in Python is an object. See an example of a function below that doubles a number.Even functions invoke the method behind the scenes.Let’s see an example of a model that applies a single fully connected layer to MNIST images to get 10 outputs.The following code should be familiar to you. We are computing output of this model on some tensor x.We know calling the model directly on some tensor executes the  function on it. How does that work?It’s the same reason in previous example. We’re inheriting the class . Internally,  has a  magic method that calls the . So, when we override  method later, it’s executed.Thus, we were able to call the model directly on tensors.In PyTorch, it is common to create a custom class inheriting from the  class to prepare our training and test datasets. Have you ever wondered why we define methods with obscure names like  and  in it?These methods are builtin magic methods of Python. You know how we can get the length of iterables like list and tuples using  function.Python allows defining a  on our custom class so that  works on it. For example,Similarly, you know how we can access elements of list and tuples using index notation.Python allows a  magic method to allow such functionality for custom classes. For example,With the above concept, now you can easily understand the builtin dataset like MNIST and what you can do with them.Let’s create a dataloader for a training dataset of MNIST digits.Now, let’s try accessing first batch from the data loader directly without looping. If we try to access it via index, we get an exception.You might have been used to doing it in this way.Have you ever wondered why do we wrap trainloader by  and then call ? Let’s demystify this.Consider a list  with 3 elements. In Python, we can create an iterator out of  using the  function.Iterators are used because they allow lazy loading such that only one element is loaded in memory at a time.We get each element and when we reach the end of the list, we get a  exception.This pattern matches our usual machine learning workflow where we take small batches of data at a time in memory and do the forward and backward pass. So,  also incorporates this pattern in PyTorch.To create iterators out of classes in Python, we need to define magic methods  and Here, the  function calls the  magic method of the class returning that same object. Then, the  function calls the  magic method of the class to return next element present in our data.In PyTorch, the implementation of DataLoader implements this pattern as follows:So, they decouple the iterator creation part and the actual data loading part.Thus, we get images and labels for a single batch.Thus, we saw how PyTorch borrows several advanced concepts from native Python itself in its API design. I hope the article was helpful to demystify how these concepts work behind the scenes and will help you become a better PyTorch user.PyTorch has emerged as one of the go-to deep learning frameworks in recent years. This popularity can be attributed to its easy to use API and it being more “pythonic”.PyTorch leverages numerous native features of Python to give us a consistent and clean API. In this article, I will explain those native features in detail. Learning these will help you better understand why you do things a certain way in PyTorch and make better use of what it has to offer.Layers such as  are some of the basic constructs in PyTorch that we use to build our models. You import the layer and apply them to tensors.Here we are able to call layer on some tensor , so it must be a function right? Is  returning a function? Let’s verify it by checking the type.Surprise!  is actually a class and layer an object of that class.“What! How could we call it then? Aren’t only functions supposed to be callable?”Nope, we can create callable objects as well. Python provides a native way to make objects created from classes callable by using magic functions.
Let’s see a simple example of a class that doubles a number.Here we add a magic method  in the class to double any number passed to it. Now, you can create an object out of this class and call it on some number.Alternatively, the above code can be combined in the single line itself.This works because everything in Python is an object. See an example of a function below that doubles a number.Even functions invoke the method behind the scenes.Let’s see an example of a model that applies a single fully connected layer to MNIST images to get 10 outputs.The following code should be familiar to you. We are computing output of this model on some tensor x.We know calling the model directly on some tensor executes the  function on it. How does that work?It’s the same reason in previous example. We’re inheriting the class . Internally,  has a  magic method that calls the . So, when we override  method later, it’s executed.Thus, we were able to call the model directly on tensors.In PyTorch, it is common to create a custom class inheriting from the  class to prepare our training and test datasets. Have you ever wondered why we define methods with obscure names like  and  in it?These methods are builtin magic methods of Python. You know how we can get the length of iterables like list and tuples using  function.Python allows defining a  on our custom class so that  works on it. For example,Similarly, you know how we can access elements of list and tuples using index notation.Python allows a  magic method to allow such functionality for custom classes. For example,With the above concept, now you can easily understand the builtin dataset like MNIST and what you can do with them.Let’s create a dataloader for a training dataset of MNIST digits.Now, let’s try accessing first batch from the data loader directly without looping. If we try to access it via index, we get an exception.You might have been used to doing it in this way.Have you ever wondered why do we wrap trainloader by  and then call ? Let’s demystify this.Consider a list  with 3 elements. In Python, we can create an iterator out of  using the  function.Iterators are used because they allow lazy loading such that only one element is loaded in memory at a time.We get each element and when we reach the end of the list, we get a  exception.This pattern matches our usual machine learning workflow where we take small batches of data at a time in memory and do the forward and backward pass. So,  also incorporates this pattern in PyTorch.To create iterators out of classes in Python, we need to define magic methods  and Here, the  function calls the  magic method of the class returning that same object. Then, the  function calls the  magic method of the class to return next element present in our data.In PyTorch, the implementation of DataLoader implements this pattern as follows:So, they decouple the iterator creation part and the actual data loading part.Thus, we get images and labels for a single batch.Thus, we saw how PyTorch borrows several advanced concepts from native Python itself in its API design. I hope the article was helpful to demystify how these concepts work behind the scenes and will help you become a better PyTorch user.PyTorch has emerged as one of the go-to deep learning frameworks in recent years. This popularity can be attributed to its easy to use API and it being more “pythonic”.PyTorch leverages numerous native features of Python to give us a consistent and clean API. In this article, I will explain those native features in detail. Learning these will help you better understand why you do things a certain way in PyTorch and make better use of what it has to offer.Layers such as  are some of the basic constructs in PyTorch that we use to build our models. You import the layer and apply them to tensors.Here we are able to call layer on some tensor , so it must be a function right? Is  returning a function? Let’s verify it by checking the type.Surprise!  is actually a class and layer an object of that class.“What! How could we call it then? Aren’t only functions supposed to be callable?”Nope, we can create callable objects as well. Python provides a native way to make objects created from classes callable by using magic functions.
Let’s see a simple example of a class that doubles a number.Here we add a magic method  in the class to double any number passed to it. Now, you can create an object out of this class and call it on some number.Alternatively, the above code can be combined in the single line itself.This works because everything in Python is an object. See an example of a function below that doubles a number.Even functions invoke the method behind the scenes.Let’s see an example of a model that applies a single fully connected layer to MNIST images to get 10 outputs.The following code should be familiar to you. We are computing output of this model on some tensor x.We know calling the model directly on some tensor executes the  function on it. How does that work?It’s the same reason in previous example. We’re inheriting the class . Internally,  has a  magic method that calls the . So, when we override  method later, it’s executed.Thus, we were able to call the model directly on tensors.In PyTorch, it is common to create a custom class inheriting from the  class to prepare our training and test datasets. Have you ever wondered why we define methods with obscure names like  and  in it?These methods are builtin magic methods of Python. You know how we can get the length of iterables like list and tuples using  function.Python allows defining a  on our custom class so that  works on it. For example,Similarly, you know how we can access elements of list and tuples using index notation.Python allows a  magic method to allow such functionality for custom classes. For example,With the above concept, now you can easily understand the builtin dataset like MNIST and what you can do with them.Let’s create a dataloader for a training dataset of MNIST digits.Now, let’s try accessing first batch from the data loader directly without looping. If we try to access it via index, we get an exception.You might have been used to doing it in this way.Have you ever wondered why do we wrap trainloader by  and then call ? Let’s demystify this.Consider a list  with 3 elements. In Python, we can create an iterator out of  using the  function.Iterators are used because they allow lazy loading such that only one element is loaded in memory at a time.We get each element and when we reach the end of the list, we get a  exception.This pattern matches our usual machine learning workflow where we take small batches of data at a time in memory and do the forward and backward pass. So,  also incorporates this pattern in PyTorch.To create iterators out of classes in Python, we need to define magic methods  and Here, the  function calls the  magic method of the class returning that same object. Then, the  function calls the  magic method of the class to return next element present in our data.In PyTorch, the implementation of DataLoader implements this pattern as follows:So, they decouple the iterator creation part and the actual data loading part.Thus, we get images and labels for a single batch.Thus, we saw how PyTorch borrows several advanced concepts from native Python itself in its API design. I hope the article was helpful to demystify how these concepts work behind the scenes and will help you become a better PyTorch user.PyTorch has emerged as one of the go-to deep learning frameworks in recent years. This popularity can be attributed to its easy to use API and it being more “pythonic”.PyTorch leverages numerous native features of Python to give us a consistent and clean API. In this article, I will explain those native features in detail. Learning these will help you better understand why you do things a certain way in PyTorch and make better use of what it has to offer.Layers such as  are some of the basic constructs in PyTorch that we use to build our models. You import the layer and apply them to tensors.Here we are able to call layer on some tensor , so it must be a function right? Is  returning a function? Let’s verify it by checking the type.Surprise!  is actually a class and layer an object of that class.“What! How could we call it then? Aren’t only functions supposed to be callable?”Nope, we can create callable objects as well. Python provides a native way to make objects created from classes callable by using magic functions.
Let’s see a simple example of a class that doubles a number.Here we add a magic method  in the class to double any number passed to it. Now, you can create an object out of this class and call it on some number.Alternatively, the above code can be combined in the single line itself.This works because everything in Python is an object. See an example of a function below that doubles a number.Even functions invoke the method behind the scenes.Let’s see an example of a model that applies a single fully connected layer to MNIST images to get 10 outputs.The following code should be familiar to you. We are computing output of this model on some tensor x.We know calling the model directly on some tensor executes the  function on it. How does that work?It’s the same reason in previous example. We’re inheriting the class . Internally,  has a  magic method that calls the . So, when we override  method later, it’s executed.Thus, we were able to call the model directly on tensors.In PyTorch, it is common to create a custom class inheriting from the  class to prepare our training and test datasets. Have you ever wondered why we define methods with obscure names like  and  in it?These methods are builtin magic methods of Python. You know how we can get the length of iterables like list and tuples using  function.Python allows defining a  on our custom class so that  works on it. For example,Similarly, you know how we can access elements of list and tuples using index notation.Python allows a  magic method to allow such functionality for custom classes. For example,With the above concept, now you can easily understand the builtin dataset like MNIST and what you can do with them.Let’s create a dataloader for a training dataset of MNIST digits.Now, let’s try accessing first batch from the data loader directly without looping. If we try to access it via index, we get an exception.You might have been used to doing it in this way.Have you ever wondered why do we wrap trainloader by  and then call ? Let’s demystify this.Consider a list  with 3 elements. In Python, we can create an iterator out of  using the  function.Iterators are used because they allow lazy loading such that only one element is loaded in memory at a time.We get each element and when we reach the end of the list, we get a  exception.This pattern matches our usual machine learning workflow where we take small batches of data at a time in memory and do the forward and backward pass. So,  also incorporates this pattern in PyTorch.To create iterators out of classes in Python, we need to define magic methods  and Here, the  function calls the  magic method of the class returning that same object. Then, the  function calls the  magic method of the class to return next element present in our data.In PyTorch, the implementation of DataLoader implements this pattern as follows:So, they decouple the iterator creation part and the actual data loading part.Thus, we get images and labels for a single batch.Thus, we saw how PyTorch borrows several advanced concepts from native Python itself in its API design. I hope the article was helpful to demystify how these concepts work behind the scenes and will help you become a better PyTorch user.PyTorch has emerged as one of the go-to deep learning frameworks in recent years. This popularity can be attributed to its easy to use API and it being more “pythonic”.PyTorch leverages numerous native features of Python to give us a consistent and clean API. In this article, I will explain those native features in detail. Learning these will help you better understand why you do things a certain way in PyTorch and make better use of what it has to offer.Layers such as  are some of the basic constructs in PyTorch that we use to build our models. You import the layer and apply them to tensors.Here we are able to call layer on some tensor , so it must be a function right? Is  returning a function? Let’s verify it by checking the type.Surprise!  is actually a class and layer an object of that class.“What! How could we call it then? Aren’t only functions supposed to be callable?”Nope, we can create callable objects as well. Python provides a native way to make objects created from classes callable by using magic functions.
Let’s see a simple example of a class that doubles a number.Here we add a magic method  in the class to double any number passed to it. Now, you can create an object out of this class and call it on some number.Alternatively, the above code can be combined in the single line itself.This works because everything in Python is an object. See an example of a function below that doubles a number.Even functions invoke the method behind the scenes.Let’s see an example of a model that applies a single fully connected layer to MNIST images to get 10 outputs.The following code should be familiar to you. We are computing output of this model on some tensor x.We know calling the model directly on some tensor executes the  function on it. How does that work?It’s the same reason in previous example. We’re inheriting the class . Internally,  has a  magic method that calls the . So, when we override  method later, it’s executed.Thus, we were able to call the model directly on tensors.In PyTorch, it is common to create a custom class inheriting from the  class to prepare our training and test datasets. Have you ever wondered why we define methods with obscure names like  and  in it?These methods are builtin magic methods of Python. You know how we can get the length of iterables like list and tuples using  function.Python allows defining a  on our custom class so that  works on it. For example,Similarly, you know how we can access elements of list and tuples using index notation.Python allows a  magic method to allow such functionality for custom classes. For example,With the above concept, now you can easily understand the builtin dataset like MNIST and what you can do with them.Let’s create a dataloader for a training dataset of MNIST digits.Now, let’s try accessing first batch from the data loader directly without looping. If we try to access it via index, we get an exception.You might have been used to doing it in this way.Have you ever wondered why do we wrap trainloader by  and then call ? Let’s demystify this.Consider a list  with 3 elements. In Python, we can create an iterator out of  using the  function.Iterators are used because they allow lazy loading such that only one element is loaded in memory at a time.We get each element and when we reach the end of the list, we get a  exception.This pattern matches our usual machine learning workflow where we take small batches of data at a time in memory and do the forward and backward pass. So,  also incorporates this pattern in PyTorch.To create iterators out of classes in Python, we need to define magic methods  and Here, the  function calls the  magic method of the class returning that same object. Then, the  function calls the  magic method of the class to return next element present in our data.In PyTorch, the implementation of DataLoader implements this pattern as follows:So, they decouple the iterator creation part and the actual data loading part.Thus, we get images and labels for a single batch.Thus, we saw how PyTorch borrows several advanced concepts from native Python itself in its API design. I hope the article was helpful to demystify how these concepts work behind the scenes and will help you become a better PyTorch user.PyTorch has emerged as one of the go-to deep learning frameworks in recent years. This popularity can be attributed to its easy to use API and it being more “pythonic”.PyTorch leverages numerous native features of Python to give us a consistent and clean API. In this article, I will explain those native features in detail. Learning these will help you better understand why you do things a certain way in PyTorch and make better use of what it has to offer.Layers such as  are some of the basic constructs in PyTorch that we use to build our models. You import the layer and apply them to tensors.Here we are able to call layer on some tensor , so it must be a function right? Is  returning a function? Let’s verify it by checking the type.Surprise!  is actually a class and layer an object of that class.“What! How could we call it then? Aren’t only functions supposed to be callable?”Nope, we can create callable objects as well. Python provides a native way to make objects created from classes callable by using magic functions.
Let’s see a simple example of a class that doubles a number.Here we add a magic method  in the class to double any number passed to it. Now, you can create an object out of this class and call it on some number.Alternatively, the above code can be combined in the single line itself.This works because everything in Python is an object. See an example of a function below that doubles a number.Even functions invoke the method behind the scenes.Let’s see an example of a model that applies a single fully connected layer to MNIST images to get 10 outputs.The following code should be familiar to you. We are computing output of this model on some tensor x.We know calling the model directly on some tensor executes the  function on it. How does that work?It’s the same reason in previous example. We’re inheriting the class . Internally,  has a  magic method that calls the . So, when we override  method later, it’s executed.Thus, we were able to call the model directly on tensors.In PyTorch, it is common to create a custom class inheriting from the  class to prepare our training and test datasets. Have you ever wondered why we define methods with obscure names like  and  in it?These methods are builtin magic methods of Python. You know how we can get the length of iterables like list and tuples using  function.Python allows defining a  on our custom class so that  works on it. For example,Similarly, you know how we can access elements of list and tuples using index notation.Python allows a  magic method to allow such functionality for custom classes. For example,With the above concept, now you can easily understand the builtin dataset like MNIST and what you can do with them.Let’s create a dataloader for a training dataset of MNIST digits.Now, let’s try accessing first batch from the data loader directly without looping. If we try to access it via index, we get an exception.You might have been used to doing it in this way.Have you ever wondered why do we wrap trainloader by  and then call ? Let’s demystify this.Consider a list  with 3 elements. In Python, we can create an iterator out of  using the  function.Iterators are used because they allow lazy loading such that only one element is loaded in memory at a time.We get each element and when we reach the end of the list, we get a  exception.This pattern matches our usual machine learning workflow where we take small batches of data at a time in memory and do the forward and backward pass. So,  also incorporates this pattern in PyTorch.To create iterators out of classes in Python, we need to define magic methods  and Here, the  function calls the  magic method of the class returning that same object. Then, the  function calls the  magic method of the class to return next element present in our data.In PyTorch, the implementation of DataLoader implements this pattern as follows:So, they decouple the iterator creation part and the actual data loading part.Thus, we get images and labels for a single batch.Thus, we saw how PyTorch borrows several advanced concepts from native Python itself in its API design. I hope the article was helpful to demystify how these concepts work behind the scenes and will help you become a better PyTorch user.PyTorch has emerged as one of the go-to deep learning frameworks in recent years. This popularity can be attributed to its easy to use API and it being more “pythonic”.PyTorch leverages numerous native features of Python to give us a consistent and clean API. In this article, I will explain those native features in detail. Learning these will help you better understand why you do things a certain way in PyTorch and make better use of what it has to offer.Layers such as  are some of the basic constructs in PyTorch that we use to build our models. You import the layer and apply them to tensors.Here we are able to call layer on some tensor , so it must be a function right? Is  returning a function? Let’s verify it by checking the type.Surprise!  is actually a class and layer an object of that class.“What! How could we call it then? Aren’t only functions supposed to be callable?”Nope, we can create callable objects as well. Python provides a native way to make objects created from classes callable by using magic functions.
Let’s see a simple example of a class that doubles a number.Here we add a magic method  in the class to double any number passed to it. Now, you can create an object out of this class and call it on some number.Alternatively, the above code can be combined in the single line itself.This works because everything in Python is an object. See an example of a function below that doubles a number.Even functions invoke the method behind the scenes.Let’s see an example of a model that applies a single fully connected layer to MNIST images to get 10 outputs.The following code should be familiar to you. We are computing output of this model on some tensor x.We know calling the model directly on some tensor executes the  function on it. How does that work?It’s the same reason in previous example. We’re inheriting the class . Internally,  has a  magic method that calls the . So, when we override  method later, it’s executed.Thus, we were able to call the model directly on tensors.In PyTorch, it is common to create a custom class inheriting from the  class to prepare our training and test datasets. Have you ever wondered why we define methods with obscure names like  and  in it?These methods are builtin magic methods of Python. You know how we can get the length of iterables like list and tuples using  function.Python allows defining a  on our custom class so that  works on it. For example,Similarly, you know how we can access elements of list and tuples using index notation.Python allows a  magic method to allow such functionality for custom classes. For example,With the above concept, now you can easily understand the builtin dataset like MNIST and what you can do with them.Let’s create a dataloader for a training dataset of MNIST digits.Now, let’s try accessing first batch from the data loader directly without looping. If we try to access it via index, we get an exception.You might have been used to doing it in this way.Have you ever wondered why do we wrap trainloader by  and then call ? Let’s demystify this.Consider a list  with 3 elements. In Python, we can create an iterator out of  using the  function.Iterators are used because they allow lazy loading such that only one element is loaded in memory at a time.We get each element and when we reach the end of the list, we get a  exception.This pattern matches our usual machine learning workflow where we take small batches of data at a time in memory and do the forward and backward pass. So,  also incorporates this pattern in PyTorch.To create iterators out of classes in Python, we need to define magic methods  and Here, the  function calls the  magic method of the class returning that same object. Then, the  function calls the  magic method of the class to return next element present in our data.In PyTorch, the implementation of DataLoader implements this pattern as follows:So, they decouple the iterator creation part and the actual data loading part.Thus, we get images and labels for a single batch.Thus, we saw how PyTorch borrows several advanced concepts from native Python itself in its API design. I hope the article was helpful to demystify how these concepts work behind the scenes and will help you become a better PyTorch user.