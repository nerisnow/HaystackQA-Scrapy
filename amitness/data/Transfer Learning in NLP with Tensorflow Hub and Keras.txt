Tensorflow 2.0 introduced Keras as the default high-level API to build models. Combined with pretrained models from Tensorflow Hub, it provides a dead-simple way for transfer learning in NLP to create good models out of the box.To illustrate the process, let’s take an example of classifying if the title of an article is clickbait or not.We will use the dataset from the paper  available .Since the goal of this article is to illustrate transfer learning, we will directly load an already pre-processed dataset into a pandas dataframe.The dataset consists of page titles and labels. The label is 1 if the title is clickbait.Let’s split the data into 70% training data and 30% validation data.Now, we install tensorflow and tensorflow-hub using pip.To use text data as features for models, we need to convert it into a numeric form. Tensorflow Hub provides various  for converting the sentences into embeddings such as BERT, NNLM and Wikiwords.Universal Sentence Encoder is one of the popular module for generating sentence embeddings. It gives back a 512 fixed-size vector for the text.
Below is an example of how we can use tensorflow hub to capture embeddings for the sentence “Hello World”.In Tensorflow 2.0, using these embeddings in our models is a piece of cake thanks to the new  module. Let’s design a tf.keras model for the binary classification task of clickbait detection.First import the required libraries.Then, we create a sequential model that will encapsulate our layers.The first layer will be a hub.KerasLayer from where we can loading models available at . We will be loading .Here are what the different parameters used mean:Next, we add a Dense layer with single node to output probability of clickbait between 0 and 1.In summary, we have a model that takes text data, projects it into 512-dimension embedding and passed that through a feedforward neural network with sigmoid activation to give a clickbait probability.Alternatively, we can implement the exact above architecture using the tf.keras functional API as well.The output of the model summary isThe number of trainable parameters is  because we’re finetuning Universal Sentence Encoder.Since we’re performing a binary classification task, we use a binary cross entropy loss along with ADAM optimizer and accuracy as the metric.Now, let’s train the model forWe reach a training accuracy of 99.62% and validation accuracy of 98.46% with only 2 epochs.Let’s test the model on a few examples.Thus, with a combination of Tensorflow Hub and tf.keras, we can leverage transfer learning easily and build high-performance models for any of our downstream tasks.Tensorflow 2.0 introduced Keras as the default high-level API to build models. Combined with pretrained models from Tensorflow Hub, it provides a dead-simple way for transfer learning in NLP to create good models out of the box.To illustrate the process, let’s take an example of classifying if the title of an article is clickbait or not.We will use the dataset from the paper  available .Since the goal of this article is to illustrate transfer learning, we will directly load an already pre-processed dataset into a pandas dataframe.The dataset consists of page titles and labels. The label is 1 if the title is clickbait.Let’s split the data into 70% training data and 30% validation data.Now, we install tensorflow and tensorflow-hub using pip.To use text data as features for models, we need to convert it into a numeric form. Tensorflow Hub provides various  for converting the sentences into embeddings such as BERT, NNLM and Wikiwords.Universal Sentence Encoder is one of the popular module for generating sentence embeddings. It gives back a 512 fixed-size vector for the text.
Below is an example of how we can use tensorflow hub to capture embeddings for the sentence “Hello World”.In Tensorflow 2.0, using these embeddings in our models is a piece of cake thanks to the new  module. Let’s design a tf.keras model for the binary classification task of clickbait detection.First import the required libraries.Then, we create a sequential model that will encapsulate our layers.The first layer will be a hub.KerasLayer from where we can loading models available at . We will be loading .Here are what the different parameters used mean:Next, we add a Dense layer with single node to output probability of clickbait between 0 and 1.In summary, we have a model that takes text data, projects it into 512-dimension embedding and passed that through a feedforward neural network with sigmoid activation to give a clickbait probability.Alternatively, we can implement the exact above architecture using the tf.keras functional API as well.The output of the model summary isThe number of trainable parameters is  because we’re finetuning Universal Sentence Encoder.Since we’re performing a binary classification task, we use a binary cross entropy loss along with ADAM optimizer and accuracy as the metric.Now, let’s train the model forWe reach a training accuracy of 99.62% and validation accuracy of 98.46% with only 2 epochs.Let’s test the model on a few examples.Thus, with a combination of Tensorflow Hub and tf.keras, we can leverage transfer learning easily and build high-performance models for any of our downstream tasks.Tensorflow 2.0 introduced Keras as the default high-level API to build models. Combined with pretrained models from Tensorflow Hub, it provides a dead-simple way for transfer learning in NLP to create good models out of the box.To illustrate the process, let’s take an example of classifying if the title of an article is clickbait or not.We will use the dataset from the paper  available .Since the goal of this article is to illustrate transfer learning, we will directly load an already pre-processed dataset into a pandas dataframe.The dataset consists of page titles and labels. The label is 1 if the title is clickbait.Let’s split the data into 70% training data and 30% validation data.Now, we install tensorflow and tensorflow-hub using pip.To use text data as features for models, we need to convert it into a numeric form. Tensorflow Hub provides various  for converting the sentences into embeddings such as BERT, NNLM and Wikiwords.Universal Sentence Encoder is one of the popular module for generating sentence embeddings. It gives back a 512 fixed-size vector for the text.
Below is an example of how we can use tensorflow hub to capture embeddings for the sentence “Hello World”.In Tensorflow 2.0, using these embeddings in our models is a piece of cake thanks to the new  module. Let’s design a tf.keras model for the binary classification task of clickbait detection.First import the required libraries.Then, we create a sequential model that will encapsulate our layers.The first layer will be a hub.KerasLayer from where we can loading models available at . We will be loading .Here are what the different parameters used mean:Next, we add a Dense layer with single node to output probability of clickbait between 0 and 1.In summary, we have a model that takes text data, projects it into 512-dimension embedding and passed that through a feedforward neural network with sigmoid activation to give a clickbait probability.Alternatively, we can implement the exact above architecture using the tf.keras functional API as well.The output of the model summary isThe number of trainable parameters is  because we’re finetuning Universal Sentence Encoder.Since we’re performing a binary classification task, we use a binary cross entropy loss along with ADAM optimizer and accuracy as the metric.Now, let’s train the model forWe reach a training accuracy of 99.62% and validation accuracy of 98.46% with only 2 epochs.Let’s test the model on a few examples.Thus, with a combination of Tensorflow Hub and tf.keras, we can leverage transfer learning easily and build high-performance models for any of our downstream tasks.