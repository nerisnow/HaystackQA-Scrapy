 is a free web application for visualizing high-dimensional data. It has built-in demos for visualizing word embeddings in NLP and image embeddings for MNIST in Computer Vision.I recently experimented with a way to load sentence embeddings along with the class labels into this tool and explore them interactively. In this blog post, I will explain the end-to-end process with an example dataset.To understand this use case, let’s take a subset of 100 movie reviews from the SST-2 dataset which are labeled as positive and negative.The dataset has a column containing the text and a label indicating whether it’s positive or negative opinion.We will introduce noise into our dataset by corrupting five of the responses with random text. It will act as an outlier for our example.Now, we will compute sentence embeddings for the headlines using the  package. First, let’s install it using pip.Next, we will create a helper function to return a NumPy array of sentence embeddings given a list of sentences.Using the above function, we can generate sentence embeddings for our data as shown below.Embedding Projector requires two TSV files to load our custom embeddings.Let’s first generate the  file for our sentence embeddings from the previous step.To generate , we simply save our original dataframe.We first go to .On the left-hand sidebar, click the  button.Then, for the first  button, upload the  file and for the second  button, upload the  file.After uploading both files, click outside and you should see the sentence embedding projection. The dimensions of embeddings are reduced to 3D by default using PCA.Let’s switch to 2D by turning off the checkbox for ‘Component #3’ in the bottom part of sidebar.On the 2D visualization, we can see how the random text is far from other groups of text as an . On hovering the point, we see the text .We can enable color coding of the points by their actual labels (positive vs negative) by using the  dropdown in the left sidebar.Select the name of the column that contains your labels. In our example file, the column name is .The points themselves are interactive. You can see the actual sentence for each point by hovering over them.You can click on the point to show the metadata. We can see below on clicking a blue point that its label is “positive” in the popup.So the blue points are positive and the red points are negative. When a point is selected, 100 nearest points in terms of cosine similarity are also highlighted.To get back to the original view, we can click on any empty white space.The color coding can be a useful heuristic for many use cases:The web app provides three standard dimensionality reduction techniques: , , and .You can choose the algorithm and their parameters from the bottom of the left sidebar.You can also use a custom keyword or full text as the axis using the  tab. This will apply a custom linear projection and can help us explore meaningful directions in the embedding space.For example, the Gmail team tried setting “yeah” on the left side and “yes” on the right side. When they projected encoder embeddings for email replies to this custom linear projection, they found replies in a casual tone (e.g. Here you go) on the left side and responses in a more formal tone clustered towards the right side.Thus, Embedding Projector is a very useful tool to better understand the datasets and models we work with. is a free web application for visualizing high-dimensional data. It has built-in demos for visualizing word embeddings in NLP and image embeddings for MNIST in Computer Vision.I recently experimented with a way to load sentence embeddings along with the class labels into this tool and explore them interactively. In this blog post, I will explain the end-to-end process with an example dataset.To understand this use case, let’s take a subset of 100 movie reviews from the SST-2 dataset which are labeled as positive and negative.The dataset has a column containing the text and a label indicating whether it’s positive or negative opinion.We will introduce noise into our dataset by corrupting five of the responses with random text. It will act as an outlier for our example.Now, we will compute sentence embeddings for the headlines using the  package. First, let’s install it using pip.Next, we will create a helper function to return a NumPy array of sentence embeddings given a list of sentences.Using the above function, we can generate sentence embeddings for our data as shown below.Embedding Projector requires two TSV files to load our custom embeddings.Let’s first generate the  file for our sentence embeddings from the previous step.To generate , we simply save our original dataframe.We first go to .On the left-hand sidebar, click the  button.Then, for the first  button, upload the  file and for the second  button, upload the  file.After uploading both files, click outside and you should see the sentence embedding projection. The dimensions of embeddings are reduced to 3D by default using PCA.Let’s switch to 2D by turning off the checkbox for ‘Component #3’ in the bottom part of sidebar.On the 2D visualization, we can see how the random text is far from other groups of text as an . On hovering the point, we see the text .We can enable color coding of the points by their actual labels (positive vs negative) by using the  dropdown in the left sidebar.Select the name of the column that contains your labels. In our example file, the column name is .The points themselves are interactive. You can see the actual sentence for each point by hovering over them.You can click on the point to show the metadata. We can see below on clicking a blue point that its label is “positive” in the popup.So the blue points are positive and the red points are negative. When a point is selected, 100 nearest points in terms of cosine similarity are also highlighted.To get back to the original view, we can click on any empty white space.The color coding can be a useful heuristic for many use cases:The web app provides three standard dimensionality reduction techniques: , , and .You can choose the algorithm and their parameters from the bottom of the left sidebar.You can also use a custom keyword or full text as the axis using the  tab. This will apply a custom linear projection and can help us explore meaningful directions in the embedding space.For example, the Gmail team tried setting “yeah” on the left side and “yes” on the right side. When they projected encoder embeddings for email replies to this custom linear projection, they found replies in a casual tone (e.g. Here you go) on the left side and responses in a more formal tone clustered towards the right side.Thus, Embedding Projector is a very useful tool to better understand the datasets and models we work with. is a free web application for visualizing high-dimensional data. It has built-in demos for visualizing word embeddings in NLP and image embeddings for MNIST in Computer Vision.I recently experimented with a way to load sentence embeddings along with the class labels into this tool and explore them interactively. In this blog post, I will explain the end-to-end process with an example dataset.To understand this use case, let’s take a subset of 100 movie reviews from the SST-2 dataset which are labeled as positive and negative.The dataset has a column containing the text and a label indicating whether it’s positive or negative opinion.We will introduce noise into our dataset by corrupting five of the responses with random text. It will act as an outlier for our example.Now, we will compute sentence embeddings for the headlines using the  package. First, let’s install it using pip.Next, we will create a helper function to return a NumPy array of sentence embeddings given a list of sentences.Using the above function, we can generate sentence embeddings for our data as shown below.Embedding Projector requires two TSV files to load our custom embeddings.Let’s first generate the  file for our sentence embeddings from the previous step.To generate , we simply save our original dataframe.We first go to .On the left-hand sidebar, click the  button.Then, for the first  button, upload the  file and for the second  button, upload the  file.After uploading both files, click outside and you should see the sentence embedding projection. The dimensions of embeddings are reduced to 3D by default using PCA.Let’s switch to 2D by turning off the checkbox for ‘Component #3’ in the bottom part of sidebar.On the 2D visualization, we can see how the random text is far from other groups of text as an . On hovering the point, we see the text .We can enable color coding of the points by their actual labels (positive vs negative) by using the  dropdown in the left sidebar.Select the name of the column that contains your labels. In our example file, the column name is .The points themselves are interactive. You can see the actual sentence for each point by hovering over them.You can click on the point to show the metadata. We can see below on clicking a blue point that its label is “positive” in the popup.So the blue points are positive and the red points are negative. When a point is selected, 100 nearest points in terms of cosine similarity are also highlighted.To get back to the original view, we can click on any empty white space.The color coding can be a useful heuristic for many use cases:The web app provides three standard dimensionality reduction techniques: , , and .You can choose the algorithm and their parameters from the bottom of the left sidebar.You can also use a custom keyword or full text as the axis using the  tab. This will apply a custom linear projection and can help us explore meaningful directions in the embedding space.For example, the Gmail team tried setting “yeah” on the left side and “yes” on the right side. When they projected encoder embeddings for email replies to this custom linear projection, they found replies in a casual tone (e.g. Here you go) on the left side and responses in a more formal tone clustered towards the right side.Thus, Embedding Projector is a very useful tool to better understand the datasets and models we work with. is a free web application for visualizing high-dimensional data. It has built-in demos for visualizing word embeddings in NLP and image embeddings for MNIST in Computer Vision.I recently experimented with a way to load sentence embeddings along with the class labels into this tool and explore them interactively. In this blog post, I will explain the end-to-end process with an example dataset.To understand this use case, let’s take a subset of 100 movie reviews from the SST-2 dataset which are labeled as positive and negative.The dataset has a column containing the text and a label indicating whether it’s positive or negative opinion.We will introduce noise into our dataset by corrupting five of the responses with random text. It will act as an outlier for our example.Now, we will compute sentence embeddings for the headlines using the  package. First, let’s install it using pip.Next, we will create a helper function to return a NumPy array of sentence embeddings given a list of sentences.Using the above function, we can generate sentence embeddings for our data as shown below.Embedding Projector requires two TSV files to load our custom embeddings.Let’s first generate the  file for our sentence embeddings from the previous step.To generate , we simply save our original dataframe.We first go to .On the left-hand sidebar, click the  button.Then, for the first  button, upload the  file and for the second  button, upload the  file.After uploading both files, click outside and you should see the sentence embedding projection. The dimensions of embeddings are reduced to 3D by default using PCA.Let’s switch to 2D by turning off the checkbox for ‘Component #3’ in the bottom part of sidebar.On the 2D visualization, we can see how the random text is far from other groups of text as an . On hovering the point, we see the text .We can enable color coding of the points by their actual labels (positive vs negative) by using the  dropdown in the left sidebar.Select the name of the column that contains your labels. In our example file, the column name is .The points themselves are interactive. You can see the actual sentence for each point by hovering over them.You can click on the point to show the metadata. We can see below on clicking a blue point that its label is “positive” in the popup.So the blue points are positive and the red points are negative. When a point is selected, 100 nearest points in terms of cosine similarity are also highlighted.To get back to the original view, we can click on any empty white space.The color coding can be a useful heuristic for many use cases:The web app provides three standard dimensionality reduction techniques: , , and .You can choose the algorithm and their parameters from the bottom of the left sidebar.You can also use a custom keyword or full text as the axis using the  tab. This will apply a custom linear projection and can help us explore meaningful directions in the embedding space.For example, the Gmail team tried setting “yeah” on the left side and “yes” on the right side. When they projected encoder embeddings for email replies to this custom linear projection, they found replies in a casual tone (e.g. Here you go) on the left side and responses in a more formal tone clustered towards the right side.Thus, Embedding Projector is a very useful tool to better understand the datasets and models we work with. is a free web application for visualizing high-dimensional data. It has built-in demos for visualizing word embeddings in NLP and image embeddings for MNIST in Computer Vision.I recently experimented with a way to load sentence embeddings along with the class labels into this tool and explore them interactively. In this blog post, I will explain the end-to-end process with an example dataset.To understand this use case, let’s take a subset of 100 movie reviews from the SST-2 dataset which are labeled as positive and negative.The dataset has a column containing the text and a label indicating whether it’s positive or negative opinion.We will introduce noise into our dataset by corrupting five of the responses with random text. It will act as an outlier for our example.Now, we will compute sentence embeddings for the headlines using the  package. First, let’s install it using pip.Next, we will create a helper function to return a NumPy array of sentence embeddings given a list of sentences.Using the above function, we can generate sentence embeddings for our data as shown below.Embedding Projector requires two TSV files to load our custom embeddings.Let’s first generate the  file for our sentence embeddings from the previous step.To generate , we simply save our original dataframe.We first go to .On the left-hand sidebar, click the  button.Then, for the first  button, upload the  file and for the second  button, upload the  file.After uploading both files, click outside and you should see the sentence embedding projection. The dimensions of embeddings are reduced to 3D by default using PCA.Let’s switch to 2D by turning off the checkbox for ‘Component #3’ in the bottom part of sidebar.On the 2D visualization, we can see how the random text is far from other groups of text as an . On hovering the point, we see the text .We can enable color coding of the points by their actual labels (positive vs negative) by using the  dropdown in the left sidebar.Select the name of the column that contains your labels. In our example file, the column name is .The points themselves are interactive. You can see the actual sentence for each point by hovering over them.You can click on the point to show the metadata. We can see below on clicking a blue point that its label is “positive” in the popup.So the blue points are positive and the red points are negative. When a point is selected, 100 nearest points in terms of cosine similarity are also highlighted.To get back to the original view, we can click on any empty white space.The color coding can be a useful heuristic for many use cases:The web app provides three standard dimensionality reduction techniques: , , and .You can choose the algorithm and their parameters from the bottom of the left sidebar.You can also use a custom keyword or full text as the axis using the  tab. This will apply a custom linear projection and can help us explore meaningful directions in the embedding space.For example, the Gmail team tried setting “yeah” on the left side and “yes” on the right side. When they projected encoder embeddings for email replies to this custom linear projection, they found replies in a casual tone (e.g. Here you go) on the left side and responses in a more formal tone clustered towards the right side.Thus, Embedding Projector is a very useful tool to better understand the datasets and models we work with. is a free web application for visualizing high-dimensional data. It has built-in demos for visualizing word embeddings in NLP and image embeddings for MNIST in Computer Vision.I recently experimented with a way to load sentence embeddings along with the class labels into this tool and explore them interactively. In this blog post, I will explain the end-to-end process with an example dataset.To understand this use case, let’s take a subset of 100 movie reviews from the SST-2 dataset which are labeled as positive and negative.The dataset has a column containing the text and a label indicating whether it’s positive or negative opinion.We will introduce noise into our dataset by corrupting five of the responses with random text. It will act as an outlier for our example.Now, we will compute sentence embeddings for the headlines using the  package. First, let’s install it using pip.Next, we will create a helper function to return a NumPy array of sentence embeddings given a list of sentences.Using the above function, we can generate sentence embeddings for our data as shown below.Embedding Projector requires two TSV files to load our custom embeddings.Let’s first generate the  file for our sentence embeddings from the previous step.To generate , we simply save our original dataframe.We first go to .On the left-hand sidebar, click the  button.Then, for the first  button, upload the  file and for the second  button, upload the  file.After uploading both files, click outside and you should see the sentence embedding projection. The dimensions of embeddings are reduced to 3D by default using PCA.Let’s switch to 2D by turning off the checkbox for ‘Component #3’ in the bottom part of sidebar.On the 2D visualization, we can see how the random text is far from other groups of text as an . On hovering the point, we see the text .We can enable color coding of the points by their actual labels (positive vs negative) by using the  dropdown in the left sidebar.Select the name of the column that contains your labels. In our example file, the column name is .The points themselves are interactive. You can see the actual sentence for each point by hovering over them.You can click on the point to show the metadata. We can see below on clicking a blue point that its label is “positive” in the popup.So the blue points are positive and the red points are negative. When a point is selected, 100 nearest points in terms of cosine similarity are also highlighted.To get back to the original view, we can click on any empty white space.The color coding can be a useful heuristic for many use cases:The web app provides three standard dimensionality reduction techniques: , , and .You can choose the algorithm and their parameters from the bottom of the left sidebar.You can also use a custom keyword or full text as the axis using the  tab. This will apply a custom linear projection and can help us explore meaningful directions in the embedding space.For example, the Gmail team tried setting “yeah” on the left side and “yes” on the right side. When they projected encoder embeddings for email replies to this custom linear projection, they found replies in a casual tone (e.g. Here you go) on the left side and responses in a more formal tone clustered towards the right side.Thus, Embedding Projector is a very useful tool to better understand the datasets and models we work with. is a free web application for visualizing high-dimensional data. It has built-in demos for visualizing word embeddings in NLP and image embeddings for MNIST in Computer Vision.I recently experimented with a way to load sentence embeddings along with the class labels into this tool and explore them interactively. In this blog post, I will explain the end-to-end process with an example dataset.To understand this use case, let’s take a subset of 100 movie reviews from the SST-2 dataset which are labeled as positive and negative.The dataset has a column containing the text and a label indicating whether it’s positive or negative opinion.We will introduce noise into our dataset by corrupting five of the responses with random text. It will act as an outlier for our example.Now, we will compute sentence embeddings for the headlines using the  package. First, let’s install it using pip.Next, we will create a helper function to return a NumPy array of sentence embeddings given a list of sentences.Using the above function, we can generate sentence embeddings for our data as shown below.Embedding Projector requires two TSV files to load our custom embeddings.Let’s first generate the  file for our sentence embeddings from the previous step.To generate , we simply save our original dataframe.We first go to .On the left-hand sidebar, click the  button.Then, for the first  button, upload the  file and for the second  button, upload the  file.After uploading both files, click outside and you should see the sentence embedding projection. The dimensions of embeddings are reduced to 3D by default using PCA.Let’s switch to 2D by turning off the checkbox for ‘Component #3’ in the bottom part of sidebar.On the 2D visualization, we can see how the random text is far from other groups of text as an . On hovering the point, we see the text .We can enable color coding of the points by their actual labels (positive vs negative) by using the  dropdown in the left sidebar.Select the name of the column that contains your labels. In our example file, the column name is .The points themselves are interactive. You can see the actual sentence for each point by hovering over them.You can click on the point to show the metadata. We can see below on clicking a blue point that its label is “positive” in the popup.So the blue points are positive and the red points are negative. When a point is selected, 100 nearest points in terms of cosine similarity are also highlighted.To get back to the original view, we can click on any empty white space.The color coding can be a useful heuristic for many use cases:The web app provides three standard dimensionality reduction techniques: , , and .You can choose the algorithm and their parameters from the bottom of the left sidebar.You can also use a custom keyword or full text as the axis using the  tab. This will apply a custom linear projection and can help us explore meaningful directions in the embedding space.For example, the Gmail team tried setting “yeah” on the left side and “yes” on the right side. When they projected encoder embeddings for email replies to this custom linear projection, they found replies in a casual tone (e.g. Here you go) on the left side and responses in a more formal tone clustered towards the right side.Thus, Embedding Projector is a very useful tool to better understand the datasets and models we work with. is a free web application for visualizing high-dimensional data. It has built-in demos for visualizing word embeddings in NLP and image embeddings for MNIST in Computer Vision.I recently experimented with a way to load sentence embeddings along with the class labels into this tool and explore them interactively. In this blog post, I will explain the end-to-end process with an example dataset.To understand this use case, let’s take a subset of 100 movie reviews from the SST-2 dataset which are labeled as positive and negative.The dataset has a column containing the text and a label indicating whether it’s positive or negative opinion.We will introduce noise into our dataset by corrupting five of the responses with random text. It will act as an outlier for our example.Now, we will compute sentence embeddings for the headlines using the  package. First, let’s install it using pip.Next, we will create a helper function to return a NumPy array of sentence embeddings given a list of sentences.Using the above function, we can generate sentence embeddings for our data as shown below.Embedding Projector requires two TSV files to load our custom embeddings.Let’s first generate the  file for our sentence embeddings from the previous step.To generate , we simply save our original dataframe.We first go to .On the left-hand sidebar, click the  button.Then, for the first  button, upload the  file and for the second  button, upload the  file.After uploading both files, click outside and you should see the sentence embedding projection. The dimensions of embeddings are reduced to 3D by default using PCA.Let’s switch to 2D by turning off the checkbox for ‘Component #3’ in the bottom part of sidebar.On the 2D visualization, we can see how the random text is far from other groups of text as an . On hovering the point, we see the text .We can enable color coding of the points by their actual labels (positive vs negative) by using the  dropdown in the left sidebar.Select the name of the column that contains your labels. In our example file, the column name is .The points themselves are interactive. You can see the actual sentence for each point by hovering over them.You can click on the point to show the metadata. We can see below on clicking a blue point that its label is “positive” in the popup.So the blue points are positive and the red points are negative. When a point is selected, 100 nearest points in terms of cosine similarity are also highlighted.To get back to the original view, we can click on any empty white space.The color coding can be a useful heuristic for many use cases:The web app provides three standard dimensionality reduction techniques: , , and .You can choose the algorithm and their parameters from the bottom of the left sidebar.You can also use a custom keyword or full text as the axis using the  tab. This will apply a custom linear projection and can help us explore meaningful directions in the embedding space.For example, the Gmail team tried setting “yeah” on the left side and “yes” on the right side. When they projected encoder embeddings for email replies to this custom linear projection, they found replies in a casual tone (e.g. Here you go) on the left side and responses in a more formal tone clustered towards the right side.Thus, Embedding Projector is a very useful tool to better understand the datasets and models we work with. is a free web application for visualizing high-dimensional data. It has built-in demos for visualizing word embeddings in NLP and image embeddings for MNIST in Computer Vision.I recently experimented with a way to load sentence embeddings along with the class labels into this tool and explore them interactively. In this blog post, I will explain the end-to-end process with an example dataset.To understand this use case, let’s take a subset of 100 movie reviews from the SST-2 dataset which are labeled as positive and negative.The dataset has a column containing the text and a label indicating whether it’s positive or negative opinion.We will introduce noise into our dataset by corrupting five of the responses with random text. It will act as an outlier for our example.Now, we will compute sentence embeddings for the headlines using the  package. First, let’s install it using pip.Next, we will create a helper function to return a NumPy array of sentence embeddings given a list of sentences.Using the above function, we can generate sentence embeddings for our data as shown below.Embedding Projector requires two TSV files to load our custom embeddings.Let’s first generate the  file for our sentence embeddings from the previous step.To generate , we simply save our original dataframe.We first go to .On the left-hand sidebar, click the  button.Then, for the first  button, upload the  file and for the second  button, upload the  file.After uploading both files, click outside and you should see the sentence embedding projection. The dimensions of embeddings are reduced to 3D by default using PCA.Let’s switch to 2D by turning off the checkbox for ‘Component #3’ in the bottom part of sidebar.On the 2D visualization, we can see how the random text is far from other groups of text as an . On hovering the point, we see the text .We can enable color coding of the points by their actual labels (positive vs negative) by using the  dropdown in the left sidebar.Select the name of the column that contains your labels. In our example file, the column name is .The points themselves are interactive. You can see the actual sentence for each point by hovering over them.You can click on the point to show the metadata. We can see below on clicking a blue point that its label is “positive” in the popup.So the blue points are positive and the red points are negative. When a point is selected, 100 nearest points in terms of cosine similarity are also highlighted.To get back to the original view, we can click on any empty white space.The color coding can be a useful heuristic for many use cases:The web app provides three standard dimensionality reduction techniques: , , and .You can choose the algorithm and their parameters from the bottom of the left sidebar.You can also use a custom keyword or full text as the axis using the  tab. This will apply a custom linear projection and can help us explore meaningful directions in the embedding space.For example, the Gmail team tried setting “yeah” on the left side and “yes” on the right side. When they projected encoder embeddings for email replies to this custom linear projection, they found replies in a casual tone (e.g. Here you go) on the left side and responses in a more formal tone clustered towards the right side.Thus, Embedding Projector is a very useful tool to better understand the datasets and models we work with. is a free web application for visualizing high-dimensional data. It has built-in demos for visualizing word embeddings in NLP and image embeddings for MNIST in Computer Vision.I recently experimented with a way to load sentence embeddings along with the class labels into this tool and explore them interactively. In this blog post, I will explain the end-to-end process with an example dataset.To understand this use case, let’s take a subset of 100 movie reviews from the SST-2 dataset which are labeled as positive and negative.The dataset has a column containing the text and a label indicating whether it’s positive or negative opinion.We will introduce noise into our dataset by corrupting five of the responses with random text. It will act as an outlier for our example.Now, we will compute sentence embeddings for the headlines using the  package. First, let’s install it using pip.Next, we will create a helper function to return a NumPy array of sentence embeddings given a list of sentences.Using the above function, we can generate sentence embeddings for our data as shown below.Embedding Projector requires two TSV files to load our custom embeddings.Let’s first generate the  file for our sentence embeddings from the previous step.To generate , we simply save our original dataframe.We first go to .On the left-hand sidebar, click the  button.Then, for the first  button, upload the  file and for the second  button, upload the  file.After uploading both files, click outside and you should see the sentence embedding projection. The dimensions of embeddings are reduced to 3D by default using PCA.Let’s switch to 2D by turning off the checkbox for ‘Component #3’ in the bottom part of sidebar.On the 2D visualization, we can see how the random text is far from other groups of text as an . On hovering the point, we see the text .We can enable color coding of the points by their actual labels (positive vs negative) by using the  dropdown in the left sidebar.Select the name of the column that contains your labels. In our example file, the column name is .The points themselves are interactive. You can see the actual sentence for each point by hovering over them.You can click on the point to show the metadata. We can see below on clicking a blue point that its label is “positive” in the popup.So the blue points are positive and the red points are negative. When a point is selected, 100 nearest points in terms of cosine similarity are also highlighted.To get back to the original view, we can click on any empty white space.The color coding can be a useful heuristic for many use cases:The web app provides three standard dimensionality reduction techniques: , , and .You can choose the algorithm and their parameters from the bottom of the left sidebar.You can also use a custom keyword or full text as the axis using the  tab. This will apply a custom linear projection and can help us explore meaningful directions in the embedding space.For example, the Gmail team tried setting “yeah” on the left side and “yes” on the right side. When they projected encoder embeddings for email replies to this custom linear projection, they found replies in a casual tone (e.g. Here you go) on the left side and responses in a more formal tone clustered towards the right side.Thus, Embedding Projector is a very useful tool to better understand the datasets and models we work with. is a free web application for visualizing high-dimensional data. It has built-in demos for visualizing word embeddings in NLP and image embeddings for MNIST in Computer Vision.I recently experimented with a way to load sentence embeddings along with the class labels into this tool and explore them interactively. In this blog post, I will explain the end-to-end process with an example dataset.To understand this use case, let’s take a subset of 100 movie reviews from the SST-2 dataset which are labeled as positive and negative.The dataset has a column containing the text and a label indicating whether it’s positive or negative opinion.We will introduce noise into our dataset by corrupting five of the responses with random text. It will act as an outlier for our example.Now, we will compute sentence embeddings for the headlines using the  package. First, let’s install it using pip.Next, we will create a helper function to return a NumPy array of sentence embeddings given a list of sentences.Using the above function, we can generate sentence embeddings for our data as shown below.Embedding Projector requires two TSV files to load our custom embeddings.Let’s first generate the  file for our sentence embeddings from the previous step.To generate , we simply save our original dataframe.We first go to .On the left-hand sidebar, click the  button.Then, for the first  button, upload the  file and for the second  button, upload the  file.After uploading both files, click outside and you should see the sentence embedding projection. The dimensions of embeddings are reduced to 3D by default using PCA.Let’s switch to 2D by turning off the checkbox for ‘Component #3’ in the bottom part of sidebar.On the 2D visualization, we can see how the random text is far from other groups of text as an . On hovering the point, we see the text .We can enable color coding of the points by their actual labels (positive vs negative) by using the  dropdown in the left sidebar.Select the name of the column that contains your labels. In our example file, the column name is .The points themselves are interactive. You can see the actual sentence for each point by hovering over them.You can click on the point to show the metadata. We can see below on clicking a blue point that its label is “positive” in the popup.So the blue points are positive and the red points are negative. When a point is selected, 100 nearest points in terms of cosine similarity are also highlighted.To get back to the original view, we can click on any empty white space.The color coding can be a useful heuristic for many use cases:The web app provides three standard dimensionality reduction techniques: , , and .You can choose the algorithm and their parameters from the bottom of the left sidebar.You can also use a custom keyword or full text as the axis using the  tab. This will apply a custom linear projection and can help us explore meaningful directions in the embedding space.For example, the Gmail team tried setting “yeah” on the left side and “yes” on the right side. When they projected encoder embeddings for email replies to this custom linear projection, they found replies in a casual tone (e.g. Here you go) on the left side and responses in a more formal tone clustered towards the right side.Thus, Embedding Projector is a very useful tool to better understand the datasets and models we work with. is a free web application for visualizing high-dimensional data. It has built-in demos for visualizing word embeddings in NLP and image embeddings for MNIST in Computer Vision.I recently experimented with a way to load sentence embeddings along with the class labels into this tool and explore them interactively. In this blog post, I will explain the end-to-end process with an example dataset.To understand this use case, let’s take a subset of 100 movie reviews from the SST-2 dataset which are labeled as positive and negative.The dataset has a column containing the text and a label indicating whether it’s positive or negative opinion.We will introduce noise into our dataset by corrupting five of the responses with random text. It will act as an outlier for our example.Now, we will compute sentence embeddings for the headlines using the  package. First, let’s install it using pip.Next, we will create a helper function to return a NumPy array of sentence embeddings given a list of sentences.Using the above function, we can generate sentence embeddings for our data as shown below.Embedding Projector requires two TSV files to load our custom embeddings.Let’s first generate the  file for our sentence embeddings from the previous step.To generate , we simply save our original dataframe.We first go to .On the left-hand sidebar, click the  button.Then, for the first  button, upload the  file and for the second  button, upload the  file.After uploading both files, click outside and you should see the sentence embedding projection. The dimensions of embeddings are reduced to 3D by default using PCA.Let’s switch to 2D by turning off the checkbox for ‘Component #3’ in the bottom part of sidebar.On the 2D visualization, we can see how the random text is far from other groups of text as an . On hovering the point, we see the text .We can enable color coding of the points by their actual labels (positive vs negative) by using the  dropdown in the left sidebar.Select the name of the column that contains your labels. In our example file, the column name is .The points themselves are interactive. You can see the actual sentence for each point by hovering over them.You can click on the point to show the metadata. We can see below on clicking a blue point that its label is “positive” in the popup.So the blue points are positive and the red points are negative. When a point is selected, 100 nearest points in terms of cosine similarity are also highlighted.To get back to the original view, we can click on any empty white space.The color coding can be a useful heuristic for many use cases:The web app provides three standard dimensionality reduction techniques: , , and .You can choose the algorithm and their parameters from the bottom of the left sidebar.You can also use a custom keyword or full text as the axis using the  tab. This will apply a custom linear projection and can help us explore meaningful directions in the embedding space.For example, the Gmail team tried setting “yeah” on the left side and “yes” on the right side. When they projected encoder embeddings for email replies to this custom linear projection, they found replies in a casual tone (e.g. Here you go) on the left side and responses in a more formal tone clustered towards the right side.Thus, Embedding Projector is a very useful tool to better understand the datasets and models we work with. is a free web application for visualizing high-dimensional data. It has built-in demos for visualizing word embeddings in NLP and image embeddings for MNIST in Computer Vision.I recently experimented with a way to load sentence embeddings along with the class labels into this tool and explore them interactively. In this blog post, I will explain the end-to-end process with an example dataset.To understand this use case, let’s take a subset of 100 movie reviews from the SST-2 dataset which are labeled as positive and negative.The dataset has a column containing the text and a label indicating whether it’s positive or negative opinion.We will introduce noise into our dataset by corrupting five of the responses with random text. It will act as an outlier for our example.Now, we will compute sentence embeddings for the headlines using the  package. First, let’s install it using pip.Next, we will create a helper function to return a NumPy array of sentence embeddings given a list of sentences.Using the above function, we can generate sentence embeddings for our data as shown below.Embedding Projector requires two TSV files to load our custom embeddings.Let’s first generate the  file for our sentence embeddings from the previous step.To generate , we simply save our original dataframe.We first go to .On the left-hand sidebar, click the  button.Then, for the first  button, upload the  file and for the second  button, upload the  file.After uploading both files, click outside and you should see the sentence embedding projection. The dimensions of embeddings are reduced to 3D by default using PCA.Let’s switch to 2D by turning off the checkbox for ‘Component #3’ in the bottom part of sidebar.On the 2D visualization, we can see how the random text is far from other groups of text as an . On hovering the point, we see the text .We can enable color coding of the points by their actual labels (positive vs negative) by using the  dropdown in the left sidebar.Select the name of the column that contains your labels. In our example file, the column name is .The points themselves are interactive. You can see the actual sentence for each point by hovering over them.You can click on the point to show the metadata. We can see below on clicking a blue point that its label is “positive” in the popup.So the blue points are positive and the red points are negative. When a point is selected, 100 nearest points in terms of cosine similarity are also highlighted.To get back to the original view, we can click on any empty white space.The color coding can be a useful heuristic for many use cases:The web app provides three standard dimensionality reduction techniques: , , and .You can choose the algorithm and their parameters from the bottom of the left sidebar.You can also use a custom keyword or full text as the axis using the  tab. This will apply a custom linear projection and can help us explore meaningful directions in the embedding space.For example, the Gmail team tried setting “yeah” on the left side and “yes” on the right side. When they projected encoder embeddings for email replies to this custom linear projection, they found replies in a casual tone (e.g. Here you go) on the left side and responses in a more formal tone clustered towards the right side.Thus, Embedding Projector is a very useful tool to better understand the datasets and models we work with. is a free web application for visualizing high-dimensional data. It has built-in demos for visualizing word embeddings in NLP and image embeddings for MNIST in Computer Vision.I recently experimented with a way to load sentence embeddings along with the class labels into this tool and explore them interactively. In this blog post, I will explain the end-to-end process with an example dataset.To understand this use case, let’s take a subset of 100 movie reviews from the SST-2 dataset which are labeled as positive and negative.The dataset has a column containing the text and a label indicating whether it’s positive or negative opinion.We will introduce noise into our dataset by corrupting five of the responses with random text. It will act as an outlier for our example.Now, we will compute sentence embeddings for the headlines using the  package. First, let’s install it using pip.Next, we will create a helper function to return a NumPy array of sentence embeddings given a list of sentences.Using the above function, we can generate sentence embeddings for our data as shown below.Embedding Projector requires two TSV files to load our custom embeddings.Let’s first generate the  file for our sentence embeddings from the previous step.To generate , we simply save our original dataframe.We first go to .On the left-hand sidebar, click the  button.Then, for the first  button, upload the  file and for the second  button, upload the  file.After uploading both files, click outside and you should see the sentence embedding projection. The dimensions of embeddings are reduced to 3D by default using PCA.Let’s switch to 2D by turning off the checkbox for ‘Component #3’ in the bottom part of sidebar.On the 2D visualization, we can see how the random text is far from other groups of text as an . On hovering the point, we see the text .We can enable color coding of the points by their actual labels (positive vs negative) by using the  dropdown in the left sidebar.Select the name of the column that contains your labels. In our example file, the column name is .The points themselves are interactive. You can see the actual sentence for each point by hovering over them.You can click on the point to show the metadata. We can see below on clicking a blue point that its label is “positive” in the popup.So the blue points are positive and the red points are negative. When a point is selected, 100 nearest points in terms of cosine similarity are also highlighted.To get back to the original view, we can click on any empty white space.The color coding can be a useful heuristic for many use cases:The web app provides three standard dimensionality reduction techniques: , , and .You can choose the algorithm and their parameters from the bottom of the left sidebar.You can also use a custom keyword or full text as the axis using the  tab. This will apply a custom linear projection and can help us explore meaningful directions in the embedding space.For example, the Gmail team tried setting “yeah” on the left side and “yes” on the right side. When they projected encoder embeddings for email replies to this custom linear projection, they found replies in a casual tone (e.g. Here you go) on the left side and responses in a more formal tone clustered towards the right side.Thus, Embedding Projector is a very useful tool to better understand the datasets and models we work with. is a free web application for visualizing high-dimensional data. It has built-in demos for visualizing word embeddings in NLP and image embeddings for MNIST in Computer Vision.I recently experimented with a way to load sentence embeddings along with the class labels into this tool and explore them interactively. In this blog post, I will explain the end-to-end process with an example dataset.To understand this use case, let’s take a subset of 100 movie reviews from the SST-2 dataset which are labeled as positive and negative.The dataset has a column containing the text and a label indicating whether it’s positive or negative opinion.We will introduce noise into our dataset by corrupting five of the responses with random text. It will act as an outlier for our example.Now, we will compute sentence embeddings for the headlines using the  package. First, let’s install it using pip.Next, we will create a helper function to return a NumPy array of sentence embeddings given a list of sentences.Using the above function, we can generate sentence embeddings for our data as shown below.Embedding Projector requires two TSV files to load our custom embeddings.Let’s first generate the  file for our sentence embeddings from the previous step.To generate , we simply save our original dataframe.We first go to .On the left-hand sidebar, click the  button.Then, for the first  button, upload the  file and for the second  button, upload the  file.After uploading both files, click outside and you should see the sentence embedding projection. The dimensions of embeddings are reduced to 3D by default using PCA.Let’s switch to 2D by turning off the checkbox for ‘Component #3’ in the bottom part of sidebar.On the 2D visualization, we can see how the random text is far from other groups of text as an . On hovering the point, we see the text .We can enable color coding of the points by their actual labels (positive vs negative) by using the  dropdown in the left sidebar.Select the name of the column that contains your labels. In our example file, the column name is .The points themselves are interactive. You can see the actual sentence for each point by hovering over them.You can click on the point to show the metadata. We can see below on clicking a blue point that its label is “positive” in the popup.So the blue points are positive and the red points are negative. When a point is selected, 100 nearest points in terms of cosine similarity are also highlighted.To get back to the original view, we can click on any empty white space.The color coding can be a useful heuristic for many use cases:The web app provides three standard dimensionality reduction techniques: , , and .You can choose the algorithm and their parameters from the bottom of the left sidebar.You can also use a custom keyword or full text as the axis using the  tab. This will apply a custom linear projection and can help us explore meaningful directions in the embedding space.For example, the Gmail team tried setting “yeah” on the left side and “yes” on the right side. When they projected encoder embeddings for email replies to this custom linear projection, they found replies in a casual tone (e.g. Here you go) on the left side and responses in a more formal tone clustered towards the right side.Thus, Embedding Projector is a very useful tool to better understand the datasets and models we work with.